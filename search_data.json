[
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

  {
    "title": "Appendix",
    "url": " /coursebook/Appendix",
   "content": "  Appendix          Shell                  Shell tricks and tips          Alright then what’s a terminal?          Common Utilities          Syntactic          Tips and tricks          What are environment variables?                    Stack Smashing      Assorted Man Pages      System Programming Jokes                  Light bulb jokes          Groaners          System Programmer (Definition)                    [1][]  AppendixShellA shell is actually how you are going to be interacting with the system. Before user friendly operating systems, when a computer started up all you had access to was a shell. This meant that all of your commands and editing had to be done this way. Nowadays, our computers start up in desktop mode, but one can still access a shell using a terminal.(Stuff) $It is ready for your next command! You can type in a lot of unix utilities like ls, echo Hello and the shell will execute them and give you the result. Some of these are what are known as shell-builtins meaning that the code is in the shell program itself. Some of these are compiled programs that you run. The shell only looks through a special variable called path which contains a list of : separated paths to search for an executable with your name, here is an example path.$ echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/gamesSo when the shell executes ls, it looks through all of those directories, finds /bin/ls and executes that.$ ls...$ /bin/lsYou can always call through the full path. That is always why in past classes if you want to run something on the terminal you’ve had to do ./exe because typically the directory that you are working in is not in the PATH variable. The . expands to your current directory and your shell executes &lt;current_dir&gt;/exe which is a valid command.Shell tricks and tips      The up arrow will get you your most recent command        ctrl-r will search commands that you previously ran        ctrl-c will interrupt your shell’s process        Add more!  Alright then what’s a terminal?A terminal is just an application that displays the output form the shell. You can have your default terminal, a quake based terminal, terminator, the options are endless!Common Utilities      cat concatenate multiple files. It is regularly used to print out the contents of a file to the terminal but the original use was concatenation.    $ cat file.txt...$ cat shakespeare.txt shakespeare.txt &gt; two_shakes.txt            diff tells you the difference of the two files. If nothing is printed, then zero is returned meaning the files are the same byte for byte. Otherwise, the longest common subsequence difference is printed    $ cat prog.txthelloworld$ cat adele.txthelloit's me$ diff prog.txt prog.txt$ diff shakespeare.txt shakespeare.txt2c2&lt; world---&gt; it's me            grep tells you which lines in a file or standard in match a POSIX pattern.    $ grep it adele.txtit's me            ls tells you which files are in the current directory.        cd this is a shell builtin but it changes to a relative or absolute directory    $ cd /usr$ cd lib/$ cd -$ pwd/usr/            man every system programmers favorite command, tells you more about all your favorite functions!        make executes programs according to a makefile.  SyntacticShells have many useful utilities like saving output to a file using redirection &gt;. This overwrites the file from the beginning. If you only meant to append to the file, you can use &gt;&gt;. Unix also allows file descriptor swapping. This means that you can take the output going to one file descriptor and make it seem like its coming out of another. The most common one is 2&gt;&amp;1 which means take the stderr and make it seem like it is coming out of standard out. This is important because when you use &gt; and &gt;&gt; they only write the standard output of the file. There are some examples below.$ ./program &gt; output.txt # To overwrite$ ./program &gt;&gt; output.txt # To append$ ./program 2&gt;&amp;1 &gt; output_all.txt # stderr &amp; stdout$ ./program 2&gt;&amp;1 &gt; /dev/null # don't care about any outputThe pipe operator has a fascinating history. The UNIX philosophy is writing small programs and chaining them together to do new and interesting things. Back in the early days, hard disk space was limited and write times were slow. Brian Kernighan wanted to maintain the philosophy while also not having to write a bunch of intermediate files that take up hard drive space. So, the UNIX pipe was born. A pipe take the stdout of the program on its left and feeds it to the stdin of the program on its write. Consider the command tee. It can be used as a replacement for the redirection operators because tee will both write to a file and output to standard out. It also has the added benefit that it doesn’t need to be the last command in the list. Meaning, that you can write an intermediate result and continue your piping.$ ./program | tee output.txt # Overwrite$ ./program | tee -a output.txt # Append$ head output.txt | wc | head -n 1 # Multi pipes$ ((head output.txt) | wc) | head -n 1 # Same as above$ ./program | tee intermediate.txt | wcThe &amp;&amp; and || operator are operators that execute a command sequentially. &amp;&amp; only executes a command if the previous command succeeds, and || always executes the next command.$ false &amp;&amp; echo \"Hello!\"$ true &amp;&amp; echo \"Hello!\"$ false || echo \"Hello!\"Shell TricksTips and tricksShell TricksWhat are environment variables?Well each process gets its own dictionary of environment variables that are copied over to the child. Meaning, if the parent changes their environment variables it won’t be transferred to the child and vice versa. This is important in the fork-exec-wait trilogy if you want to exec a program with different environment variables than your parent (or any other process).For example, you can write a C program that loops through all of the time zones and executes the date command to print out the date and time in all locals. Environment variables are used for all sorts of programs so modifying them is important.Stack SmashingEach thread uses a stack memory. The stack ‘grows downwards’ - if a function calls another function, then the stack is extended to smaller memory addresses. Stack memory includes non-static automatic (temporary) variables, parameter values and the return address. If a buffer is too small some data (e.g. input values from the user), then there is a real possibility that other stack variables and even the return address will be overwritten. The precise layout of the stack’s contents and order of the automatic variables is architecture and compiler dependent. However with a little investigative work we can learn how to deliberately smash the stack for a particular architecture.The example below demonstrates how the return address is stored on the stack. For a particular 32 bit architecture Live Linux Machine, we determine that the return address is stored at an address two pointers (8 bytes) above the address of the automatic variable. The code deliberately changes the stack value so that when the input function returns, rather than continuing on inside the main method, it jumps to the exploit function instead.// Overwrites the return address on the following machine:// http://cs-education.github.io/sys/#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;void breakout() {    puts(\"Welcome. Have a shell...\");    system(\"/bin/sh\");}void input() {  void *p;  printf(\"Address of stack variable: %p\\n\", &amp;p);  printf(\"Something that looks like a return address on stack: %p\\n\", *((&amp;p)+2));  // Let's change it to point to the start of our sneaky function.  *((&amp;p)+2) = breakout;}int main() {    printf(\"main() code starts at %p\\n\",main);        input();    while (1) {        puts(\"Hello\");        sleep(1);    }    return 0;}There are a lot of ways that computers tend to get around this.Assorted Man PagesPut in man pages and other discussionsSystem Programming Jokes0x43 0x61 0x74 0xe0 0xf9 0xbf 0x5f 0xff 0x7f 0x00Warning: Authors are not responsible for any neuro-apoptosis caused by these “jokes.” - Groaners are allowed.Light bulb jokesQ. How many system programmers does it take to change a lightbulb?A. Just one but they keep changing it until it returns zero.A. None they prefer an empty socket.A. Well you start with one but actually it waits for a child to do all of the work.GroanersWhy did the baby system programmer like their new colorful blankie? It was multithreaded.Why are your programs so fine and soft? I only use 400-thread-count or higher programs.Where do bad student shell processes go when they die? Forking Hell.Why are C programmers so messy? They store everything in one big heap.System Programmer (Definition)A system programmer is…Someone who knows sleepsort is a bad idea but still dreams of an excuse to use it.Someone who never lets their code deadlock… but when it does, causes more problems than everyone else combined.Someone who believes zombies are real.Someone who doesn’t trust their process to run correctly without testing with the same data, kernel, compiler, RAM, filesystem size,file system format, disk brand, core count, CPU load, weather, magnetic flux, orientation, pixie dust, horoscope sign, wall color, wall gloss and reflectance, motherboard, vibration, illumination, backup battery, time of day, temperature, humidity, lunar position, sun-moon, co-position…A system program …Evolves until it can send email.Evolves until it has the potential to create, connect and kill other programs and consume all possible CPU,memory,network,… resources on all possible devices but chooses not to. Today."
  },{
    "title": "Background",
    "url": " /coursebook/Background",
   "content": "  Background          Systems Architecture                  Assembly          Atomic Operations          Caching          Interrupts          Extra: Out of order instructions          Optional: Hyperthreading                    Debugging and Environments                  ssh          git          Editors          clean code          Asserts                    Valgrind                  TSAN                    GDB                  Setting breakpoints programmatically          Checking memory content          Undefined Behavior Sanitizer          Clang Static Build Tools          strace and ltrace          printfs                    Homework 0                  So you want to master System Programming? And get a better grade than B?          Watch the videos and write up your answers to the following questions          Chapter 1          Chapter 2          Chapter 3          Chapter 4          Chapter 5          C Development          Optional: Just for fun                    UIUC Specific Guidlines                  Piazza                    [1][]  BackgroundSystems ArchitectureThis section is a short review of System Architecture topics that you’ll need for System Programming.AssemblyWhat is assembly? Assumbly is the lowest that you’ll get to machine language without writing 1’s and 0’s. Each computer has an architecture, and that architecture has an associated assembly language. Each assembly command has a 1:1 mapping to a set of 1’s and 0’s that tell the computer exactly what to do. For example the following in the widely used x86 Assembly language add one to the memory address 20.add BYTE PTR [0x20], 1Why do we mention this? Because it is important that although you are going to be doing most of this class in C That this is what the code is translated into. Serious implications arise for race conditions and atomic operations.Atomic OperationsAn operation is atomic if no other processor should interrupt it. Take for example the above assembly code to add one to a register. In the architecture, it may actually have a few different steps on the circuit. The operation may start by fetching the value of the memory from the stick of ram, then storing it in the cache or a register, and then finally writing back. The problem comes in if two processors try to do it at the same time. The two processors could (at the same time) copy the value of the memory address, add one, and store the same result back. Resulting in the value only being incremented once. That is why we have a special set of instructions on modern systems called atomic operations. If an instruction is atomic, it makes sure that only one processor or thread performs any intermediate step at a time. With x86 this is done by the lock prefix.lock add BYTE PTR [0x20], 1Why don’t we do this for everything? It makes commands slower! If every time a computer does something it has to make sure that the other cores or processors aren’t doing anything, it’ll be much slower. Most of the time we differentiate these.CachingAh yes Caching. One of computer science’s greatest problems. Caching that we are referring is processor caching. Whenever you do a memory access if the particular address is already in the cache then the processor will perform the operation on the cache (such as adding) and update the actual memory later because updating memory is slow. If it isn’t, the processor requests a chunk of memory from the the memory chip and stores it in cache, kicking out the least recently used page (roughly). This is done because the processor cache is roughly ten times faster to reach than the memory in terms of time. Naturally this leads to problems because there are two different copies of the same value. This isn’t a class about caching, just know how this could impact your code.InterruptsInterrupts are a very important part of system programming. An interrupt is internally an electrical signal that is delivered to the processor when something happens – this is hardware interrupt. Then the hardware decides if this is something that it should handle (i.e. handling keyboard or mouse input for older keyboard and mouses) or it should pass to the operating system. The operating system then decides if this is something that it should handle (i.e. paging a memory table from disk) or something the application should handle (i.e. a SEGFAULT). The application then decides if it is an error (SEGFAULT) or not (SIGPIPE for example) and reports to the user. Applications can also send signals to the kernel and to the hardware as well.An important application of this is this is how system calls are served! There is a well established set of registers that the arguments go in according to the kernel as well as a system call “number” again defined by the kernel. Then the operating system triggers an interrupt which the kernel catches and serves the system call.Extra: Out of order instructionsFor those of you who have and haven’t taken the systems architecture course, you may not have heard about out of order instruction execution. Out of order execution is an amazing development by processors recently. Processors now instead of executing a sequence of instructions (let’s say assigning a variable and then another variable) execute instructions before the current one is done. This is because modern processors spend a lot of time waiting for memory accesses and other I/O driven applications. This means that a processor while it is waiting for an operation to complete will execute the next few operations. If any of the operations would possibly alter the previous operation or there is a barrier, the processor will not reorder the instructions.This naturally has allowed CPUs to become more energy efficient while executing more instructions in real time but naturally comes with security risks and more complex architectures. Where system programmers are worried is that (this is discussed in later chapters) operationg with mutex locks among threads are out of order – meaning that a software implementation of a mutex will not work without copious memory barriers. Basically, the programmer has to adopt the mental model of updates may not be seen among a series of threads without a barrier on modern processors.One of the most prominent bugs with respect to this is Spectre. Spectre is a bug where instructions that otherwise wouldn’t be executed are due to out of order instruction execution. Here is an example  char *a[10];   for (int i = 10; i != 1; --i) {    a[i] = calloc(1, 1);  }  a[0] = 0xCAFE;  int val;  int j = 10; // This will be in a register  int i = 10; // This will be in main memory  for (int i = 10; i != 0; --i, --j) {    if (i) {        val = *a[j];    }  }Let’s anaylze this code. The first loop allocates 9 elements through a valid malloc. The last element is 0xCAFE, meaning a dereference should result in a segfault. For the first 9 iterations, the branch is taken and the second branch in the dereference (the question mark) also returns the first value. The interesting part happens in the last iteration. The resulting behavior of the program is to skip the last iteration and val never gets asisgned the last previous value.But under the right compilation conditions and compiler flags, the instructions will get out of order executed. The processor thinks that the branch will be taken – it has been taken in the last 9 iterations. As such the processor will load that code up. Due to out of order instruction execution, while the value of i is being fetched from memory – we have to force it not to be in a register – the processor will try to dereference that address. Naturally that should result in a segfault, but since the address was never logically reached by the program the result is just discarded.Now here is the trick. Even though the value of the calculation would’ve resulted in a segfault, the bug doesn’t clear the cache that refers to the physical memory where 0xCAFE is located (this not an exact explanation, but essentially how it works) Since it is still in the cache, if you again trick the processor to read form the cache using val then you will read a memory value that you wouldn’t be able to read normally. This includes important information like passwords, payment info etc etc.Optional: HyperthreadingHyperthreading is a new technology and is in no way shape or form multithreading. Hyperthreading is allowing one physical core appear as many virtual cores to the operating system The operating system can then schedule processes on these virtual cores and one core will execute them. Each core interleaves processes. While the core is waiting for one memory access to complete, it may perform a few instructions of another process. The overal result is more instructions executed in a shorter time. This is also potentially means that you can divide the number of cores you need to power smaller devices.There be dragons here though. With hyperthreading, you must be wary of optimizations. A famous hyperthreading bug that caused programs to crash if at least two processes was scheduled on a physical core, using specific registers, in a tight loop. The actual problem is better explained through an architecture lens. But, the actual application was found through systems programmers working on OCaml’s mainline. http://gallium.inria.fr/blog/intel-skylake-bug/Debugging and EnvironmentsI’m going to tell you a secret about this course: it is about working smarter not harder. The course can be time consuming but the reason that so many people see it as such (and why so many student’s don’t see it as such) is the relative familiarity of people with their tools. Let’s go through some of the common tools that you’ll be working on and need to be familiar with.sshssh is short for the Secure Shell. It is a network protocol that allows you to spawn a shell on a remote machine. Most of the times in this class you will need to ssh into your VM like this&gt; ssh netid@sem-cs241-VM.cs.illinois.eduIf you don’t want to type your password out every time, you can generate an ssh key that uniquely identifies your machine. If you already have a key pair, you can skip to the copy id stage.  &gt; ssh-keygen -t rsa -b 4096  # Do whatever keygen tells you  # Don't feel like you need a passcode if your login password is secure  &gt; ssh-copy-id netid@sem-cs241-VM.cs.illinois.edu  # Enter your password for maybe the final time  &gt; ssh  netid@sem-cs241-VM.cs.illinois.eduIf you still think that that is too much typing, you can always alias hosts. You may need to restart your VM or reload sshd for this to take effect. The config file is available on linux and mac distros. For windows you’ll have to use the Windows Linux Subsystem or configure any aliases in PuTTY&gt; cat ~/.ssh/configHost vm  User          netid  HostName      sem-cs241-VM.cs.illinois.edu&gt; ssh vmgitWhat is ‘git‘? Git is a version control system. What that means is git stores the entire history of a directory. We refer to the directory as a repository. So what do you need to know is a few things. First, create your repository with the repo creator. If you haven’t already signed into enterprise github, make sure to do so otherwise your repository won’t be created for you. After that, that means your repository is created on the server. Git is a decentralized version control system, meaning that you’ll need to get a repository onto your VM. We can do this with a clone. Whatever you do, do not go through the README.md tutorial.$ git clone https://github-dev.cs.illinois.edu/cs241-fa18/&lt;netid&gt;.gitThis will create a local repsitory. The workflow is you make a change on your local repository, add the changes to a current commit, actually commit, and push the changes to the server.$ # edit the file, maybe using vim$ git add &lt;file&gt;$ git commit -m \"Committing my file\"$ git push origin masterNow to explain git well, you need to understand that git for our purposes will look like a linked list. You will always be at the head of master, and you will do the edit-add-commit-push loop. We have a separate branch on github that we push feedback to under a specific branch which you can view on github website. The markdown file will have information on the test cases and results (like standard out).Every so often git can break. Here are a list of commands you probably won’t need to fix your repo      git-cherry-pick        git-pack        git-gc        git-clean        git-rebase        git-stash/git-apply/git-pop        git-branch  In addition if you are currently on a branch, and you don’t see either$ git statusOn branch masterYour branch is up-to-date with 'origin/master'.nothing to commit, working directory cleanor$ git statusOn branch masterYour branch is up-to-date with 'origin/master'.Changes not staged for commit:  (use \"git add &lt;file&gt;...\" to update what will be committed)  (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory)        modified:   &lt;FILE&gt;        ...no changes added to commit (use \"git add\" and/or \"git commit -a\")And something like$ git statusHEAD detached at 4bc4426nothing to commit, working directory cleanDon’t panic, but your repository may be in an unworkable state. If you aren’t nearing a deadline, come in to office hours or ask your question on Piazza, and we’d be happy to help. In an emergency scenario, delete your repository and re-clone (you’ll have to add the release as above). This will lose any local uncommitted changes. Make sure to copy any files you were working on outside the directory, remove and copy them back inEditorsSome people take this as an opportunity to learn a new editor, others not so much. The first part is those of you who want to learn a new editor. In the editor war that spans decades, we have come to the battle of vim vs emacs.Vim is a text editor and unix-like utilty. You enter vim by typing vim &lt;file&gt;. This takes you into the editor. You start off in normal mode. In this mode you can move around with many keys with the most common ones being jklh. To exit vim from this mode, you need to type :q which quits. If you have any unsaved edits, you must either save them :w, save and quit :wq, or discard changes :q!. To make edits you can either type i to change you into insert mode or a to change to insert mode after the cursor. This is the very basics when it comes to vimEmacs is more of a way of life, and I don’t mean that figuratively. A lot people when they switch to the emacs environment realize that it is a powerful operating system lacking a decent text editor. This means emacs can house a terminal, gdb session, ssh session, code and a whole lot more. It would not be fitting any other way to introduce you the gnu-emacs any other way than the gnu-docs . Just note that emacs is insanely powerful. You can do almost anything with it. There are a fair number of students who like the IDE-aspect of other programming languages, just know you can set up emacs to be an IDE, but you have to learn a bit of lisp .Then there are those of you who like to use your own editors. That is completely fine. For this we require sshfs which has ports on many different machines      Windows        Mac        Linux  At that point the files on your VM are synced with the files on your machine and edits can be made and will be sync’edAt the time of writing, Bhuvy (That’s me!) really likes to use spacemacs which marries both vim and emacs and both of their difficulties. I’ll give my soapbox for why I like it, but be warned that if you are starting from absolutely no vim or emacs experience the learning curve along with this course may be too much.      Extensible. Spacemacs has a clean design written in lisp. There are 100s of packages ready to be installed by editing your spacemacs config and reloading that do everything from syntax checking, automatic static analyzing, etc etc.        Most of the good parts from vim and emacs. Emacs is really good at doing everything buy being a fast editor. Vim is really good at making fast edits and moving around. Spacemacs is the best of both worlds allowing vim keybindings with all the emacs goodness underneath.        Lots of preconfiguration done. As opposed with a fresh emacs install, a lot of the configurations with langauge and projects are done for you like neotree, helm, various language layers. All they you have to do is navigate neotree to the base of your project and emacs will turn into an IDE for that programming language.  clean codeMake your code modular using helper functions. If there is a repeated task (getting the pointers to contiguous blocks in the malloc MP, for example), make them helper functions. And make sure each function does one thing very well, so that you don’t have to debug twice. Let’s say that we are doing selection sort by finding the minimum element each iteration like so,void selection_sort(int *a, long len){     for(long i = len-1; i &gt; 0; --i){         long max_index = i;         for(long j = len-1; j &gt;= 0; --j){             if(a[max_index] &lt; a[j]){                  max_index = j;             }         }         int temp = a[i];         a[i] = a[max_index];         a[max_index] = temp;     }}Many can see the bug in the code, but it can help to refactor the above method intolong max_index(int *a, long start, long end);void swap(int *a, long idx1, long idx2);void selection_sort(int *a, long len);And the error is specifically in one function. In the end, we are not a class about refactoring/debugging your code. In fact, most kernel code is so atrocious that you don’t want to read it. But for the sake of debugging, it may benefit you in the long run to adopt some of these practices.AssertsUse assertions to make sure your code works up to a certain point – and importantly, to make sure you don’t break it later. For example, if your data structure is a doubly linked list, you can do something like assert(node == node-&gt;next-&gt;prev) to assert that the next node has a pointer to the current node. You can also check the pointer is pointing to an expected range of memory address, not null, -&gt;size is reasonable etc. The NDEBUG macro will disable all assertions, so don’t forget to set that once you finish debugging. assert linkHere’s a quick example with assert. Let’s say that I’m writing code using memcpyassert(!(src &lt; dest+n &amp;&amp; dest &lt; src+n)); //Checks overlapmemcpy(dest, src, n);This check can be turned off at compile time, but will save you tons of trouble debugging!ValgrindValgrind is a suite of tools designed to provide debugging and profiling tools to make your programs more correct and detect some runtime issues. The most used of these tools is Memcheck, which can detect many memory-related errors that are common in C and C++ programs and that can lead to crashes and unpredictable behaviour (for example, unfreed memory buffers). To run Valgrind on your program:valgrind --leak-check=full --show-leak-kinds=all myprogram arg1 arg2Arguments are optional and the default tool that will run is Memcheck. The output will be presented in form of number of allocations, number of freed allocations, and the number of errors.Suppose we have a simple program like this:#include &lt;stdlib.h&gt;void dummy_function() {    int* x = malloc(10 * sizeof(int));    x[10] = 0;        // error 1:as you can see here we write to an out of bound memory address}                    // error 2: memory leak the allocated x not freedint main(void) {    dummy_function();    return 0;}This program compiles and run with no errors. Let’s see what Valgrind will output.==29515== Memcheck, a memory error detector==29515== Copyright (C) 2002-2015, and GNU GPL'd, by Julian Seward et al.==29515== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright info==29515== Command: ./a==29515== ==29515== Invalid write of size 4==29515==    at 0x400544: dummy_function (in /home/rafi/projects/exocpp/a)==29515==    by 0x40055A: main (in /home/rafi/projects/exocpp/a)==29515==  Address 0x5203068 is 0 bytes after a block of size 40 alloc'd==29515==    at 0x4C2DB8F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)==29515==    by 0x400537: dummy_function (in /home/rafi/projects/exocpp/a)==29515==    by 0x40055A: main (in /home/rafi/projects/exocpp/a)==29515== ==29515== ==29515== HEAP SUMMARY:==29515==     in use at exit: 40 bytes in 1 blocks==29515==   total heap usage: 1 allocs, 0 frees, 40 bytes allocated==29515== ==29515== LEAK SUMMARY:==29515==    definitely lost: 40 bytes in 1 blocks==29515==    indirectly lost: 0 bytes in 0 blocks==29515==      possibly lost: 0 bytes in 0 blocks==29515==    still reachable: 0 bytes in 0 blocks==29515==         suppressed: 0 bytes in 0 blocks==29515== Rerun with --leak-check=full to see details of leaked memory==29515== ==29515== For counts of detected and suppressed errors, rerun with: -v==29515== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)Invalid write: It detected our heap block overrun, writing outside of allocated block.Definitely lost: Memory leak — you probably forgot to free a memory block.Valgrind is a very effective tool to check for errors at runtime. C is very special when it comes to such behavior, so after compiling your program you can use Valgrind to fix errors that your compiler may not catch and that usually happen when your program is running.For more information, you can refer to the valgrind website.TSANThreadSanitizer is a tool from Google, built into clang and gcc, to help you detect race conditions in your code. For more information about the tool, see the Github wiki. Note, that running with tsan will slow your code down a bit. Consider the following code.#include &lt;pthread.h&gt;#include &lt;stdio.h&gt;int global;void *Thread1(void *x) {    global++;    return NULL;}int main() {    pthread_t t[2];    pthread_create(&amp;t[0], NULL, Thread1, NULL);    global = 100;    pthread_join(t[0], NULL);}// compile with gcc -fsanitize=thread -pie -fPIC -ltsan -g simple_race.cWe can see that there is a race condition on the variable global. Both the main thread and the thread created with will try to change the value at the same time. But, does ThreadSantizer catch it?$ ./a.out==================WARNING: ThreadSanitizer: data race (pid=28888)  Read of size 4 at 0x7f73ed91c078 by thread T1:    #0 Thread1 /home/zmick2/simple_race.c:7 (exe+0x000000000a50)    #1  :0 (libtsan.so.0+0x00000001b459)  Previous write of size 4 at 0x7f73ed91c078 by main thread:    #0 main /home/zmick2/simple_race.c:14 (exe+0x000000000ac8)  Thread T1 (tid=28889, running) created by main thread at:    #0  :0 (libtsan.so.0+0x00000001f6ab)    #1 main /home/zmick2/simple_race.c:13 (exe+0x000000000ab8)SUMMARY: ThreadSanitizer: data race /home/zmick2/simple_race.c:7 Thread1==================ThreadSanitizer: reported 1 warningsIf we compiled with the debug flag, then it would give us the variable name as well.GDBIntroduction to gdbSetting breakpoints programmaticallyA very useful trick when debugging complex C programs with GDB is setting breakpoints in the source code.int main() {    int val = 1;    val = 42;    asm(\"int $3\"); // set a breakpoint here    val = 7;}$ gcc main.c -g -o main &amp;&amp; ./main(gdb) r[...]Program received signal SIGTRAP, Trace/breakpoint trap.main () at main.c:66     val = 7;(gdb) p val$1 = 42Checking memory contentMemory ContentFor example,int main() {    char bad_string[3] = {'C', 'a', 't'};    printf(\"%s\", bad_string);}$ gcc main.c -g -o main &amp;&amp; ./main$ Cat ZVQ� $(gdb) l1 #include &lt;stdio.h&gt;2 int main() {3     char bad_string[3] = {'C', 'a', 't'};4     printf(\"%s\", bad_string);5 }(gdb) b 4Breakpoint 1 at 0x100000f57: file main.c, line 4.(gdb) r[...]Breakpoint 1, main () at main.c:44     printf(\"%s\", bad_string);(gdb) x/16xb bad_string0x7fff5fbff9cd: 0x63  0x61  0x74  0xe0  0xf9  0xbf  0x5f  0xff0x7fff5fbff9d5: 0x7f  0x00  0x00  0xfd  0xb5  0x23  0x89  0xff(gdb)Here, by using the x command with parameters 16xb, we can see that starting at memory address 0x7fff5fbff9c (value of bad_string), printf would actually see the following sequence of bytes as a string because we provided a malformed string without a null terminator.What do you actually use to run your program? A shell! A shell is a programming language that is running inside your terminal. A terminal is merely a window to input commands. Now, on POSIX we usually have one shell called sh that is linked to a POSIX compliant shell called dash. Most of the time, you use a shell called bash that is not entirely POSIX compliant but has some nifty built in features. If you want to be even more advanced, zsh has some more powerful features like tab complete on programs and fuzzy patterns, but that is neither here nor there.Undefined Behavior SanitizerThe undefined behavior sanitizer is a wonderful tool provided by the llvm project. It allows you to compile code witha runtime checker to make sure that you don’t do undefined behavior for various categories. We will try to include it into our projects, but requires support form all the external libraries that we use so we may not get around to all of them.Undefined behavior - why we can’t solve it in generalAlso please please read Chris Lattner’s 3 Part blog post on undefined behavior. It can shed light on debug builds and the mystery of compiler optimization.Clang Static Build Toolsstrace and ltraceprintfsWhen all else fails, print! Each of your functions should have an idea of what it is going to do (ie find_min better find the minimum element). You want to test that each of your functions is doing what it set out to do and see exactly where your code breaks. In the case with race conditions, tsan may be able to help, but having each thread print out data at certain times could help you identify the race condition.To make printfs useful, try to have a macro that fills in the context by which the printf was called – a log statement if you will. A simple useful but untested log statement could be as follows. Try to make a test and figure out something that is going wrong, then log the state of your variables.  #include &lt;execinfo.h&gt;  #include &lt;stdio.h&gt;  #include &lt;stdlib.h&gt;  #include &lt;stdarg.h&gt;  #include &lt;unistd.h&gt;    // bt is print backtrace  const int num_stack = 10;  int __log(int line, const char *file, int bt, const char *fmt, ...) {    if (bt) {      void *raw_trace[num_stack];      size_t size = backtrace(raw_trace, sizeof(raw_trace) / sizeof(raw_trace[0]));      char **syms = backtrace_symbols(raw_trace, size);      for(ssize_t i = 0; i &lt; size; i++) {        fprintf(stderr, \"|%s:%d| %s\\n\", file, line, syms[i]);      }      free(syms);     }    int ret = fprintf(stderr, \"|%s:%d| \", file, line);    va_list args;    va_start(args, fmt);     ret += vfprintf(stderr, fmt, args);    va_end(args);    ret += fprintf(stderr, \"\\n\");    return ret;  }  #ifdef DEBUG  #define log(...) __log(__LINE__, __FILE__, 0, __VA_ARGS__)  #define bt(...) __log(__LINE__, __FILE__, 1, __VA_ARGS__)  #else  #define log(...)  #define bt(...)  #endif  //Use as log(args like printf) or bt(args like printf) to either log or get backtrace  int main() {    log(\"Hello Log\");    bt(\"Hello Backtrace\");  }And then use as such.Homework 0// First, can you guess which lyrics have been transformed into this C-like system code?char q[] = \"Do you wanna build a C99 program?\";#define or \"go debugging with gdb?\"static unsigned int i = sizeof(or) != strlen(or);char* ptr = \"lathe\"; size_t come = fprintf(stdout,\"%s door\", ptr+2);int away = ! (int) * \"\";int* shared = mmap(NULL, sizeof(int*), PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0);munmap(shared,sizeof(int*));if(!fork()) { execlp(\"man\",\"man\",\"-3\",\"ftell\", (char*)0); perror(\"failed\"); }if(!fork()) { execlp(\"make\",\"make\", \"snowman\", (char*)0); execlp(\"make\",\"make\", (char*)0)); }exit(0);So you want to master System Programming? And get a better grade than B?int main(int argc, char** argv) {    puts(\"Great! We have plenty of useful resources for you, but it's up to you to\");    puts(\" be an active learner and learn how to solve problems and debug code.\");    puts(\"Bring your near-completed answers the problems below\");    puts(\" to the first lab to show that you've been working on this.\");    printf(\"A few \\\"don't knows\\\" or \\\"unsure\\\" is fine for lab 1.\\n\");     puts(\"Warning: you and your peers will work hard in this class.\");    puts(\"This is not CS225; you will be pushed much harder to\");    puts(\" work things out on your own.\");    fprintf(stdout,\"This homework is a stepping stone to all future assignments.\\n\");    char p[] = \"So, you will want to clear up any confusions or misconceptions.\\n\";    write(1, p, strlen(p) );    char buffer[1024];    sprintf(buffer,\"For grading purposes, this homework 0 will be graded as part of your lab %d work.\\n\", 1);    write(1, buffer, strlen(buffer));    printf(\"Press Return to continue\\n\");    read(0, buffer, sizeof(buffer));    return 0;}Watch the videos and write up your answers to the following questionsImportant!The virtual machine-in-your-browser and the videos you need for HW0 are here:http://cs-education.github.io/sys/Questions? Comments? Use the current semester’s CS241 Piazza: https://piazza.com/The in-browser virtual machine runs entirely in Javascript and is fastest in Chrome. Note the VM and any code you write is reset when you reload the page, so copy your code to a separate document. The post-video challenges are not part of homework 0 but you learn the most by doing rather than just passively watching - so we suggest you have some fun with each end-of-video challenge.HW0 questions are below. Copy your answers into a text document sd you’ll need to submit them later in the course.Chapter 1In which our intrepid hero battles standard out, standard error, file descriptors and writing to files      Hello, World! (system call style) Write a program that uses write() to print out “Hi! My name is &lt;Your Name&gt;”.        Hello, Standard Error Stream! Write a function to print out a triangle of height n to standard error. Your function should have the signature void write_triangle(int n) and should use write(). The triangle should look like this, for n = 3:    ******            Writing to files Take your program from “Hello, World!” modify it write to a file called hello_world.txt. Make sure to to use correct flags and a correct mode for open() (man 2 open is your friend).        Not everything is a system call Take your program from “Writing to files” and replace write() with printf(). Make sure to print to the file instead of standard out!        What are some differences between write() and printf()?  Chapter 2Sizing up C types and their limits, int and char arrays, and incrementing pointers      How many bits are there in a byte?        How many bytes are there in a char?        How many bytes the following are on your machine? int, double, float, long, and long long        On a machine with 8 byte integers, the declaration for the variable data is int data[8]. If the address of data is 0x7fbd9d40, then what is the address of data+2?        What is data[3] equivalent to in C? Hint: what does C convert data[3] to before dereferencing the address? Remember, the type of a string constant abc is an array.        Why does this segfault?    char *ptr = \"hello\";*ptr = 'J';            What is the value of the variable str_size?    ssize_t str_size = sizeof(\"Hello\\0World\")            What is the value of the variable str_len    ssize_t str_len = strlen(\"Hello\\0World\")            Give an example of X such that sizeof(X) is 3.        Give an example of Y such that sizeof(Y) might be 4 or 8 depending on the machine.  Chapter 3Program arguments, environment variables, and working with character arrays (strings)      What are two ways to find the length of argv?        What does argv[0] represent?        Where are the pointers to environment variables stored (on the stack, the heap, somewhere else)?        On a machine where pointers are 8 bytes, and with the following code:    char *ptr = \"Hello\";char array[] = \"Hello\";        What are the values of sizeof(ptr) and sizeof(array)? Why?        What data structure manages the lifetime of automatic variables?  Chapter 4Heap and stack memory, and working with structs      If I want to use data after the lifetime of the function it was created in ends, where should I put it? How do I put it there?        What are the differences between heap and stack memory?        Are there other kinds of memory in a process?        Fill in the blank: “In a good C program, for every malloc, there is a ___”.        What is one reason malloc can fail?        What are some differences between time() and ctime()?        What is wrong with this code snippet?    free(ptr);free(ptr);            What is wrong with this code snippet?    free(ptr);printf(\"%s\\n\", ptr);            How can one avoid the previous two mistakes?        Create a struct that represents a Person. Then make a typedef, so that struct Person can be replaced with a single word. A person should contain the following information: their name (a string), their age (an integer), and a list of their friends (stored as a pointer to an array of pointers to Persons).        Now, make two persons on the heap, “Agent Smith” and “Sonny Moore”, who are 128 and 256 years old respectively and are friends with each other. Create functions to create and destroy a Person (Person’s and their names should live on the heap).        create() should take a name and age. The name should be copied onto the heap. Use malloc to reserve sufficient memory for everyone having up to ten friends. Be sure initialize all fields (why?).        destroy() should free up not only the memory of the person struct, but also free all of its attributes that are stored on the heap. Destroying one person should not destroy any others.  Chapter 5Text input and output and parsing using getchar, gets, and getline.      What functions can be used for getting characters from stdin and writing them to stdout?        Name one issue with gets().        Write code that parses the string “Hello 5 World” and initializes 3 variables to “Hello”, 5, and “World”.        What does one need to define before including getline()?        Write a C program to print out the content of a file line-by-line using getline().  C DevelopmentThese are general tips for compiling and developing using a compiler and git. Some web searches will be useful here      What compiler flag is used to generate a debug build?        You fix a problem in the Makefile and type make again. Explain why this may be insufficient to generate a new build.        Are tabs or spaces used to indent the commands after the rule in a Makefile?        What does git commit do? What’s a sha in the context of git?        What does git log show you?        What does git status tell you and how would the contents of .gitignore change its output?        What does git push do? Why is it not just sufficient to commit with git commit -m ’fixed all bugs’ ?        What does a non-fast-forward error git push reject mean? What is the most common way of dealing with this?  Optional: Just for fun      Convert your a song lyrics into System Programming and C code covered in this wiki book and share on Piazza.        Find, in your opinion, the best and worst C code on the web and post the link to Piazza.        Write a short C program with a deliberate subtle C bug and post it on Piazza to see if others can spot your bug.        Do you have any cool/disastrous system programming bugs you’ve heard about? Feel free to share with your peers and the course staff on piazza.  UIUC Specific GuidlinesPiazzaTAs and student assistants get a ton of questions. Some are well-researched, and some…are not. This is a handy guide that’ll help you move away from the latter and towards the former. Oh, and did I mention that this is an easy way to score points with your internship managers? Ask yourself…      Am I running on EWS?        Did I check the man pages?        Have I searched for similar questions/followups on Piazza?        Have I read the MP/DS specification completely?        Have I watched all of the videos?        Did I Google the error message and a few permutations thereof if necessary?        Did I try commenting out, printlining, and/or stepping through parts of the code bit by bit to find out precisely where the error occurs?        Did I commit my code to git in case the TAs need more context?        Did I include the console/GDB/Valgrind output **AND** code surrounding the bug in my Piazza post?        Have I fixed other segmentation faults not related to the issue I’m having?        Am I following good programming practice? (i.e. encapsulation, functions to limit repetition, etc)  "
  },{
    "title": "Deadlock",
    "url": " /coursebook/Deadlock",
   "content": "  Deadlock          Resource Allocation Graphs      Coffman conditions      Approaches to solving deadlock                  Extra: Banker’s Algorithm                    Dining Philosophers                  Failed Solutions                    Viable Solutions                  Leaving the Table (Stallings’ Solution)          Partial Ordering (Dijkstra’s Solution)          Extra: Clean/Dirty Forks (Chandra/Misra Solution)          Advanced Solutions: Actor Model (other Message passing models)                    Topics      Questions      [1][]  DeadlockDeadlock is defined as when a system cannot make and forward progress. We define a system for the rest of the chapter as a set of rules by which a set of processes can move from one state to another, where a state is either working or waiting for a particular resource. Forward progress is defined as if there is at least one process wokring or we can award a process waiting for a resource that resource. In a lot of systems, Deadlock is just avoided by ignore the entire concept . Have you heard about turn it on and off again? For products where the stakes are low (User Operating Systems, Phones), it may be more efficient not prevent deadlock. But in the cases where “failure is not an option” - Apollo 13, you need a system that tracks deadlock and breaks it or better yet prevents it entirely. Apollo 13 may have not failed because of deadlock, but probably wouldn’t be good to restart the system on liftoff.Mission critical operating systems need this guarentee formally because playing the odds with people’s lives isn’t a good idea. Okay so how do we do this? We model the problem. Even though it is a common statistical phrase that all models are wrong, the more accurate the model is to the system the better chance that it’ll work better.Resource Allocation Graphs[13]r.35One such way is modeling the system with a resource allocation graph (RAG). A resource allocation graph tracks which resource is held by which process and which process is waiting for a resource of a particular type. It is very powerful and simple tool to illustrate how interacting processes can deadlock. If a process is using a resource, an arrow is drawn from the resource node to the process node. If a process is requesting a resource, an arrow is drawn from the process node to the resource node. If there is a cycle in the Resource Allocation Graph and each resource in the cycle provides only one instance, then the processes will deadlock. For example, if process 1 holds resource A, process 2 holds resource B and process 1 is waiting for B and process 2 is waiting for A, then process 1 and 2 process will be deadlocked [ragfigure].[12]r.35Assume that the processes ask for exclusive access to the file. If you have a bunch of processes running and they request resources and the operating system ends up in this state, you deadlock! You may not see this because the operating system may preempt some processes breaking the cycle but there is still a change that your three lonely processes could deadlock. You can also make these kind of graphs with make and rule dependencies with our parmake MP for example.Coffman conditionsThere are four necessary and sufficient conditions for deadlock – meaning if these conditions hold then there is a non-zero probability that the system will deadlock at any given iteration. These are known as the Coffman Conditions .      Mutual Exclusion: no two processes can obtain a resource at the same time.        Circular Wait: there exists a cycle in the Resource Allocation Graph, or there exists a set of processes {P1,P2,…} such that P1 is waiting for resources held by P2, which is waiting for P3,…, which is waiting for P1.        Hold and Wait: a process once obtaining a resource does not let go.        No pre-emption: nothing can force the process to give up a resource.  Deadlock can happen if and only if the four coffman conditions are satisified.(\\rightarrow) If the system is deadlocked, the four coffman conditions are apparent.      For the purposes of contradiction, assume that there is no cicular wait. If not then that means the resource allocation graph is acyclic, meaning that there is at least one one process that is not waiting on any resource to be freed. Since the system can move forward, the system is not deadlocked.        For the purposes of contradiction, assume that there is no mutual exclusion. If not, that means that no process is waiting on any other process for a resource. This breaks circular wait and the previous argument proves correctness.        For the purposes of contradiction, assume that processes don’t hold and wait but our system still deadlocks. Since we have circular wait from the first condition at least one process must be waiting on another process. If that and processes don’t hold and wait, that means one process must let go of a resource. Since the system has moved forward, it cannot be deadlocked.        For the purposes of contradiction, assume that we have preemption, but the system cannot be un-deadlocked. Have one process, or create one process, that recognizes the circular wait that must be apprent from above and break on the of the links. By the first branch, we must not have deadlock.  (\\leftarrow) If the four conditions are apparent, the system is deadlocked. We will prove that if the system is not deadlocked, the four conditions are not apparent. Though this proof is not formal, let us build a system with the three requirements not including circular wait. Let assume that there is a set of processes (P = {p_1, p_2, …, p_n}) and there is a set of resources (R = {r_1, r_2, …, r_m}). For simplicity, a process can only request one resource at a time but the proof can be generalized to multiple. Let assume that the system is at different states at different times (t). Let us assume that the state of the system is a tuple ((h_t, w_t)) where there are two function (h_t: R \\rightarrow P \\cup {\\text{unassigned}}) that maps resources to the processes that own them (this is a function, meaning that we have mutual exclusion) and or unassigned and (w_t: P \\rightarrow R \\cup {\\text{satisfied}}) that maps the requests that each process makes to a resource or if the process is satisfied. Let (L_t \\subseteq P \\times R) be a set of list of requests that a process uses to release a resource at any given time. The evolution of the system is at each step at every time.      Release all resources in (L_t).        Find a process that is requesting a resource        If that resource is available give it to that process, generating a new ((h_{t+1}, w_{t+1})) and exit the current iteration.        Else find another process and try the same resource allocation procedure in the previous step.  If all processes have been surveyed and none updates the system, consider it deadlocked. More formally, this system is deadlocked means if (\\exists t_0, \\forall t \\geq t_0, \\forall p \\in P, w_t(p) \\neq \\text{satisfied} \\text{ and } \\exists q, q \\neq p \\rightarrow h_t(w_t(p)) = q) (which is what we need to prove).These conditions imply deadlock. Deadlock for a system is defined as no work can be done now or later. Work can be done if a process is satisfied, or we can release a resource to a process. No process is satisfied by definition. Since the system can’t preempt and release resources (by no preemption), the processes have to do it themselves. If the processes has a resource, it will not let it go until after it is satisfied by hold and wait. All resources requested by the processes are owned by other processes meaning that no process will let any resource go. Since we have shown no processes will give up a resource and no process is satisfied, the system is in deadlock. Formalize SubproofThe last condition to address is circular wait. Circular wait means that there exists (\\forall p \\in P, w_t(p) \\neq \\text{satisfied} \\text{ and } \\exists q, q \\neq p \\rightarrow h_t(w_t(p)) = q). Which is what we needed to show.If you break any of them, you cannot have deadlock! Consider the scenario where two students need to write both pen and paper and there is only one of each. Breaking mutual exclusion means that the students share the pen and paper. Breaking circular wait could be that the students agree to grab the pen then the paper. As a proof by contradiction, say that deadlock occurs under the rule and the conditions. Without loss of generality, that means a student would have to be waiting on a pen while holding the paper and the other waiting on a pen and holding the paper. We have contradicted ourselves because one student grabbed the paper without grabbing the pen, so deadlock must not be able to occur. Breaking hold and wait could be that the students try to get the pen and then the paper and if a student fails to grab the paper then they release the pen. This introduces a new problem called livelock which will be discussed latter. Breaking preemption means that if the two students are in deadlock the teacher can come in and break up the deadlock by giving one of the students one the held on items or tell both students to put the items down.livelock relates to deadlock but it is not exactly deadlock. Consider the breaking hold and wait solution as above. Though deadlock is avoided, if we pick up the same device (a pen or the paper) again and again in the exact same pattern, neither of us will get any writing done. More generally, livelock happens when the process looks like it is executing but no meaningful work is done. Livelock is generally harder to detect because the processes generally look like they are working to the outside operating system wheras in deadlock the operating system generally knows when two processes are waiting on a system wide resource. Another problem is that there are nessisary conditions for livelock (i.e. deadlock does not occur) but not sufficient conditions – meaning there is no set of rules where livelock has to occur. You must formally prove in a system by what is known as an invariant. One has to enumerate each of the steps of a system and if each of the steps eventually (after some finite number of steps) leads to forward progress, the system is not livelocked. There are even better systems that prove bounded waits; a system can only be livelocked for at most (n) cycles which may be important for something like stock exchanges.Approaches to solving deadlockIgnoring deadlock is the most obvious approach that started the chapter out detailing. Quite humorously, the name for this approach is called the ostrich algorithm. Though there is no apparent source, the idea for the algorithm comes from the concept of an ostrich sticking its head in the sand. When the operating system detects deadlock, it does nothing out of the ordinary and hopes that the deadlock goes away. Now this is a slight misnomer because the operating system doesn’t do anything abnormal – it is not like an operating system deadlocks every few minutes because it runs  100 processes all requesting shared libraries. An operating system still preempts processes when stopping them for context switches. The operating system has the ability to interrupt any system call, potentially breaking a deadlock scenario. The OS also makes some files read-only thus making the resource shareable. What the algorithm refers to is that if there is an adversary that specifically crafts a program – or equivalently a user who poorly writes a program – that deadlock could not be caught by the operating system. For everyday life, this tends to be fine. When it is not we can turn to the following method.Deadlock detection allows the system to enter a deadlocked state. After entering, the system uses the information that it has to break deadlock. As an example, consider multiple processes accessing files. The operating system is able to keep track of all of the files/resources through file descriptors at some level either abstracted through an API or directly. If the operating system detects a directed cycle in the operating system file descriptor table it may break one process’ hold through scheduling for example and let the system proceed. Why this is a popular choice in this realm is that there is no way of knowing which resources a program will select without running the program. This is an extension of Rice’s theorem that says that we cannot know any semantic feature without running the program (semantic meaning like what files it tries to open). So theoretically, it is sound. The problem then gets introduced that we could reach a livelock scenario if we preempt a set of resources again and again. The way around this is mostly probabilistic. The operating system chooses a random resource to break hold and wait. Now even though a user can craft a program where breaking hold and wait on each resource will result in a livelock, this doesn’t happen as often on machines that run programs in practice or the livelock that does happen happens for a couple of cycles. These kind of systems are good for products that need to maintain a non-deadlocked state but can tolerate a small chance of livelock for a short period of time. The following proof is not required for our 241 related puproses but is included for concreteness.Deadlock prevention is making sure that deadlock cannot happen, meaning that you break a Coffman condition. This works the best inside a single program and the software engineer making the choice to break a certain coffman condition. Consider the Banker’s Algorithm . It is another algorithm for deadlock avoidance. The whole implementation is outside the scope of this class, just know that there are more generalized algorithms for operating systems.Extra: Banker’s AlgorithmThe banker algorithm is actually not too complicated. We can start out with the single resource solution. Let’s say that I’m a banker. As a banker I have a finite amount of money. As having a finite amount of money, I want to make loans and eventually get my money back. Let’s say that we have a set of (n) people where each of them have a set amount or a limit (a_i) ((i) being the (i)th process) that they need to obtain before they can do any work. I keep track in my book how much I’ve given to each person (l_i), and I have some amount of principle (p) at any given time. For people to request money, they do the following. Consider the state of the system ((A={a_1, a_2, …}, L={l_1, l_2, …}, p)). An assumption of this system is that we have (p &gt; \\inf a_i), or we have enough money to satisfy one person. Also, each person will work for a finite period of time and give back our money.      A person (j) requests (m) from me                  if (m &lt; p), they are denied.                    if (m + l_j &gt; a_i) they are denied                    Pretend we are in a new state ((A={…, a_j, …}, L={.., l_j + m, …}, p - m)) where the process is granted the resource.                  if now person (j) is either satisfied ((l_j == a_j)) or (\\exists i, a_j - l_j &lt; p). In other words we have enough money to satisfy one other person. If either, consider the transaction safe and give them the money.  Why does this work? Well at the start we are in a safe state – defined by we have enough money to satisfy at least one person. Each of these “loans” results in a safe state. If we have exhausted our reserve, one person is working and will give us money greater than or equal to our previous “loan”, thus putting us in a safe state again. Since we always have the ability to make one additional move the system can never deadlock. Now, there is no guarentee that the system won’t livelock. If the process we hope to request something never does, no work will be done – but not due to deadlock. This analogy expands to higher orders of magnitude but requires that either a process can do its work entirely or there exists a process whose combination of resources can be satisfied, which makes the algorithm a little more tricky (an additional for loop) but nothing too bad. There are a fair bit of downsides to this      The program first needs to know how much of each resource a process needs. A lot of times that is impossible or the process requests the wrong amount because the programmer didn’t forsee it.        The system could livelock.        We know in most systems that resources are generally not homogenous. Of course there are things like pipes and sockets but for the most part there is only 1 of a particular file. This could mean that the runtime of the algorithm could be slow for systems with millions of resources.        Also, this can’t keep track of resources that come and go. A process may delete a resource as a side effect or create a resource. The algorithm assumes a static allocation and that each process performs a non-destructive operation.  Dining PhilosophersThe Dining Philosophers problem is a classic synchronization problem. Imagine I invite (n) (let’s say 5) philosophers to a meal. We will sit them at a table with 5 chopsticks, one between each philosopher. A philosopher alternates between wanting to eat or think. To eat the philosopher must pick up the two chopsticks either side of their position. The original problem required each philosopher to have two forks, but one can eat with a single fork so we rule this out. However these chopsticks are shared with his neighbor.[10]r.3Is it possible to design an efficient solution such that all philosophers get to eat? Or, will some philosophers starve, never obtaining a second chopstick? Or will all of them deadlock? For example, imagine each guest picks up the chopstick on their left and then waits for the chopstick on their right to be free. Oops - our philosophers have deadlocked! Each of the philosophers are essentially the same, meaning that each philosopher has the same instruction set based on the other philosopher ie you can’t tell every even philosopher to do one thing and every odd philosopher to do another thing.Failed Solutionsvoid* philosopher(void* forks){     info phil_info = forks;     pthread_mutex_t* left_fork = phil_info-&gt;left_fork;     pthread_mutex_t* right_fork = phil_info-&gt;right_fork;     while(phil_info-&gt;simulation){          pthread_mutex_lock(left_fork);          pthread_mutex_lock(right_fork);          eat(left_fork, right_fork);          pthread_mutex_unlock(left_fork);          pthread_mutex_unlock(right_fork);     }}This looks good but. What if everyone picks up their left fork and is waiting on their right fork? We have deadlocked the program. It is important to note that deadlock doesn’t happen all the time and the probability that this solution deadlocks goes down as the number of philosophers goes up. What is really important to note is that eventually that this solution will deadlock, letting threads starve which is bad. So now you are thinking about breaking one of the coffman conditions. Let’s break Hold and Wait!void* philosopher(void* forks){     info phil_info = forks;     pthread_mutex_t* left_fork = phil_info-&gt;left_fork;     pthread_mutex_t* right_fork = phil_info-&gt;right_fork;     while(phil_info-&gt;simulation){          pthread_mutex_lock(left_fork);          pthread_mutex_lock(right_fork);          eat(left_fork, right_fork);          pthread_mutex_unlock(left_fork);          pthread_mutex_unlock(right_fork);     }}Now our philosopher picks up the left fork and tries to grab the right. If it’s available, they eat. If it’s not available, they put the left fork down and try again. No deadlock! But, there is a problem. What if all the philosophers pick up their left at the same time, try to grab their right, put their left down, pick up their left, try to grab their right…. We have now livelocked our solution! Our poor philosopher are still starving, so let’s give them some proper solutions.Viable SolutionsThe naive arbitrator solution is have one arbitrator (a mutex for example). Have each of the philosopher ask the arbitrator for permission to eat (i.e. trylock the mutex). This solution allows one philosopher to eat at a time. When they are done, another philosopher can ask for permission to eat. This prevents deadlock because there is no circular wait! No philosopher has to wait on any other philosopher. The advanced arbitrator solution is to implement a class that determines if the philosopher’s forks are in the arbitrator’s possession. If they are, they give them to the philosopher, let him eat, and take the forks back. This has the added bonus of being able to have multiple philosopher eat at the same time.There are a lot of problems with these solutions. One is that they are slow and have a single point of failure or the arbitrator. Assuming that all the philosophers are good-willed, the arbitrator needs to be fair and be able to determine if a transaction would cause deadlock in the multi-arbitrator case. Further more in practical systems, the arbitrator tends to give forks to the same processes because of scheduling or pseudorandomness. Another important thing to note is that this prevents deadlock for the entire system. But in our model of dining philosophers, the philosopher has to release the lock themselves. Then you can consider the case of the malicious philosopher (let’s say Decartes because of his Evil Demons) could hold on to the arbitrator forever. He would make forward progress and the system would make forward progress but there is no way of ensuring that each process makes forward progress without assuming something about the processes or having true preemption – meaning that a higher authority (let’s say Steve Jobs) tells them to stop eating forcibly.Prove arbitrator’s doesn’t deadlockLeaving the Table (Stallings’ Solution)Why does the first solution deadlock? Well there are (n) philosophers and (n) chopsticks. What if there is only 1 philsopher at the table? Can we deadlock? No. How about 2 philsophers? 3? … You can see where this is going. Stallings’ solutions says to remove philosophers from the table until deadlock is not possible – think about what the magic number of philosophers at the table. The way to do this in actual system is through semaphores and letting a certain number of philosopher through. This has the benefit that multiple philosophers can be eating.In the case that the philosophers aren’t evil, this solution requires a lot of time-consuming context switching. There is also no reliable way to know the number of resources before hand. In the dining philosophers case, this is solved because everything is known but trying to specify and operating system where you don’t know which file is going to get opened by what process leads you with a faulty solution. And again since semaphores are system constructs, they obey system timing clocks which means that the same processes tend to get added back into the queue again. Now if a philosopher becomes evil, then the problem becomes that there is no preemption. A philosopher can eat for as long as they want and the system will continue to function but that means the fairness of this solution can be low in the worst case. This works best with timeouts (or forced context switches) in order to ensure bounded wait times.Stallings’ Solution Doesn’t Deadlock.Let’s number the philosophers ({p_0, p_1, .., p_{n-1}}) and the resources ({r_0, r_1, .., r_{n-1}}). A philosopher (p_i) needs resource (r_{i-1 \\mod n}) and (r_{i + 1 \\mod n}). Without loss of generality, let us take (p_i) out of the picture. Each resource had exactly two philosophers that could use it. Now resources (r_{i-1 \\mod n}) and (r_{i + 1 \\mod n}) only have on philosopher waiting on it. Even if hold and wait, no preemption, and mutual exclusion or present, the resources can never enter a state where one philosopher requests them and they are held by another philosopher because only one philosopher can request them. Since there is no way to generate a cycle otherwise, circular wait cannot hold. Since circular wait cannot hold, deadlock cannot happen.Partial Ordering (Dijkstra’s Solution)This is Dijkstra’s solution . He was the one to propose this problem on an exam. Why does the first solution deadlock? Dijkstra thought that the last philosopher who picks up his left fork (causing the solution to deadlock) should pick up his right. He accomplishes it by number the forks (1..n), and tells each of the philosopher to pick up his lower number fork. Let’s run through the deadlock condition again. Everyone tries to pick up their lower number fork first. Philosopher (1) gets fork (1), Philosopher (2) gets fork (2), and so on until we get to Philosopher (n). They have to choose between fork (1) and (n). fork (1) is already held up by philosopher (1), so they can’t pick up that fork, meaning he won’t pick up fork (n). We have broken circular wait! Meaning deadlock isn’t possible.The problems to this is that an entity either needs to know the finite set of resources or be able to produce a consistent partial order suck that circular wait cannot happen. This also implies that there needs to be some entity, either the operating system or another process, deciding on the number and all of the philosophers need to agree on the number as new resources come in. As we have also see with previous solutions, this relies on context switching so this prioritizes philosophers that have already eaten but can be made more fair by introducing random sleeps and waits.Prove dijkstra’s doesn’t deadlockExtra: Clean/Dirty Forks (Chandra/Misra Solution)There are many more advanced solutions.Detail the Clean/Dirty Solution, and citeAdvanced Solutions: Actor Model (other Message passing models)Detail the Actor Model, and citeTopics      Coffman Conditions        Resource Allocation Graphs        Dining Philosophers        Failed DP Solutions        Livelocking DP Solutions        Working DP Solutions: Benefits/Drawbacks  Questions      What are the Coffman Conditions?        What do each of the Coffman conditions mean? (e.g. can you provide a definition of each one)        Give a real life example of breaking each Coffman condition in turn. A situation to consider: Painters, Paint, Paintbrushes etc. How would you assure that work would get done?        Be able to identify when Dining Philosophers code causes a deadlock (or not). For example, if you saw the following code snippet which Coffman condition is not satisfied?    // Get both locks or nonepthread_mutex_lock(a);if(pthread_mutex_trylock( b )) { /* failure */  pthread_mutex_unlock( a );}            The following calls are made    // Thread 1pthread_mutex_lock(m1) // successpthread_mutex_lock(m2) // blocks// Thread 2pthread_mutex_lock(m2) // successpthread_mutex_lock(m1) // blocks        What happens and why? What happens if a third thread calls pthread_mutex_lock(m1) ?        How many processes are blocked? As usual assume that a process is able to complete if it is able to acquire all of the resources listed below.                  P1 acquires R1                    P2 acquires R2                    P1 acquires R3                    P2 waits for R3                    P3 acquires R5                    P1 waits for R4                    P3 waits for R1                    P4 waits for R5                    P5 waits for R1            (Draw out the resource graph!)"
  },{
    "title": "Filesystems",
    "url": " /coursebook/Filesystems",
   "content": "  Filesystems          What is a filesystem?                  The File API                    Storing data on disk                  File Contents          Directory Implementation          Unix Directory Conventions          How do I list the contents of a directory?          Linking          Pathing          Metadata                    Permissions and bits                  User id/Group id          Reading/Changing file permissions          Understanding the ‘umask’          The ‘setuid’ bit          The ‘sticky’ bit                    Virtual filesystems and other filesystems                  Managing files and filesystems          Obtaining random data          Copying Files          Updating Modification Time          Managing Filesystems                    Memory Mapped IO      Reliable Single Disk Filesystems                  RAID          Higher Level Raids                    Simple Filesystem Model                  Performing Reads          Performing Writes          Adding Deletes                    Topics      Questions      [1][]  FilesystemsWhat is a filesystem?You may have encountered the old unix adage, “everything is a file”. In most UNIX systems, files operations provide an interface to abstract many different operations. Network sockets, hardware devices and even data on disk are all represented by a file-like object. A file-like object must follow certain conventions:      It must present it self to the filesystem        It must support common filesystem operations, such as open, read, write  A filesystem is an implementation of the file interface. In this chapter, we will be exploring the various callbacks a filesystem provides and some typical functionality or implementation details associated with this. In this class, we will mostly talk about filesystems that serve to allow users to access data on disk. These filesystems are integral to modern computers. Filesystems not only deal with storing local files, they handle special devices that allow for safe communication between the kernel and user space. Filesystems also deal with failures, scalability, indexing, encryption, compression and performance. Filesystems handle the abstraction between a file which contains data and how exactly that data is stored on disk, partitioned, and protected.Although filesystems are usually thought of as a kind of tree, most filesystems are usually a directed graph, a model we will explore in depth later in this chapter. Before we dive into the details of a filesystem, let’s take a look at some examples.      ext4 Usually mounted at /, this is the filesystem that usually provides disk access as you’re used to.        procfs Usually mounted at /proc, provides information and control over processes.        sysfs Usually mounted at /sys, a more mordern version of /proc that also allows control over various other hardware such as network sockets.        tmpfs Mounted at /tmp in some systems, an in-memory filesystem to hold temporary files.        sshfs A filesystem that syncs files across the ssh protocol.  To clarify, a mount point is simply a mapping of a directory to a filesystem represented in the kernel. What this means is that to resolve which filesystem a structure a call must resolve to. Meaning that /root is resolved by the ext4 filesystem in our case, but /proc/2 is resolved by the procfs system even though it contains / as a subsystem.As you may have noticed, some of these filesystems provide an interface to things that aren’t a “file” as you might colloquially refer to them. Filesystems such as procfs are usually refered to as virtual filesystems, since they don’t provide data access in the same sense as a traditional filesystem would. Technically, all filesystems in the kernel are represented by virtual filesystem, but in our class we will differentiate virtual filesystems as filesystems that actually don’t store anything on a hard disk.The File APIA filesystem must provide callback functions to a variety of actions. Some of them as listed below:      open Opens a file for IO        read Read contents of a file        write Write to a file        close Close a file and free associated resources        chmod Modify permissions of a file        ioctl Interact with device parameters of character devices such as terminals  Not every filesystem supports all the possible callback functions. For example many filesystems do not implement ioctl or link. In this chapter, we will not be examining each filesystem callback. If you would like to learn more about this interface, try looking at the documentation for FUSE, the source code for glibc or the linux manpages.Storing data on diskIn order to understand how a filesystem interacts with data on disk, there are three key terms we will be using.      disk block A disk block is a portion of the disk that is reserved for storing the contents of a file or a directory.        inode An inode is a file or directory. This means that an inode contains metadata about the file as well as pointers to disk blocks so that the file can actually be written to or read from.        superblock A superblock contains metadata about the inodes and disk blocks. An example superblock can store how full each disk block is, which inodes are being used etc. Modern filesystems may actually contain multiple superblocks and a sort-of super-super block that keeps track of which sectors are governed by which superblocks. This tends to help with fragmentation.  These structures all are presented in the diagram below.It may seem overwhelming, but by the end of this chapter, we will be able to make sense of every part of the filesystem.In order to reason about data on some form of storage (spinning disks, solid state drives, magnetic tape, etc.), it is common practice to first consider the medium of storage as a collection of blocks. A block can be thought of as a continguous region on disk, and while it’s size is sometimes determined by some property of the underlying hardware, it is more frequently determined based off of the size of a page of memory for a given system, so that data from the disk can be cached in memory for faster access - a very important feature of many filesystems.Usually, a filesystem has a special block denoted as a superblock which stores metadata about the filesystem such as a journal (which logs changes to the filesystem), a table of inodes and where the first inode is stored on disk, etc. The important thing about a superblock is that it is in a known location on disk.The inode is the most important structure for our filesystem as it represents a file. Before we explore it in depth, let’s list out the key information we need to have a usable file:      Name        File size        Time created, last modified, last accessed        Permissions        Filepath        Checksum        File data  File ContentsFrom Wikipedia:  In a Unix-style file system, an index node, informally referred to as an inode, is a data structure used to represent a filesystem object, which can be one of various things including a file or a directory. Each inode stores the attributes and disk block location(s) of the filesystem object’s data. Filesystem object attributes may include manipulation metadata (e.g. change, access, modify time), as well as owner and permission data (e.g. group-id, user-id, permissions).Typically, the superblock stores an array of inodes, each of which stores direct, and potentially several kinds of indirect pointers to disk blocks. Since inodes are stored in the superblock, most filesystems have a limit on how many inodes can exist. Since each inode corresponds to a file, this is also a limit on how many files that filesystem can have. Trying to overcome this problem by storing inodes in some other location greatly increases the complexity of the filesystem. Trying to reallocate space for the inode table is also infeasible since every byte following the end of the inode array would have to be shifted, a highly expensive operation. This isn’t to say there aren’t any solutions at all, although typically there is no need to increase the number of inodes since the number of inodes is usually sufficiently high.Big idea: Forget names of files: The ‘inode’ is the file.It is common to think of the file name as the ‘actual’ file. It’s not! Instead consider the inode as the file. The inode holds the meta-information (last accessed, ownership, size) and points to the disk blocks used to hold the file contents. However, the inode does not usually store a filename. Filenames are usually only stored in directories. (see below)For example, to read the first few bytes of the file, follow the first direct block pointer to the first direct block and read the first few bytes. Writing follows the same process. If you want to read the entire file, keep reading direct blocks until you’ve read a number of bytes equal to the size of the file. If the total size of the file is less than that of the number of direct blocks multiplied by the size of a block, then unsued block pointers will be undefined. Similarly, if the size of a file is not a multiple of the size of a block, data past the end of the last byte in the last block will be garbage.What if a file is bigger than the maximum space addressable by it’s direct blocks?  “All problems in computer science can be solved by another level of indirection.” - David WheelerExcept the problem of too many layers of indirection.To solve this problem, we introduce the indirect blocks. An indirect block is a block that store pointers to more data blocks. Similarly a double indirect block stores pointers to indirect blocks and the concept can be generalized to arbitrary levels of indirection. This is a very important concept, since as inodes are stored in the superblock, or some other strucutre in a well known location with a constant amount of space, indirection allows exponential increases in the amount of space an inode can keep track of.As a worked example, suppose we divide the disk into 4KB blocks and we want to address up to 2^32 blocks. The maximum disk size is 4KB *2^32 = 16TB (remember 2^10 = 1024). A disk block can store 4KB / 4B (each pointer needs to be 32 bits) = 1024 pointers. Each pointer refers to a 4KB disk block - so you can refer up to 1024*4KB = 4MB of data. For the same disk configuration, a double indirect block stores 1024 pointers to 1024 indirection tables. Thus a double-indirect block can refer up to 1024 * 4MB = 4GB of data. Similarly, a triple indirect block can refer up to 4TB of data. Naturally though, this is three times as slow.Directory ImplementationA directory is just a mapping of names to inode numbers. It’s typically just a normal file, but with some special bits set in its inode and a very specific structure for its contents. POSIX provides a small set of functions to read the filename and inode number for each entry, which we will talk about in depth later in this chapter.Let’s think about what it looks like in the actual file system. Theoretically, directories are just like actual files. The disk blocks will contain directory entries or dirents. What that means is that our disk block can look like this| inode_num | name   | | ----------- | ------ || 2043567   | hi.txt | | ... |Each directory entry could either be a fixed size, or a variable c-string. It depends on how the particular filesystem implements it at the lower level. To see a mapping of filenames to inode numbers on a POSIX system, from a shell, use ls with the -i option# ls -i12983989 dirlist.c      12984068 sandwich.cUnix Directory ConventionsIn standard unix file systems the following entries are specially added on requests to read a directory.      . represents the current directory        .. represents the parent directory        ~ is the name of the home directory usually  Note that ... is NOT a valid representation of any directory (this not the grandparent directory). It could however be the name of a file on disk. Though confusingly, zsh provides this as a handy shortcut to the grandparent directory should it exist.Additional facts about name-related conventions:      Files that start with ’.’ on disk are conventionally considered ’hidden’ and will not be listed by programs like ls without additional flags (-a). This is not a feature of the filesystem and programs may choose to ignore this.        Some files may also start with a null byte. These are usually abstract unix sockets and are used to prevent cluttering up the filesystem since they will be effectively hidden by any program not expecting them. They will, however, be listed by tools that detail information about sockets, so this is not a feature providing security.  How do I list the contents of a directory?While interacting with a file in C is done by using open to open the file and then read or write to interact with the file before calling close to release resources, directories have special calls such as, opendir, closedir and readdir. There is no function writedir since typically that implies creating a file or link.To explore these functions, let’s write a program to search the contents of a directory for a particular file. (The code below has a bug, try to spot it!)int exists(char *directory, char *name)  {    struct dirent *dp;    DIR *dirp = opendir(directory);    while ((dp = readdir(dirp)) != NULL) {        puts(dp-&gt;d_name);        if (!strcmp(dp-&gt;d_name, name)) {        return 1; /* Found */        }    }    closedir(dirp);    return 0; /* Not Found */}The above code has a subtle bug: It leaks resources! If a matching filename is found then ‘closedir’ is never called as part of the early return. Any file descriptors opened, and any memory allocated, by opendir are never released. This means eventually the process will run out of resources and an open or opendir call will fail.The fix is to ensure we free up resources in every possible code-path. In the above code this means calling closedir before return 1. Forgetting to release resources is a common C programming bug because there is no support in the C lanaguage to ensure resources are always released with all codepaths.Note: after a call to fork(), either (XOR) the parent or the child can use readdir(), rewinddir() or seekdir(). If both the parent and the child use the above, behavior is undefined.There are two main gotchas and one consideration: The readdir function returns “.” (current directory) and “..” (parent directory). If you are looking for sub-directories, you need to explicitly exclude these directories.For many applications it’s reasonable to check the current directory first before recursively searching sub-directories. This can be achieved by storing the results in a linked list, or resetting the directory struct to restart from the beginning.The following code attempts to list all files in a directory recursively. As an exercise, try to identify the bugs it introduces.void dirlist(char *path) {  struct dirent *dp;  DIR *dirp = opendir(path);  while ((dp = readdir(dirp)) != NULL) {     char newpath[strlen(path) + strlen(dp-&gt;d_name) + 1];     sprintf(newpath,\"%s/%s\", newpath, dp-&gt;d_name);     printf(\"%s\\n\", dp-&gt;d_name);     dirlist(newpath);  }}int main(int argc, char **argv) { dirlist(argv[1]); return 0; }Did you find all 5 bugs?// Check opendir result (perhaps user gave us a path that can not be opened as a directoryif (!dirp) { perror(\"Could not open directory\"); return; }// +2 as we need space for the / and the terminating 0char newpath[strlen(path) + strlen(dp-&gt;d_name) + 2];// Correct parametersprintf(newpath,\"%s/%s\", path, dp-&gt;d_name);// Perform stat test (and verify) before recursingif (0 == stat(newpath,&amp;s) &amp;&amp; S_ISDIR(s.st_mode)) dirlist(newpath)// Resource leak: the directory file handle is not closed after the while loopclosedir(dirp);One final note of caution: readdir is not thread-safe! Additionally readdir_r is deprecated, so multi-threaded searches use readdir with external synchronization.See the man page of readdir for more details.LinkingLinks are what force us to model a filesystem as a tree rather than a graph. While modelling the filesystem as a tree would imply that every inode has a unique parent directory, links allow inodes to present themselves as files in multiple places, potentially with different names, thus leading to an inode having multiple parents directories.The first kind of link is a hard link. A hard link is simply an entry in a directory assigning some name to an inode number that already has a different name and mapping in either the same directory or a different one. If we already have a file on a file system we can create another link to the same inode using the ‘ln’ command:$ ln file1.txt blip.txtHowever blip.txt is the same file; if I edit blip I’m editing the same file as ‘file1.txt!’ We can prove this by showing that both file names refer to the same inode:$ ls -i file1.txt blip.txt134235 file1.txt134235 blip.txtThe equivalent C call is link// Function Prototypeint link(const char *path1, const char *path2);link(\"file1.txt\", \"blip.txt\");For simplicity the above examples made hard links inside the same directory. Hard links can be created anywhere inside the same filesystem.The second kind of link is a soft link - or a symbolic link or a symlink. A symbolic link is different because it does not deal with inode numbers directly. Instead a symbolic link is a regular file with a special bit set and stores a path to another file. Quite simply, without the special bit, it is nothing more than a text file with a file path inside. Note that when people generally talk about a link without specifying hard or soft, they are referring to a hard link.To create a symbolic link in the shell use ln -s. To read the contents of the link as just a file use readlink. These are both demonstrated below:$ ln -s file1.txt file2.txt$ ls -i file1.txt blip.txt134235 file1.txt134236 file2.txt134235 blip.txt$ cat file1.txtfile1!$ cat file2.txtfile1!$ cat blip.txtfile1!$ echo edited file2 &gt;&gt; file2.txt # &gt;&gt; is bash syntax for append to file$ cat file1.txtfile1!edited file2$ cat file2.txtI'm file1!edited file2$ cat blip.txtfile1!edited file2$ readlink myfile.txtfile2.txtNote that file2.txt and file1.txt have different inode numbers, unlike the hard link, blip.txt.There is a C library call to create symlinks which is similar to link:symlink(const char *target, const char *symlink);Some advantages of symbolic links are      Can refer to files that don’t exist yet        Unlike hard links, can refer to directories as well as regular files        Can refer to files (and directories) that exist outside of the current file system  However, symlinks have a key disadvantage, they as slower than regular files and directories. When the links contents are read, they must be interpreted as a new path to the target file, resulting in an additional call to open and read since the real file must be opened and read. Another disadvantage is that POSIX will not let you hard link directories whereas softlinks are allowed. The ln command will only allow root to do this and only if you provide the -d option. However even root may not be able to perform this because most filesystems prevent it!The integrity of the file system assumes the directory structure (excluding softlinks which we will talk about later) is a non-cyclic tree that is reachable from the root directory. It becomes expensive to enforce or verify this constraint if directory linking is allowed. Breaking these assumptions can cause file integrity tools to not be able to repair the file system. Recursive searches potentially never terminate and directories can have more than one parent but “..” can only refer to a single parent. All in all, a bad idea.When you remove a file (using rm or unlink) you are removing an inode reference from a directory. However the inode may still be referenced from other directories. In order to determine if the contents of the file are still required, each inode keeps a reference count that is updated whenever a new link is created or destroyed. This count only tracks hard links, symlinks are allowed to refer to a non-existant file and thus, do not matter.An example use of hard-links is to efficiently create multiple archives of a file system at different points in time. Once the archive area has a copy of a particular file, then future archives can re-use these archive files rather than creating a duplicate file. Apple’s “Time Machine” software does this.PathingNow that we have definitions talk about directories, we come across the concept of a path. A path is a sequence of directories that provide one with a “path” in the filesystem graph. However, there are some nuances. It is possible to have a path called a/b/../c/./. Since .. and . are special entries in directories, this is a valid path that actually refers to a/c. Most filesystem functions will allow uncompressed paths to be passed in. The C library provides a function realpath to compress the path or get the realpath. To simplify by hand remember that .. means ‘parent folder’ and that . means ‘current folder’. Below is an example that illustrates the simplification of the a/b/../c/. by using cd in a shell to navigate a filesystem.      cd a (in a)        cd b (in a/b)        cd .. (in a, because .. represents ‘parent folder’)        cd c (in a/c)        cd . (in a/c, because . represents ‘current folder’)  Thus, this path can be simplified to a/c.MetadataHow can we distinguish between a regular file and a directory? For that matter there’s many other attributes that files also might contain. You may know that on most UNIX systems, unlike windows systems, a file’s type is not determined by its extension. How does the system know what type the file is?All of this information is stored within an inode. To access it, use the stat calls. For example, to find out when my ‘notes.txt’ file was last accessed.struct stat s;stat(\"notes.txt\", &amp;s);printf(\"Last accessed %s\", ctime(&amp;s.st_atime));There are actually three versions of stat;int stat(const char *path, struct stat *buf);int fstat(int fd, struct stat *buf);int lstat(const char *path, struct stat *buf);For example, you can use fstat to find out the meta-information about a file if you already have an file descriptor associated with that fileFILE *file = fopen(\"notes.txt\", \"r\");int fd = fileno(file); /* Just for fun - extract the file descriptor from a C FILE struct */struct stat s;fstat(fd, &amp; s);printf(\"Last accessed %s\", ctime(&amp;s.st_atime));lstat is almost the same as stat but handles symbolic links differently. From the stat man page:  lstat() is identical to stat(), except that if pathname is a symbolic link, then it returns information about the link itself, not the file that it refers to.The stat functions make use of struct stat. From the stat man page:struct stat {    dev_t     st_dev;         /* ID of device containing file */    ino_t     st_ino;         /* Inode number */    mode_t    st_mode;        /* File type and mode */    nlink_t   st_nlink;       /* Number of hard links */    uid_t     st_uid;         /* User ID of owner */    gid_t     st_gid;         /* Group ID of owner */    dev_t     st_rdev;        /* Device ID (if special file) */    off_t     st_size;        /* Total size, in bytes */    blksize_t st_blksize;     /* Block size for filesystem I/O */    blkcnt_t  st_blocks;      /* Number of 512B blocks allocated */    struct timespec st_atim;  /* Time of last access */    struct timespec st_mtim;  /* Time of last modification */    struct timespec st_ctim;  /* Time of last status change */};The st_mode field can be used to distinguish between regular files and directories. To accomplish this, you will also need the macros, S_ISDIR and S_ISREG.   struct stat s;   if (0 == stat(name, &amp;s)) {      printf(\"%s \", name);      if (S_ISDIR( s.st_mode)) puts(\"is a directory\");      if (S_ISREG( s.st_mode)) puts(\"is a regular file\");   } else {      perror(\"stat failed - are you sure I can read this file's meta data?\");   }Permissions and bitsPermissions are a key part of the way UNIX systems provide security in a filesystem. You may have noticed that the st_mode field in struct stat contains more than just the file type. It also contains the mode, a description detailing what a user can and can’t do with a given file. There are usually three sets of permissions for any file. Permissions for the user, the group and other. For each of the three catagories we need to keep track of whether or not the user is allowed to read the file, write to the file, and execute the file. Since there are three categories and three permissions, permissions are usually represented as a 3-digit octal number. For each digit the least significant byte corresponds to read privilages, the middle one to write privilages and the final byte to execute privilages. They are always presented as User, Group, Other (UGO). Below are some common examples:      755: rwx r-x r-x    user: rwx, group: r-x, others: r-x    User can read, write and execute. Group and others can only read and execute.        644: rw- r– r–    user: rw-, group: r–, others: r–    User can read and write. Group and others can only read.  It is worth noting that the rwx bits for a directory have slightly different meaning. Write-access to a directory will allow you to create or delete new files or directories inside (you can think about this as just having write access to the dirent mappings). Read-access to a directory will allow you to list a directory’s contents (this is just read access to the dirent mapping). Execute will allow you to enter the directory and access it. Without the execute bit it is not possible to create or remove files or directories since you cannot access them. You can, however, list the contents of the directory.There are several command line utilities for interacting with a file’s mode. mknod changes the type of the file. chmod takes a number and a file and changes the permission bits. However, before we can dicuss chmod in detail, we must also understand the user id (uid) and group id (gid) as well.User id/Group idEvery user in a UNIX system has a user id. This is a unique number that can identify a user. Similarly, users can be added to collections called groups, and every group also has a uniquely identifying number. Groups have a variety of uses on UNIX systems. They can be assigned capabilities - a way of describing the level of control a user has over a system. For example, a group you may have run into is the sudoers group, a set of trusted users who are allowed to use the command sudo to temporarily gain higher privilages. (We’ll talk more about how sudo works in this chapter). Every file, upon creation, an owner, the creator of the file. This owner’s user id (uid) can be found inside the st_mode file of a struct stat with a call to stat. Similarly the group id (gid) is set as well.Every process can determine it’s uid and gid with getuid and getgid. When a processes tries to open a file with a specific mode, it’s uid and gid are compared with the uid and gid of the file. If the uids match, then the process’s request to open the file will be compared with the bits on the user field of the file’s permissions. Similarly, if the gids match, then the process’s request will be compared with the group field of the permissions. Finally, if none of the ids match, then the other field will apply.Reading/Changing file permissionsBefore we discuss how to change permission bits, we should be able to read them. In C, the stat family of library calls can be used. To read permission bits from the command line, use ‘ls -l’. Note that the permissions will outputed in the format ‘drwxrwxrwx’. The first character indicates the type of file type. Possible values for the first character:      (-) regular file        (d) directory        (c) character device file        (l) symbolic link        (p) pipe        (b) block device        (s) socket  Alternatively, use the program stat which presents all the information that one could retrieve from the stat library call.To change the permission bits, there is a system call, int chmod(const char *path, mode_t mode);. In order to simplify our examples, we will be using the command line utility of the same name chmod (short of “change mode”). There are two common ways to use chmod ; either with an octal value or with a symbolic string:$ chmod 644 file1$ chmod 755 file2$ chmod 700 file3$ chmod ugo-w file4$ chmod o-rx file4The base-8 (‘octal’) digits describe the permissions for each role: The user who owns the file, the group and everyone else. The octal number is the sum of three values given to the three types of permission: read(4), write(2), execute(1)Example: chmod 755 myfile      r + w + x = digit * user has 4+2+1, full permission        group has 4+0+1, read and execute permission        all users have 4+0+1, read and execute permission  Understanding the ‘umask’The umask subtracts (reduces) permission bits from 777 and is used when new files and new directories are created by open, mkdir etc. By default the umask is set to 022 (octal), which means that group and other privileges will not include the writable bit . Each process (including the shell) has a current umask value. When forking, the child inherits the parent’s umask value.For example, by setting the umask to 077 in the shell, ensures that future file and directory creation will only be accessible to the current user,$ umask 077$ mkdir secretdirAs a code example, suppose a new file is created with open() and mode bits 666 (write and read bits for user,group and other):open(\"myfile\", O_CREAT, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH);If umask is octal 022, then the permissions of the created file will be 0666 &amp; ~022 ie.S_IRUSR | S_IWUSR | S_IRGRP | S_IROTHThe ‘setuid’ bitYou may have noticed an additional bit that files with execute permission may have set. This bit is the setuid bit. It indicated that when run, the program with set the uid of the user to that of the owner of the file. Similar, there is a setgid bit which sets the gid of the executor to the gid of the owner. The canonical example of a program with setuid set is sudo.sudo is usually a program that is owned by the root user - a user that has all capabilities. By using sudo an otherwise unprivilaged user can gain access to most parts of the system. This is useful for running programs that may require elevated privilages, such as using chown to change ownership of a file, or to use mount to mount or unmount filesystems (an action we will discuss later in this chapter). Here are some examples:$ sudo mount /dev/sda2 /stuff/mydisk$ sudo adduser fred$ ls -l /usr/bin/sudo-r-s--x--x  1 root  wheel  327920 Oct 24 09:04 /usr/bin/sudoEffective uid/gidWhen executing a process with the setuid bit, it is still possible to determine a user’s original uid with getuid. The real action of the setuid bit is to set the effective userid (euid) which can be determined with geteuid. The actions of getuid and geteuid are described below.      getuid returns the real user id (zero if logged in as root)        geteuid returns the effective userid (zero if acting as root, e.g. due to the setuid flag set on a program)  These functions can allow one to write a program that can only be run by a privilaged user by checking geteuid or go a step further and ensure that the only user who can run the code is root by using getuid.The ‘sticky’ bitSticky bits as we use them today do not serve the same purpose as their intial introduction. Sticky bits were a bit that could be set on an executable file that would allow a program’s text segment to remain in swap even after the end of the program’s execution. This made subsequent executions of the same program faster. Today, this behavior is no longer supported and the sticky bit only holds meaning when set on a directory,When a directory’s sticky bit is set only the file’s owner, the directory’s owner, and the root user can rename or delete the file. This is useful when multiple users have write access to a common directory. A common use of the sticky bit is for the shared and writable /tmp directory.To set the sticky bit, use chmod +t.aneesh$ mkdir stickyaneesh$ chmod +t sticky aneesh$ ls -ldrwxr-xr-x  7 aneesh aneesh    4096 Nov  1 14:19 .drwxr-xr-x 53 aneesh aneesh    4096 Nov  1 14:19 ..drwxr-xr-t  2 aneesh aneesh    4096 Nov  1 14:19 stickyaneesh$ su newusernewuser$ rm -rf stickyrm: cannot remove 'sticky': Permission deniednewuser$ exitaneesh$ rm -rf stickyaneesh$ ls -ldrwxr-xr-x  7 aneesh aneesh    4096 Nov  1 14:19 .drwxr-xr-x 53 aneesh aneesh    4096 Nov  1 14:19 ..Note that in the example above, the username is prepended to the prompt, and the command su is used to switch users.Virtual filesystems and other filesystemsPOSIX systems, such as Linux and Mac OSX (which is based on BSD) include several virtual filesystems that are mounted (available) as part of the file-system. Files inside these virtual filesystems do not exist on the disk; they are generated dynamically by the kernel when a process requests a directory listing. Linux provides 3 main virtual filesystems/dev  - A list of physical and virtual devices (for example network card, cdrom, random number generator)/proc - A list of resources used by each process and (by tradition) set of system information/sys - An organized list of internal kernel entitiesFor example if I want a continuous stream of 0s, I can cat /dev/zero.Another example is the file /dev/null - a great place to store bits that you never need to read! Bytes sent to /dev/null/ are never stored - they are simply discarded. A common use of /dev/null is to discard standard output. For example,$ ls . &gt;/dev/nullManaging files and filesystemsGiven the multitude of operations that are availible to you from the filesystem, let’s explore some tools and techniques that can be used to manage files and filesystems.One example is creating a secure directory. Suppose you created your own directory in /tmp and then set the permissions so that only you can use the directory (see below). Is this secure?$ mkdir /tmp/mystuff$ chmod 700 /tmp/mystuffThere is a window of opportunity between when the directory is created and when it’s permissions are changed. This leads to several vulnerabilities that are based on a race condition.Another user replaces mystuff with a hardlink to an existing file or directory owned by the second user, then they would be able to read and control the contents of the mystuff directory. Oh no - our secrets are no longer secret!However in this specific example the /tmp directory has the sticky bit set, so other users may not delete the mystuff directory, and the simple attack scenario described above is impossible. This does not mean that creating the directory and then later making the directory private is secure! A better version is to atomically create the directory with the correct permissions from its inception -$ mkdir -m 700 /tmp/mystuffObtaining random data/dev/random is a file which contains number generator where the entropy is determined from environmental noise. Random will block/wait until enough entropy is collected from the environment./dev/urandom is like random, but differs in the fact that it allows for repetition (lower entropy threshold), thus wont block.Copying FilesUse the versatile dd command. For example, the following command copies 1 MB of data from the file /dev/urandom to the file /dev/null. The data is copied as 1024 blocks of blocksize 1024 bytes.$ dd if=/dev/urandom of=/dev/null bs=1k count=1024Both the input and output files in the example above are virtual - they don’t exist on a disk. This means the speed of the transfer is unaffected by hardware power.dd is also commonly used to make a copy of a disk or an entire filesystem to create images that can either be burned on to other disks or to distribute data to other users.Updating Modification TimeThe touch executable creates file if it does not exist and also updates the file’s last modified time to be the current time. For example, we can make a new private file with the current time:$ umask 077       # all future new files will maskout all r,w,x bits for group and other access$ touch file123   # create a file if it does not exist, and update its modified time$ stat file123  File: `file123'  Size: 0           Blocks: 0          IO Block: 65536  regular empty fileDevice: 21h/33d Inode: 226148      Links: 1Access: (0600/-rw-------)  Uid: (395606/ angrave)   Gid: (61019/     ews)Access: 2014-11-12 13:42:06.000000000 -0600Modify: 2014-11-12 13:42:06.001787000 -0600Change: 2014-11-12 13:42:06.001787000 -0600An example use of touch is to force make to recompile a file that is unchanged after modifying the compiler options inside the makefile. Remeber that make is ‘lazy’ - it will compare the modified time of the source file with the corresponding output file to see if the file needs to be recompiled$ touch myprogram.c   # force my source file to be recompiled$ makeManaging FilesystemsTo manage filesystems on your machine, use mount. Using mount without any options generates a list (one filesystem per line) of mounted filesystems including networked, virtual and local (spinning disk / SSD-based) filesystems. Here is a typical output of mount$ mount/dev/mapper/cs241--server_sys-root on / type ext4 (rw)proc on /proc type proc (rw)sysfs on /sys type sysfs (rw)devpts on /dev/pts type devpts (rw,gid=5,mode=620)tmpfs on /dev/shm type tmpfs (rw,rootcontext=\"system_u:object_r:tmpfs_t:s0\")/dev/sda1 on /boot type ext3 (rw)/dev/mapper/cs241--server_sys-srv on /srv type ext4 (rw)/dev/mapper/cs241--server_sys-tmp on /tmp type ext4 (rw)/dev/mapper/cs241--server_sys-var on /var type ext4 (rw)rw,bind)/srv/software/Mathematica-8.0 on /software/Mathematica-8.0 type none (rw,bind)engr-ews-homes.engr.illinois.edu:/fs1-homes/angrave/linux on /home/angrave type nfs (rw,soft,intr,tcp,noacl,acregmin=30,vers=3,sec=sys,sloppy,addr=128.174.252.102)Notice that each line includes the filesystem type source of the filesystem and mount point. To reduce this output we can pipe it into grep and only see lines that match a regular expression.&gt;mount | grep proc  # only see lines that contain 'proc'proc on /proc type proc (rw)none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)Filesystem MountingSuppose you had downloaded a bootable linux disk image from the URLwget $URLBefore putting the filesystem on a CD, we can mount the file as a filesystem and explore its contents. Note, mount requires root access, so let’s run it using sudo$ mkdir arch$ sudo mount -o loop archlinux-2015.04.01-dual.iso ./arch$ cd archBefore the mount command, the arch directory is new and obviously empty. After mounting, the contents of arch/ will be drawn from the files and directories stored in the filesystem stored inside the archlinux-2014.11.01-dual.iso file. The loop option is required because we want to mount a regular file not a block device such as a physical disk.The loop option wraps the original file as a block device - in this example we will find out below that the file system is provided under /dev/loop0 : We can check the filesystem type and mount options by running the mount command without any parameters. We will pipe the output into grep so that we only see the relevant output line(s) that contain ‘arch’$ mount | grep arch/home/demo/archlinux-2014.11.01-dual.iso on /home/demo/arch type iso9660 (rw,loop=/dev/loop0)The iso9660 filesystem is a read-only filesystem originally designed for optical storage media (i.e. CDRoms). Attempting to change the contents of the filesystem will fail$ touch arch/nocandotouch: cannot touch `/home/demo/arch/nocando': Read-only file systemMemory Mapped IOWhile we traditionally think of reading and writing from a file as operation that happen by using the read and write calls, there is an alternative, mapping a file into memory using mmap. mmap can also be used for IPC, and you can see more about mmap as a system call that enables shared memory in the IPC chapter. In this chapter, we’ll briefly explore mmap as a filesystem operation.mmap takes a file and maps its contents into memory. This allows a user to treat the entire file as a buffer in memory for easier semantics while programming, and to avoid having to read a file as discreete chunks explicitly.Not all filesystems support using mmap for IO, but amongst those that do, not all have the same behavior. Some will simply implement mmap as a wrapper around read and write. Others will add additional optimizations by taking advantage of the kernel’s page cache. Of course, such optimization can be used in the implementation of read and write as well, so often using mmap does not impact performance.mmap is used to perform some operations such as loading libraries and processes into memory. If many programs only need read-access to the same file (e.g. /bin/bash, the C library) then the same physical memory can be shared between multiple processes.The process to map a file into memory is simple:      mmap requires a filedescriptor, so we need to open the file first        We seek to our desired size and write one byte to ensure that the file is sufficient length        When finished call munmap to unmap the file from memory.  Here is a quick example.#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;sys/mman.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;string.h&gt;int fail(char *filename, int linenumber) {  fprintf(stderr, \"%s:%d %s\\n\", filename, linenumber, strerror(errno));  exit(1);  return 0; /*Make compiler happy */}#define QUIT fail(__FILE__, __LINE__ )int main() {  // We want a file big enough to hold 10 integers  int size = sizeof(int) * 10;  int fd = open(\"data\", O_RDWR | O_CREAT | O_TRUNC, 0600); //6 = read+write for me!  lseek(fd, size, SEEK_SET);  write(fd, \"A\", 1);  void *addr = mmap(0, size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);  printf(\"Mapped at %p\\n\", addr);  if (addr == (void*) -1 ) QUIT;  int *array = addr;  array[0] = 0x12345678;  array[1] = 0xdeadc0de;  munmap(addr,size);  return 0;}The careful reader may notice that our integers were written in least-significant-byte format (because that is the endianess of the CPU) and that we allocated a file that is one byte too many! The PROT_READ | PROT_WRITE options specify the virtual memory protection. The option PROT_EXEC (not used here) can be set to allow CPU execution of instructions in memory (e.g. this would be useful if you mapped an executable or library).Reliable Single Disk FilesystemsMost filesystems cache significant amounts of disk data in physical memory. Linux, in this respect, is particularly extreme: All unused memory is used as a giant disk cache. The disk cache can have significant impact on overall system performance because disk I/O is slow. This is especially true for random access requests on spinning disks where the disk read-write latency is dominated by the seek time required to move the read-write disk head to the correct position.For efficiency, the kernel caches recently used disk blocks. For writing, we have to choose a trade-off between performance and reliability: Disk writes can also be cached (“Write-back cache”) where modified disk blocks are stored in memory until evicted. Alternatively a ‘write-through cache’ policy can be employed where disk writes are sent immediately to the disk. The latter is safer (as filesystem modifications are quickly stored to persistent media) but slower than a write-back cache; If writes are cached then they can be delayed and efficiently scheduled based on the physical position of each disk block. Note this is a simplified description because solid state drives (SSDs) can be used as a secondary write-back cache.Both solid state disks (SSD) and spinning disks have improved performance when reading or writing sequential data. Thus operating system can often use a read-ahead strategy to amortize the read-request costs (e.g. time cost for a spinning disk) and request several contiguous disk blocks per request. By issuing an I/O request for the next disk block before the user application requires the next disk block, the apparent disk I/O latency can be reduced.If your data is important and needs to be force written to disk, call sync to request that a filesystem changes be written (flushed) to disk. However, not all operating systems honor this request and even if the data is evicted from the kernel buffers the disk firmware use an internal on-disk cache or may not yet have finished changing the physical media. Note you can also request that all changes associated with a particular file descriptor are flushed to disk using fsync(int fd)If your operating system fails in the middle of an operation, most modern file systems do something called journalling that work around this. What the file system does is before it completes a potentially expensive operation, is that it writes what it is going to do down in a journal. In the case of a crash or failure, one can step through the journal and see which files are corrupt and fix them. This is a way to salvage hard disks in cases there is critical data and there is no apparent backup.Even though it is unlikely for your computer, programming for datacenters means that disks fail every few seconds. Disk failures are measured using “Mean-Time-Failure”. For large arrays, the mean failure time can be surprisingly short. For example if the MTTF(single disk) = 30,000 hours, then the MTTF(1000 disks)= 30000/1000=30 hours or about a day and a half!RAIDOne way to protect against this is to store the data twice! This is the main principle of a “RAID-1” disk array. RAID is short for redundant array of inexpensive disks. By duplicating the writes to a disk with writes to another backup disk, there are exactly two copies of the data. If one disk fails, the other disk serves as the only copy until it can be re-cloned. Reading data is faster since data can be requested from either disk, but writes are potentially twice as slow because now two write commands need to be issued for every disk block write. Compared to using a single disk, the cost of storage per byte has doubled.Another common RAID scheme is RAID-0, meaning that a file could be split up amoung two disks, but if any one of the disks fail then the files are irrecoverable. This has the benefit of halving write times because one part of the file could be writing to hard disk one and another part to hard disk two.It is also common to combine these systems. If you have a lot of hard disks, consider RAID-10. This is where you have two systems of RAID-1, but the systems are hooked up in RAID-0 to each other. This means you would get roughly the same speed from the slowdowns but now any one disk can fail and you can recover that disk. If two disks from opposing raid partitions fail there is a chance that recover can happen though we don’t could on it most of the time.Higher Level RaidsRAID-3 uses parity codes instead of mirroring the data. For each N-bits written we will write one extra bit, the ‘Parity bit’ that ensures the total number of 1s written is even. The parity bit is written to an additional disk. If any one disk (including the parity disk) is lost, then its contents can still be computed using the contents of the other disks.One disadvantage of RAID-3 is that whenever a disk block is written, the parity block will always be written too. This means that there is effectively a bottleneck in a separate disk. In practice, this is more likely to cause a failure because one disk is being used 100% of the time and once that disk fails then the other disks are more prone to failure.A single disk failure will not result in data loss (because there is sufficient data to rebuild the array from the remaining disks). Data-loss will occur when a two disks are unusable because there is no longer sufficient data to rebuild the array. We can calculate the probability of a two disk failure based on the repair time which includes not just the time to insert a new disk but the time required to rebuild the entire contents of the array.MTTF = mean time to failureMTTR = mean time to repairN = number of original disksp = MTTR / (MTTF-one-disk / (N-1))Using typical numbers (MTTR=1day, MTTF=1000days, N-1 = 9, p=0.009)There is a 1% chance that another drive will fail during the rebuild process (at that point you had better hope you still have an accessible backup of your original data. In practice the probability of a second failure during the repair process is likely higher because rebuilding the array is I/O-intensive (and on top of normal I/O request activity). This higher I/O load will also stress the disk arrayRAID-5 is similar to RAID-3 except that the check block (parity information) is assigned to different disks for different blocks. The check-block is ‘rotated’ through the disk array. RAID-5 provides better read and write performance than RAID-3 because there is no longer the bottleneck of the single parity disk. The one drawback is that you need more disks to have this setup and there are more complicated algorithms need to be usedFailure is the common case. Google reports 2-10% of disks fail per year Now multiply that by 60,000+ disks in a single warehouse… Must survive failure of not just a disk, but a rack of servers or a whole data centerSolutions Simple redundancy (2 or 3 copies of each file) e.g., Google GFS (2001) More efficient redundancy (analogous to RAID 3++) e.g., Google Colossus filesystem (~2010): customizable replication including Reed-Solomon codes with 1.5x redundancySimple Filesystem ModelOkay so software developers need to implement filesystems all the time. Don’t believe me? Take a look at Hadoop, GlusterFS, Qumulo, etc. Filesystems are hot areas of research now because people have realized that the software models that we devised don’t take full advantage of our current hardware. As such, you may be doing a lot of filesystem writing. Here we will go over one of our fake filesystems that we talke about in class and “walk through” some of the code andSo what does our hypothetical filesystem look like? We will base it off of the Minixfs, the first filesystem that Linux ran on. We’ll do this and try to make the filesystem as easy as possible. Laid out sequentually, we have our superblock. The superblock stores important metadata about the filesystem. After the superblock, we’ll keep a map of which inodes are being used. The n’th bit is set if the n’th inode – from the inode root – is set with the exact same convention for data blocks. Let’s consider a single file that spans all of its direct blocks and some indirect blocks with some omitted. In our pretend filesystem, for simplicity let’s assume that we are not going to delete any directory entries and every file is compact. This means that each file uses up any partial used data block before allocating a new data block. A sample image is below.We will assume that a data block is 4 KB.Performing ReadsPerforming reads tend to be pretty easy in our filesystem because our files our compact Let’s say that we want to read the entirety of this particular file. What we’d start by doing is go to the inode’s direct struct and find the first direct inode number. In our case it is #7. Then we find the 7th data block from the start of all data blocks. Then we read all of those bytes. We do the same thing for all of the direct nodes. What do we do after? We go to the indirect block and read the indirect block. We know that every 4 bytes of the indirect block are either a sentinel node (-1) or the number of another data block. In our particular example, the first four bytes evaluate to the integer 5, meaning that our data continues on the 5th datablock from the beginning. We do the same for data block #4 and we stop after because we exceed the size of the inodeNow, let’s think about the edge cases. How would you start the read starting at an arbitrary offset of (n) bytes given that block sizes are (4 KBs). How many indirect blocks should there be if the filesystem is correct (hint: think about using the size of the inode)Performing WritesPerforming writes fall into two categories, writes to files and writes to directories. First we’ll focus on files and assume that we are writing a byte to the (6)th KB of our file. To perform a write on a file at a particular offset, first you must go to the data block would start at that offset. For this particular example we would have to go to the 2nd or indexed number 1 inode to perform our write. We would once again fetch this number from the inode, go to the root of the data blocks, go to the (5)th data block and perform our write at the (2)KB offset from this block because we skipped the first four kilobytes of the file in block 7. We perform our write and go on our merry waySome questions for you. How would you consider performing a write that would go across data block boundaries? How would you consider performing a write whose write after adding the offset would extend the length of the file? How would you consider performing a write where the offset is greater than the length of the original file?Performing a write to a directory meaning that an inode needs to be added to a directory. If we pretend that the example above is a directory. We know that we will be adding at most one directory entry at a time. Meaning that we have to have enough space for one directory entry in our datablocks. Luckily the last data block that we have has enough free space. This means we just need to find the number of the last data block as we did above, go to where the data ends, and write one directory entry. Don’t forget to update the size of the directory so that the next creation doesn’t overwrite your file!Some more questions. How would you consider performing a write when the last data block is already full? How about when all the direct blocks have just been filled up and the inode doesn’t have an indirect block? What about when the first indirect entry (#4) is full? These are all edge cases you have to think about for a filesystem that really get youAdding DeletesAlthough this isn’t part of the lab originally. If you were to ask what happens when a file gets deleted, it’s actually pretty simple. If the inode is a file, then remove the directory entry in the parent directory by marking it as invalid (maybe making it point to inode -1) and skip it in your reads. You need to make sure to decrease the hard link count of the inode and if the count reaches zero, free the inode in the inode map and free all associated data blocks so they are reclaimed by the filesystem.If the inode is a directory, first you have to recursively remove every directory entry inside After, you have to mark the directory’s inode as free and set the associated datablocks to free as well. Why don’t we have to check hardlink counts for directories? Because you can’t hard link directories! Meaning, we can just easily delete it.Topics      Superblock        Data Block        Inode        Relative Path        File Metadata        Hard and Soft Links        Permission Bits        Mode bits        Working with Directories        Virtual File System        Reliable File Systems        RAID  Questions      How big can files be on a file system with 15 Direct blocks, 2 double, 3 triple indirect, 4kb blocks and 4byte entries? (Assume enough infinite blocks)        What is a superblock? Inode? Datablock?        How do I simplify /./proc/../dev/./random/        In ext2, what is stored in an inode, and what is stored in a directory entry?        What are /sys, /proc, /dev/random, and /dev/urandom?        What are permission bits?        How do you use chmod to set user/group/owner read/write/execute permissions?        What does the “dd” command do?        What is the difference between a hard link and a symbolic link? Does the file need to exist?        “ls -l” shows the size of each file in a directory. Is the size stored in the directory or in the file’s inode?  "
  },{
    "title": "C Programming Language",
    "url": " /coursebook/Introc",
   "content": "  C Programming Language          History of C      Features      Crash course intro to C      Preprocessor      Language Facilities                  Keywords          C data types          Operators                    Common C Functions                  Input/Output          string.h          Conventions/Errno                    System Calls      C Memory Model                  Structs          Strings in C          Two places for strings                    Pointers                  Pointer Basics          Pointer Arithmetic          What is a void pointer?                    Shell      Common Bugs                  Double Frees          Returning pointers to automatic variables          Buffer overflow/ underflow          Using uninitialized variables          Assuming Uninitialized memory will be zeroed                    Logic and Program flow mistakes                  Equal vs. equality          Undeclared or incorrectly prototyped functions          Extra Semicolons                    Topics      Questions/Exercises      [1][]  C Programming LanguageC is the de-facto programming language to do serious system serious programming. Why? Most kernels are written in largely in C. The linux kernel and the XNU kernel of which Mac OS X is based off. The Windows Kernel uses C++, but doing system programming on that is much harder on windows that UNIX for beginner system programmers. Most of you have some experience with C++, but C is a different beast entirely. You don’t have nice abstractions like classes and RAII to clean up memory. You are going to have to do that yourself. C gives you much more of an opportunity to shoot yourself in the foot but lets you do thinks at a much finer grain level.History of CC was developed by Dennis Ritchie and Ken Thompson at Bell Labs back in 1973 . Back then, we had gems of programming languages like Fortran, ALGOL, and LISP. The goal of C was two fold. One, to target the most popular computers at the time liek the PDP-7. Two, try and remove some of the lower level constructs like managing registers, programming assembly for jumps and instead create a language that had the power to express programs procedurally (as opposed to mathematically like lisp) with more readable code all while still having the ability to interface with the operating system. It sounded like a tough feat. At first, it was only used internally at Bell Labs along with the UNIX operating system.The first “real” standardization is with Brian Kerninghan and Dennis Ritchies book . It is still widely regarded today as the only portable set of C instructions. The K&amp;R book is known as the de-facto standard for learning C. There were different standards of C from ANSI to ISO after the Unix guides. The one that we will be mainly focusing on is the POSIX C library. Now to get the elephant out of the room, the Linux kernel is not entirely POSIX compliant. Mostly, it is because they didn’t want to pay the fee for compliance but also it doesn’t want to be completely compliant with a bunch of different standards because then it has to ensue increasing development costs to maintain compliance.Fast forward however many years, and we are at the current C standard put forth by ISO: C11. Not all the code that we us in this class will be in this format. We will aim to using C99 as the standard that most computers recognize. We will talk about some off-hand features like getline because they are so widely used with the GNU-C library. We’ll begin by providing a decently comprehensive overview of the language with pairing facilities.Features      Fast. There is nothing separating you and the system.        Simple. C and its standard library pose a simple set of portable functions.        Memory Management. C let’s you manage your memory. This can also bite you if you have memory errors.        It’s Everywhere. Pretty much every computer that is not embedded has some way of interfacing with C. The standard library is also everywhere. C has stood the test of time as a popular language, and it doesn’t look like it is going anywhere.  Crash course intro to CThe only way to start learning C is by starting with hello world. As per the original example that Kernighan and Ritchie proposed way back when, the hello world hasn’t changed that much.#include &lt;stdio.h&gt;int main(void) {     printf(\"Hello World\\n\");    return 0; }      The #include directive takes the file stdio.h (which stands for standard input and output) located somewhere in your operating system, copies the text, and substitutes it where the #include was.        The int main(void) is a function declaration. The first word int tells the compiler what the return type of the function is. The part before the parens (main) is the function name. In C, no two functions can have the same name in a single compiled program, shared libraries are a different touchy subject. Then, what comes after is the paramater list. When we give the parameter list for regular functions (void) that means that the compiler should error if the function is called with any arguments. For regular functions having a declaration like void func() means that you are allowed to call the function like func(1, 2, 3) because there is no delimiter . In the case of main, it is a special function. There are many ways of declaring main but the ones that you will be familiar with are int main(void), int main(), and int main(int argc, char *argv[]).        printf(Hello World); is what we call a function call. printf is defined as a part of stdio.h. The function has been compiled and lives somewhere else on our machine. All we need to do is include the header and call the function with the appropriate parameters (a string literal Hello World). If you don’t have the newline, the buffer will not be flushed. It is by convention that buffered IO is not flushed until a newline.        return 0;. main has to return an integer. By convention, return 0 means success and anything else means failure .  $ gcc main.c -o main$ ./mainHello World$      gcc is short for the GNU-Compiler-Collection which has a host of compilers ready for use. The compiler infers from the extension that you are trying to compile a .c file        ./main tells your shell to execute the program in the current directory called main. The program then prints out hello world  PreprocessorWhat is the preprocessor? Preprocessing is an operation that the compiler performs before actually compiling the program. It is a copy and paste command. Meaning the following susbtitution is performed.#define MAX_LENGTH 10char buffer[MAX_LENGTH]// Afterchar buffer[10]There are side effects to the preprocessor though. One problem is that the preprocessor needs to be able to tokenize properly, meaning trying to redefine the interals of the C language with a preprocessor may be impossible. Another problem is that they can’t be nested infinitely – there is an unbounded depth where they need to stop. Macros are also just simple text substitutions.#define min(a,b) ((a)&lt;(b) ? (a) : (b))int x = 4;if(min(x++, 5)) printf(\"%d is six\", x);Macros are simple text substitution so the above example expands to x++ &lt; 100 ? x++ : 100 (parenthesis omitted for clarity). Now for this case, it is opaque what gets printed out but it will be 6. Also consider the edge case when operator precedence comes into play.#define min(a,b) a&lt;b ? a : bint x = 99;int r = 10 + min(99, 100); // r is 100!Macros are simple text substitution so the above example expands to 10 + 99 &lt; 100 ? 99 : 100. You can have logical problems with the flexibility of certain parameters. One common source of confusion is with static arrays and the sizeof operator.#define ARRAY_LENGTH(A) (sizeof((A)) / sizeof((A)[0]))int static_array[10]; // ARRAY_LENGTH(static_array) = 10int* dynamic_array = malloc(10); // ARRAY_LENGTH(dynamic_array) = 2 or 1What is wrong with the macro? Well, it works if we have a static array like the first array because sizeof a static array returns the number of bytes that array takes up, and dividing it by the sizeof(an_element) would give you the number of entries. But if we use a pointer to a piece of memory, taking the sizeof the pointer and dividing it by the size of the first entry won’t always give us the size of the array.Language FacilitiesKeywordsC has an assortment of keywords. Here are some constructs that you should know briefly as of C99.      break is a keyword that is used in case statements or looping statements. When used in a case statement, the program jumps to the end of the block.    switch(1) {  case 1: /* Goes to this switch */    puts(\"1\");    break; /* Jumps to the end of the block */  case 2: /* Ignores this program */    puts(\"2\");    break;} /* Continues here */        In the context of a loop, it breaks out of the inner-most loop. The loop can be either a for, while, or do-while construct    while(1) {  while(2) {    break; /* Breaks out of while(2) */  } /* Jumps here */  break; /* Breaks out of while(1) */} /* Continues here */            const is a language level construct that tells the compiler that this data should not be modified. If one tries to change a const variable, the program will not even compile. const works a little differently when put before the type, the compiler flips the first type and const. Then the compiler uses a left associativity rule. Meaning that whatever is left of the pointer is constant. This is known as const-correctedness.    const int i = 0; // Same as \"int const i = 0\"char *str = ...; // Mutable pointer to a mutable stringconst char *const_str = ...; // Mutable pointer to a constant stringchar const *const_str2 = ...; // Same as aboveconst char *const const_ptr_str = ...;// Constant pointer to a constant string        But, it is important to know that this is a compiler imposed restriction only. There are ways of getting around this and the program will run fine with defined behavior. In systems programming, the only type of memory that you can’t write to is system write-protected memory.    const int i = 0; // Same as \"int const i = 0\"(*((int *)&amp;i)) = 1; // i == 1 nowconst char *ptr = \"hi\";*ptr = '\\0'; // Will cause a Segmentation Violation            continue is a control flow statement that exists only in loop constructions. Continue will skip the rest of the loop body and set the program counter back to the start of the loop before.    int i = 10;while(i--) {  if(1) continue; /* This gets triggered */  *((int *)NULL) = 0;} /* Then reaches the end of the while loop */            do {} while(); is another loop constructs. These loops execute the body and then check the condition at the bottom of the loop. If the condition is zero, the loop body is not executed and the rest of the program is executed. Otherwise, the loop body is executed.    int i = 1;do {  printf(\"%d\\n\", i--);} while (i &gt; 10) /* Only executed once */            enum is to declare an enumeration. An enumeration is a type that can take on many, finite values. If you have an enum and don’t specify any numerics, the c compiler when generate a unique number for that enum (within the context of the current enum) and use that for comparisons. To declare an instance of an enum, you must say enum &lt;type&gt; varname. The added benefit to this is that C can type check these expressions to make sure that you are only comparing alike types.    enum day{ monday, tuesday, wednesday,  thursday, friday, saturday, sunday};void process_day(enum day foo) {  switch(day) {    case monday:      printf(\"Go home!\\n\"); break;    // ...  }}        It is completely possible to assign enum values to either be different or the same. Just don’t rely on the compiler for consistent numbering. If you are going to use this abstraction, try not to break it.    enum day{   monday = 0,   tuesday = 0,   wednesday = 0,  thursday = 1,   friday = 10,   saturday = 10,   sunday = 0};void process_day(enum day foo) {  switch(day) {    case monday:      printf(\"Go home!\\n\"); break;    // ...  }}            extern is a special keyword that tells the compiler that the variable may be defined in another object file or a library, so the compiler doesn’t throw an error when either the variable is not defined or if the variable is defined twice because the first file will really be referencing the variable in the other file.    // file1.cextern int panic;void foo() {  if (panic) {    printf(\"NONONONONO\");  } else {    printf(\"This is fine\");  }}//file2.cint panic = 1;            for is a keyword that allows you to iterate with an initialization condition, a loop invariant, and an update condition. This is meant to be a replacement for the while loop    for (initialization; check; update) {  //...}// Typicallyint i;for (i = 0; i &lt; 10; i++) {  //...}        One thing to note is that as of the C89 standard, you cannot declare variables inside the for loop. This is because there was a disagreement in the standard for how the scoping rules of a variable defined in the loop would work. It has since been resolved with more recent standards, so people can use the for loop that they know and love today    for(int i = 0; i &lt; 10; ++i) {      }        The order of evaluation for a for loop is as follows                  Perform the initialization condition.                    Check the invariant. If false, terminate the loop and execute the next statement. If true, continue to the body of the loop.                    Perform the body of the loop.                    Perform the update condition.                    Jump to (2).                  goto is a keyword that allows you to do conditional jumps. Do not use goto in your programs. The reason being is that it makes your code infinitely more hard to understand when strung together with multiple chains. It is fine to use in some contexts though. The keyword is usually used in kernel contexts when adding another stack frame for cleanup isn’t a good idea. The canonical example of kernel cleanup is as below.    void setup(void) {  Doe *deer;  Ray *drop;  Mi *myself;  if (!setupdoe(deer)) {    goto finish;  }   if (!setupray(drop)) {    goto cleanupdoe;  }   if (!setupmi(myself)) {    goto cleanupray;  }  perform_action(deer, drop, myself);cleanupray:  cleanup(drop);cleanupdoe:  cleanup(deer);finish:  return;}             if else else-if are control flow keywords. There are a few ways to use these (1) A bare if (2) An if with an else (3) an if with an else-if (4) an if with an else if and else. The statements are always executed from the if to the else. If any of the intermediate conditions are true, the if block performs that action and goes to the end of that block.    // (1)if (connect(...))  return -1;// (2)if (connect(...)) {  exit(-1);} else {  printf(\"Connected!\");}// (3)if (connect(...)) {  exit(-1);} else if (bind(..)) {  exit(-2);}// (1)if (connect(...)) {  exit(-1);} else if (bind(..)) {  exit(-2);} else {  printf(\"Successfully bound!\");}            inline is a compiler keyword that tells the compiler it’s okay not to create a new function in the assembly. Instead, the compile is hinted at substituting the function body directly into the calling function. This is not always recommended explicitly as the compiler is usually smart enough to know when to inline a function for you.    inline int max(int a, int b) {  return a &lt; b ? a : b;}int main() {  printf(\"Max %d\", max(a, b));  // printf(\"Max %d\", a &lt; b ? a : b);}            restrict is a keyword that tells the compiler that this particular memory region shouldn’t overlap with all other memory regions. The use case for this is to tell users of the program that it is undefined behavior if the memory regions overlap.    memcpy(void * restrict dest, const void* restrict src, size_t bytes);void add_array(int *a, int * restrict c) {  *a += *c;}int *a = malloc(3*sizeof(*a));*a = 1; *a = 2; *a = 3;add_array(a + 1, a) // Well definedadd_array(a, a) // Undefined            return is a control flow operator that exits the current function. If the function is void then it simply exits the functions. Otherwise another parameter follows as the return value.    void process() {  if (connect(...)) {    return -1;  } else if (bind(...)) {    return -2  }  return 0;}            signed is a modifier which is rarely used, but it forces an type to be signed instead of unsigned. The reasont that this is so rarely used is because types are signed by default and need to have the unsigned modifier to make them unsigned but it may be useful in cases where you want the compiler to default a signed type like.    int count_bits_and_sign(signed representation) {  //...}            sizeof is an operator that is evaluated at compile time, which evaluates to the number of bytes that the expression contains. Meaning that when the compiler infers the type the following code changes.    char a = 0;printf(\"%zu\", sizeof(a++));        char a = 0;printf(\"%zu\", 1);        Which then the compiler is allowed to operate on further. A note that you must have a complete definition of the type at compile time or else you may get an odd error. Consider the following    // file.cstruct person;printf(\"%zu\", sizeof(person));// file2.cstruct person {  // Declarations}        This code will not compile because sizeof is not able to compile file.c without knowing the full declaration of the person struct. That is typically why we either put the full declaration in a header file or we abstract the creation and the interaction away so that users cannot access the internals of our struct. Also, if the compiler knows the full length of an array object, it will use that in the expression instead of decaying it to a pointer.    char str1[] = \"will be 11\";char* str2 = \"will be 8\";sizeof(str1) //11 because it is an arraysizeof(str2) //8 because it is a pointer        Be careful, using sizeof for the length of a string!        static is a type specifier with three meanings.                  When used with a global variable or function declaration it means that the scope of the variable or the function is only limited to the file.                    When used with a function variable, that declares that the variable has static allocation – meaning that the variable is allocated once at program start up not every time the program is run.              static int i = 0;static int _perform_calculation(void) {  // ...}char *print_time(void) {  static char buffer[200]; // Shared every time a function is called  // ...}            struct is a keyword that allows you to pair multiple types together into a new structure. Structs are contiguous regions of memory that one can access specific elements of each memory as if they were separate variables.    struct hostname {  const char *port;  const char *name;  const char *resource;}; // You need the semicolon at the end// Assign each individuallystruct hostname facebook;facebook.port = \"80\";facebook.name = \"www.google.com\";facebook.resource = \"/\";// You can use static initialization in later versions of cstruct hostname google = {\"80\", \"www.google.com\", \"/\"};            switch case default Switches are essentially glorified jump statements. Meaning that you take either a byte or an integer and the control flow of the program jumps to that location.    switch(/* char or int */) {  case INT1: puts(\"1\");  case INT2: puts(\"2\");  case INT3: puts(\"3\");}        If we give a value of 2 then    switch(2) {  case 1: puts(\"1\"); /* Doesn't run this */  case 2: puts(\"2\"); /* Runs this */  case 3: puts(\"3\"); /* Also runs this */}        The break statement        typedef declares an alias for a type. Often used with structs to reduce the visual clutter of having to write ‘struct’ as part of the type.    typedef float real; real gravity = 10;// Also typedef gives us an abstraction over the underlying type used. // In the future, we only need to change this typedef if we// wanted our physics library to use doubles instead of floats.typedef struct link link_t; //With structs, include the keyword 'struct' as part of the original types        In this class, we regularly typedef functions. A typedef for a function can be this for example    typedef int (*comparator)(void*,void*);int greater_than(void* a, void* b){    return a &gt; b;}comparator gt = greater_than;        This declares a function type comparator that accepts two void* params and returns an integer.        union is a new type specifier. A union is one piece of memory that a bunch of variables occupy. It is used to maintain consistency while having the flexibility to switch between types without mainting functions to keep track of the bits. Consider an example where we have different pixel values.    union pixel {  struct values {    char red;    char blue;    char green;    char alpha;  } values;  uint32_t encoded;}; // Ending semicolon neededunion pixel a;// When modifying or readinga.values.red;a.values.blue = 0x0;// When writing to a filefprintf(picture, \"%d\", a.encoded);            unsigned is a type modifier that forces unsigned behavior in the variables they modify. Unsigned can only be on primitive int types (like int and long). There is a lot of behavior associated with unsigned arthmetic and whatnot, just know for the most part unless you need to do bit shifting you probably won’t need it.        void is a two folded keyword. When used in terms of function or parameter definition then it means that it returns no value or accepts no parameter specifically. The following declares a function that accepts no parameters and returns nothing.    void foo(void);        The other use of void is when you are defining. A void * pointer is just a memory address. It is specified as an incomplete type meaning that you cannot dereference it but it can be promoted to any time to any other type. Pointer arithmetic with these pointer is undefined behavior.    int *array = void_ptr; // No cast needed            volatile is a compiler keyword. This means that the compiler should not optimize its value out. Consider the following simple function.    int flag = 1;pass_flag(&amp;flag);while(flag) {    // Do things unrelated to flag}        The compiler may, since the internals of the while loop have nothing to do with the flag, optimize it to the following even though a function may alter the data.    while(1) {    // Do things unrelated to flag}        If you put the volatile keyword then it forces the compiler to keep the variable in and perform that check. This is particularly useful for cases where you are doing multi-process or multi-threading programs so that we can        while  represents the traditional while loop. There is a condition at the top of the loop. While that condition evaluates to a non-zero value, the loop body will be run.  C data types      char Represents exactly one byte of data. The number of bits in a byte might vary. unsigned char and signed char mean the exact same thing. This must be aligned on a boundary (meaning you cannot use bits in between two addresses). The rest of the types will assume 8 bits in a byte.        short (short int) must be at least two bytes. This is aligned on a two byte boundary, meaning that the address must be divisble by two.        int must be at least two bytes. Again aligned to a two byte boundary . On most machines this will be 4 bytes.        long (long int) must be at least four bytes, which are aligned to a four byte boundary. On some machines this can be 8 bytes.        long long must be at least eight bytes, aligned to an eight byte boundary.        float represents an IEEE-754 single percision floating point number tightly specified by IEEE . This will be four bytes aligned to a four byte boundary on most machines.        double represents an IEEE-754 double percision floating point number specified by the same standard, which is aligned to the nearest eight byte boundary.  OperatorsOperators are language constructs in C that are defined as part of the grammar of the language.      [] is the subscript operator. a[n] == (a + n)* where n is a number type and a is a pointer type.        -&gt; is the structure dereference operator. If you have a pointer to a struct *p, you can use this to access one of its elements. p-&gt;element.        . is the structure reference operator. If you have an object on the stack a then you can access an element a.element.        +/-a is the unary plus and minus operator. They either keep or negate the sign, respectively, of the integer or float type underneath.        *a is the dereference operator. If you have a pointer *p, you can use this to access the element located at this memory address. If you are reading, the return value will be the size of the underlying type. If you are writing, the value will be written with an offset.        &amp;a is the addressof operator. This takes the an element and returns its address.        ++ is the increment operator. You can either take it prefix or postfix, meaning that the variable that is being incremented can either be before or after the operator. a = 0; ++a === 1 and a = 1; a++ === 0.        – is the decrement operator. Same semantics as the increment operator except with decreasing the value by one.        sizeof is the sizeof operator. This is also mentioned in the keywords section.        a &lt;mop&gt; b where ``&lt;mop&gt;=+, -, *, %, / are the mathematical binary operators. If the operands are both number types, then the operations are plus, minus, times, modulo, and division respectively. If the left operand is a pointer and the right operand is an integer type, then only plus or minux may be used and the rules for pointer arithmetic are invoked.        &gt;&gt;/&lt;&lt; are the bit shift operators. The operand on the right has to be an integer type whose signedness is ignored unless it is signed negative in which case the behavior is undefined. The operator on the left decides a lot of semantics. If we are left shifting, there will always be zeros introduced on the right. If we are right shifting there are a few different cases                  If the operand on the left is signed, then the integer is sign extended. This means that if the number has the sign bit set, then any shift right will introduce ones on the left. If the number does not have the sign bit set, any shift right will introduce zeros on the left.                    If the operand is unsigned, zeros will be introduced on the left either way.              unsigned short uns = -127; // 1111111110000001short sig = 1; // 0000000000000001uns &lt;&lt; 2; // 1111111000000100sig &lt;&lt; 2; // 0000000000000100uns &gt;&gt; 2; // 1111111111100000sig &gt;&gt; 2; // 0000000000000000            &lt;=/&gt;= are the greater than equal to/less than equal to operators. They do as the name implies.        &lt;/&gt; are the greater than/less than operators. They again do as the name implies.        ==/= are the equal/not equal to operators. They once again do as the name implies.        &amp;&amp; is the logical and operator. If the first operand is zero, the second won’t be evaluated and the expression will evaluate to 0. Otherwise, it yields a 1-0 value of the second operand.        || is the logical or operator. If the first operand is not zero, then second won’t be evaluated and the expression will evaluate to 1. Otherwise, it yields a 1-0 value of the second operand.        ! is the logical not operator. If the operand is zero, then this will return 1. Otherwise, it will return 0.        &amp; If a bit is set in both operands, it is set in the output. Otherwise, it is not.        | If a bit is set in either operand, it is set in the output. Otherwise, it is not.        ~ If a bit is set in the input, it will not be set in the output and vice versa.        ?: is the tertinary operator. You put a boolean condition before the and if it evaluates to non-zero the element before the colon is returned otherwise the element after is. 1 ? a : b === a and 0 ? a : b === b.        a, b is the comma operator. a is evaluated and then b is evaluated and b is returned.  Common C FunctionsTo find more information about any functions, use the man pages. Note the man pages are organized into sections. Section 2 are System calls. Section 3 are C libraries. On the web, Google man 7 open. In the shell, man -S2 open or man -S3 printfInput/Outputprintf is the function with which most people are familiar. The first parameter is a format string that includes placeholders for the data to be printed. Common format specifiers are %s&lt;/span&gt; treat the argument as a c string pointer, keep printing all characters until the NULL-character is reached; &lt;span&gt;%d print the argument as an integer; ``%p print the argument as a memory address. By default, for performance, printf does not actually write anything out until its buffer is full or a newline is printed.char *name = ... ; int score = ...;printf(\"Hello %s, your result is %d\\n\", name, score);printf(\"Debug: The string and int are stored at: %p and %p\\n\", name, &amp;score );// name already is a char pointer and points to the start of the array. // We need \"&amp;\" to get the address of the int variableprintf calls the system call write. printf includes an internal buffer so, to increase performance printf may not call write everytime you call printf. printf is a C library function. write is a system call and as we know system calls are expensive. On the other hand, printf uses a buffer which suits our needs better at that pointTo print strings and single characters use puts( name ) and putchar( c ) where name is a pointer to a C string and c is just a charputs(\"Current selection: \");putchar('1');To print to other file streams use ``fprintf( _file_ , “Hello %s, score: %d”, name, score); Where _file_ is either predefined ‘stdout’ ‘stderr’ or a FILE pointer that was returned by fopen or fdopen. You can also use file descriptors in the printf family of functions! Just use dprintf(int fd, char* format_string, ...); Just remember the stream may be buffered, so you will need to assure that the data is written to the file descriptor.To print data into a C string, use sprintf or better snprintf. snprintf returns the number of characters written excluding the terminating byte. In the above example, this would be a maximum of 199. We would use sprintf in cases where we know that the size of the string will not be anything more than a certain fixed amount (think about printing an integer, it will never be more than 11 characters with the null byte)// Fixedchar int_string[20];sprintf(int_string, \"%d\", integer);// Variable lengthchar result[200];int len = snprintf(result, sizeof(result), \"%s:%d\", name, score);If I want to printf to call write without a newline fflush( FILE* inp ). The contents of the file will be written. If I wanted to write “Hello World” with no newline, I could write it like this.int main(){    fprintf(stdout, \"Hello World\");    fflush(stdout);    return 0;}In addition to the printf family, there is gets. gets is deprecated in C99 standard and has been removed from the latest C standard (C11). Programs should use fgets or getline instead.char *fgets (char *str, int num, FILE *stream); ssize_t getline(char **lineptr, size_t *n, FILE *stream);// Example, the following will not read more than 9 charschar buffer[10];char *result = fgets(buffer, sizeof(buffer), stdin);The result is NULL if there was an error or the end of the file is reached. Note, unlike gets, fgets copies the newline into the buffer, which you may want to discard. On the other hand, one of the advantages of getline is that will automatically (re-) allocate a buffer on the heap of sufficient size.// ssize_t getline(char **lineptr, size_t *n, FILE *stream); /* set buffer and size to 0; they will be changed by getline */char *buffer = NULL;size_t size = 0;ssize_t chars = getline(&amp;buffer, &amp;size, stdin);// Discard newline character if it is present,if (chars &gt; 0 &amp;&amp; buffer[chars-1] == '\\n')     buffer[chars-1] = '\\0';// Read another line.// The existing buffer will be re-used, or, if necessary,// It will be `free`'d and a new larger buffer will `malloc`'dchars = getline(&amp;buffer, &amp;size, stdin);// Later... don't forget to free the buffer!free(buffer);In addition to those functions, we have perror that has a two-fold meaning. Let’s say that you have a function call that just failed because you checked the man page and it is a failing return code.perror(const char* message) will print the English version of the error to stderr.int main(){    int ret = open(\"IDoNotExist.txt\", O_RDONLY);    if(ret &lt; 0){        perror(\"Opening IDoNotExist:\");    }    //...    return 0;}To have a library function parse input, use scanf (or fscanf or sscanf) to get input from the default input stream, an arbitrary file stream or a C string respectively. It’s a good idea to check the return value to see how many items were parsed. scanf functions require valid pointers. It’s a common source of error to pass in an incorrect pointer value. For example,int *data = (int *) malloc(sizeof(int));char *line = \"v 10\";char type;// Good practice: Check scanf parsed the line and read two values:int ok = 2 == sscanf(line, \"%c %d\", &amp;type, &amp;data); // pointer errorWe wanted to write the character value into c and the integer value into the malloc’d memory. However, we passed the address of the data pointer, not what the pointer is pointing to! So sscanf will change the pointer itself. i.e. the pointer will now point to address 10 so this code will later fail e.g. when free(data) is called.Now, scanf will just keep reading characters until the string ends. To stop scanf from causing a buffer overflow, use a format specifier. Make sure to pass one less than the size of the buffer.char buffer[10];scanf(\"%9s\", buffer); // reads up to 9 charactes from input (leave room for the 10th byte to be the terminating byte)string.hMore information about all of these functions. Any behavior not in the docs like passing strlen(NULL) is considered undefined behavior.      int strlen(const char s) returns the length of the string not including the null byte        int strcmp(const char s1, const char s2) returns an integer determining the lexicographic order of the strings. If s1 where to come before s2 in a dictionary, then a -1 is returned. If the two strings are equal, then 0. Else, 1.        char strcpy(char dest, const char src) Copies the string at src to dest. assumes dest has enough space for src        char strcat(char dest, const char src) Concatenates the string at src to the end of destination. This function assumes that there is enough space for src at the end of destination including the NULL byte        char strdup(const char dest) Returns a malloc’ed copy of the string.        char strchr(const char haystack, int needle) Returns a pointer to the first occurrence of needle in the haystack. If none found, NULL is returned.        char strstr(const char haystack, const char needle) Same as above but this time a string!        char *strtock(const char *str, const char *delims)    A dangerous but useful function strtok takes a string and tokenizes it. Meaning that it will transform the strings into separate strings. This function has a lot of specs so please read the man pages a contrived example is below.    #include &lt;stdio.h&gt;#include &lt;string.h&gt;int main(){    char* upped = strdup(\"strtok,is,tricky,!!\");    char* start = strtok(upped, \",\");    do{        printf(\"%s\\n\", start);    }while((start = strtok(NULL, \",\")));    return 0;}        Output    strtokistricky!!        What happens when I change upped like this?    char* upped = strdup(\"strtok,is,tricky,,,!!\");            For integer parsing use long int strtol(const char nptr, char *endptr, int base); or long long int strtoll(const char nptr, char *endptr, int base);.    What these functions do is take the pointer to your string *nptr and a base (ie binary, octal, decimal, hexadecimal etc) and an optional pointer endptr and returns a parsed value.    int main(){    const char *nptr = \"1A2436\";    char* endptr;    long int result = strtol(nptr, &amp;endptr, 16);    return 0;}        Be careful though! Error handling is tricky because the function won’t return an error code. If you give it a string that is not a number it will return 0. This means you cant differentiate between a valid “0” and an invalid string. See the man page for more details on strol behavior with invalid and out of bounds values. A safer alternative is use to sscanf (and check the return value).    int main(){    const char *input = \"0\"; // or \"!##@\" or \"\"    char* endptr;    long int parsed = strtol(input, &amp;endptr, 10);    if(parsed == 0){        // Either the input string was not a valid base-10 number or it really was zero!    }    return 0;}            void memcpy(void dest, const void src, size_t n) moves n bytes starting at src to dest. Be careful, there is undefined behavior when the memory regions overlap. This is one of the classic works on my machine examples because many times valgrind won’t be able to pick it up because it will look like it works on your machine. When the autograder hits, fail. Consider the safer version below.        void memmove(void dest, const void src, size_t n) does the same thing as above, but if the memory regions overlap then it is guaranteed that all the bytes will get copied over correctly. memcpy and memmove both in &lt;string.h&gt;? Because strings are essentially raw memory with a null byte at the end of them!  Conventions/ErrnoConventions and errno, talk about what the unix conventions are for processes and what the errno conventions areSystem CallsSystem Calls, talk about what a system call is and an aside for the practicalities of system calls and what actually happensC Memory ModelStructsIn low-level terms, a struct is just a piece of contiguous memory, nothing more. Just like an array, a struct has enough space to keep all of its members. But unlike an array, it can store different types. Consider the contact struct declared abovestruct contact {    char firstname[20];    char lastname[20];    unsigned int phone;};struct contact bhuvan;/* a lot of times we will do the following typdef so we can just write contact contact1 */typedef struct contact contact;contact bhuvan;/* You can also declare the struct like this to get it done in one statement */typedef struct optional_name {    ...} contact;If you compile the code without any optimizations and reordering, you can expect the addresses of each of the variables to look like this.&amp;bhuvan           // 0x100&amp;bhuvan.firstname // 0x100 = 0x100+0x00&amp;bhuvan.lastname  // 0x114 = 0x100+0x14&amp;bhuvan.phone     // 0x128 = 0x100+0x28Because all your compiler does is say ‘hey reserve this much space, and I will go and calculate the offsets of whatever variables you want to write to’. The offsets are where the variable starts at. The phone variables starts at the 0x128th bytes and continues for sizeof(int) bytes, but not always. Offsets don’t determine where the variable ends though. Consider the following hack that you see in a lot of kernel code.typedef struct {    int length;    char c_str[0];} string;const char* to_convert = \"bhuvan\";int length = strlen(to_convert);// Let's convert to a c stringstring* bhuvan_name;bhuvan_name = malloc(sizeof(string) + length+1);/*Currently, our memory looks like this with junk in those black spaces                ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ bhuvan_name = |___|___|___|___|___|___|___|___|___|___|___|                                                           */bhuvan_name-&gt;length = length;/*This writes the following values to the first four bytesThe rest is still garbage                ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ bhuvan_name = | 0 | 0 | 0 | 6 |___|___|___|___|___|___|___|                                                           */strcpy(bhuvan_name-&gt;c_str, to_convert);/*Now our string is filled in correctly at the end of the struct                ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ____ bhuvan_name = | 0 | 0 | 0 | 6 | b | h | u | v | a | n | \\0 |                                                           ‾*/strcmp(bhuvan_name-&gt;c_str, \"bhuvan\") == 0 //The strings are equal!Extra: Struct packingStructs may require something called padding (tutorial). We do not expect you to pack structs in this course, just know that it is there This is because in the early days (and even now) when you have to an address from memory you have to do it in 32bit or 64bit blocks. This also meant that you could only request addresses that were multiples of that. Meaning thatstruct picture{    int height;    pixel** data;    int width;    char* enconding;}// You think picture looks like this. One box is four bytes| h   data    w   encod ||___ ___ ___ ___ ___ ___||___|___ ___|___|___ ___|Would conceptually look like thisstruct picture{    int height;    char slop1[4];    pixel** data;    int width;    char slop2[4];    char* enconding;}| h   ?   data    w   ?   encod ||___ ___ ___ ___ ___ ___ ___ ___||___|___|___ ___|___|___|___ ___|This is on a 64-bit system. This is not always the case because sometimes your processor supports unaligned accesses. What does this mean? Well there are two options you can set an attributestruct __attribute__((packed, aligned(4))) picture{    int height;    pixel** data;    int width;    char* enconding;}// Will look like this| h   data    w   encod ||___ ___ ___ ___ ___ ___||___|___ ___|___|___ ___|But now, every time I want to access data or encoding, I have to do two memory accesses. The other thing you can do is reorder the struct, although this is not always possiblestruct picture{    int height;    int width;    pixel** data;    char* enconding;}// You think picture looks like this| h   w    data   encod ||___ ___ ___ ___ ___ ___||___|___|___ ___|___ ___|Strings in CIn C we have Null Terminated strings rather than Length Prefixed for historical reasons. What that means for your average everyday programming is that you need to remember the null character! A string in C is defined as a bunch of bytes until you reach ‘’ or the Null Byte.Two places for stringsWhenever you define a constant string (ie one in the form char* str = constant) That string is stored in the data or lstlisting segment that is read-only meaning that any attempt to modify the string will cause a segfault.If one, however, malloc’s space, one can change that string to be whatever they want. Forgetting to NULL terminate a string is a big affect on the strings! Bounds checking is important. The heart bleed bug mentioned earlier in the wiki book is partially because of this.Strings in C are represented as characters in memory. The end of the string includes a NULL (0) byte. So “ABC” requires four(4) bytes [A,B,C,\\0. The only way to find out the length of a C string is to keep reading memory until you find the NULL byte. C characters are always exactly one byte each.When you write a string literal ABC in an expression the string literal evaluates to a char pointer (char ), which points to the first byte/char of the string. This means ptr in the example below will hold the memory address of the first character in the string.String constants are constantchar array[] = \"Hi!\"; // array contains a mutable copy strcpy(array, \"OK\");char *ptr = \"Can't change me\"; // ptr points to some immutable memorystrcpy(ptr, \"Will not work\");String literals are character arrays stored in the code segment of the program, which is immutable. Two string literals may share the same space in memory. An example follows:char *str1 = \"Brandon Chong is the best TA\";char *str2 = \"Brandon Chong is the best TA\";The strings pointed to by str1 and str2 may actually reside in the same location in memory.Char arrays, however, contain the literal value which has been copied from the code segment into either the stack or static memory. These following char arrays do not reside in the same place in memory.char arr1[] = \"Brandon Chong didn't write this\";char arr2[] = \"Brandon Chong didn't write this\";char *ptr = \"ABC\"Some common ways to initialize a string include:char *str = \"ABC\";char str[] = \"ABC\";char str[]={'A','B','C','\\0'};char ary[] = \"Hello\";char *ptr = \"Hello\";ExampleThe array name points to the first byte of the array. Both ary and ptr can be printed out:char ary[] = \"Hello\";char *ptr = \"Hello\";// Print out address and contentsprintf(\"%p : %s\\n\", ary, ary);printf(\"%p : %s\\n\", ptr, ptr);The array is mutable, so we can change its contents. Be careful not to write bytes beyond the end of the array though. Fortunately, “World” is no longer than “Hello”In this case, the char pointer ptr points to some read-only memory (where the statically allocated string literal is stored), so we cannot change those contents.strcpy(ary, \"World\"); // OKstrcpy(ptr, \"World\"); // NOT OK - Segmentation fault (crashes)We can, however, unlike the array, we change ptr to point to another piece of memory,ptr = \"World\"; // OK!ptr = ary; // OK!ary = (..anything..) ; // WONT COMPILE// ary is doomed to always refer to the original array.printf(\"%p : %s\\n\", ptr, ptr);strcpy(ptr, \"World\"); // OK because now ptr is pointing to mutable memory (the array)What to take away from this is that pointers * can point to any type of memory while C arrays [] can only point to memory on the stack. In a more common case, pointers will point to heap memory in which case the memory referred to by the pointer CAN be modified.PointersPointer BasicsDeclaring a PointerA pointer refers to a memory address. The type of the pointer is useful - it tells the compiler how many bytes need to be read/written. You can declare a pointer as follows.int *ptr1;char *ptr2;Due to C’s grammar, an int* or any pointer is not actually its own type. You have to precede each pointer variable with an asterisk. As a common gotcha, the followingint* ptr3, ptr4;Will only declare *ptr3 as a pointer. ptr4 will actually be a regular int variable. To fix this declaration, keep the * preceding to the pointerint *ptr3, *ptr4;Keep this in mind for structs as well. If one does not typedef them, then the pointer goes after the type.struct person *ptr3;Reading/Writing with pointersLet’s say that we declare a pointer int ptr. For the sake of discussion, let’s say that ptr points to memory address 0x1000. If we want to write to a pointer, we can dereference and assign *ptr.*ptr = 0; // Writes some memory.What C will do is take the type of the pointer which is an int and writes sizeof(int) bytes from the start of the pointer, meaning that bytes 0x1000, 0x1001, 0x1002, 0x1003 will all be zero. The number of bytes written depends on the pointer type. It is the same for all primitive types but structs are a little different.Pointer ArithmeticYou can add an integer to a pointer. However, the pointer type is used to determine how much to increment the pointer. For char pointers this is trivial because characters are always one byte:char *ptr = \"Hello\"; // ptr holds the memory location of 'H'ptr += 2; //ptr now points to the first'l'If an int is 4 bytes then ptr+1 points to 4 bytes after whatever ptr is pointing at.char *ptr = \"ABCDEFGH\";int *bna = (int *) ptr;bna +=1; // Would cause iterate by one integer space (i.e 4 bytes on some systems)ptr = (char *) bna;printf(\"%s\", ptr);/* Notice how only 'EFGH' is printed. Why is that? Well as mentioned above, when performing 'bna+=1' we are increasing the **integer** pointer by 1, (translates to 4 bytes on most systems) which is equivalent to 4 characters (each character is only 1 byte)*/return 0;Because pointer arithmetic in C is always automatically scaled by the size of the type that is pointed to, you can’t perform pointer arithmetic on void pointers.You can think of pointer arithmetic in C as essentially doing the followingIf I want to doint *ptr1 = ...;int *offset = ptr1 + 4;Thinkint *ptr1 = ...;char *temp_ptr1 = (char*) ptr1;int *offset = (int*)(temp_ptr1 + sizeof(int)*4);To get the value. Every time you do pointer arithmetic, take a deep breath and make sure that you are shifting over the number of bytes you think you are shifting over.What is a void pointer?A pointer without a type (very similar to a void variable). Void pointers are used when either a datatype you’re dealing with is unknown or when you’re interfacing C code with other programming languages. You can think of this as a raw pointer, or just a memory address. You cannot directly read or write to it because the void type does not have a size. For Examplevoid *give_me_space = malloc(10);char *string = give_me_space;This does not require a cast because C automatically promotes void* to its appropriate type. Note:gcc and clang are not total ISO-C compliant, meaning that they will let you do arithmetic on a void pointer. They will treat it as a char * pointer. Do not do this because it may not work with all compilers!ShellLook in the appendix for Life in the Terminal!Common Bugsvoid mystrcpy(char*dest, char* src) {   // void means no return value     while( *src ) { dest = src; src ++; dest++; }  }In the above code it simply changes the dest pointer to point to source string. Also the nuls bytes are not copied. Here’s a better version -  while( *src ) { *dest = *src; src ++; dest++; }   *dest = *src;Note it’s also usual to see the following kind of implementation, which does everything inside the expression test, including copying the nul byte.  while( (*dest++ = *src++ )) {};Double FreesA double free error is when you accidentally attempt to free the same allocation twice.int *p = malloc(sizeof(int));free(p);*p = 123; // Oops! - Dangling pointer! Writing to memory we don't own anymorefree(p); // Oops! - Double free!The fix is first to write correct programs! Secondly, it’s good programming hygiene to reset pointers once the memory has been freed. This ensures the pointer can’t be used incorrectly without the program crashing.Fix:p = NULL; // Now you can't use this pointer by mistakeReturning pointers to automatic variablesint *f() {    int result = 42;    static int imok;    return &amp;imok; // OK - static variables are not on the stack    return &amp;result; // Not OK}Automatic variables are bound to stack memory only for the lifetime of the function. After the function returns it is an error to continue to use the memory. ## Insufficient memory allocationstruct User {   char name[100];};typedef struct User user_t;user_t *user = (user_t *) malloc(sizeof(user));In the above example, we needed to allocate enough bytes for the struct. Instead, we allocated enough bytes to hold a pointer. Once we start using the user pointer we will corrupt memory. The correct code is shown below.struct User {   char name[100];};typedef struct User user_t;user_t * user = (user_t *) malloc(sizeof(user_t));Buffer overflow/ underflowFamous example: Heart Bleed (performed a memcpy into a buffer that was of insufficient size). Simple example: implement a strcpy and forget to add one to strlen, when determining the size of the memory required.#define N (10)int i = N, array[N];for( ; i &gt;= 0; i--) array[i] = i;C does not check that pointers are valid. The above example writes into array[10] which is outside the array bounds. This can cause memory corruption because that memory location is probably being used for something else. In practice, this can be harder to spot because the overflow/underflow may occur in a library call e.g.gets(array); // Let's hope the input is shorter than my array!Strings require strlen(s)+1 bytesEvery string must have a null byte after the last characters. To store the string “Hi” it takes 3 bytes: [H] [i] [.  char *strdup(const char *input) {  /* return a copy of 'input' */    char *copy;    copy = malloc(sizeof(char*));     /* nope! this allocates space for a pointer, not a string */    copy = malloc(strlen(input));     /* Almost...but what about the null terminator? */    copy = malloc(strlen(input) + 1); /* That's right. */    strcpy(copy, input);   /* strcpy will provide the null terminator */    return copy;}Using uninitialized variablesint myfunction() {  int x;  int y = x + 2;...Automatic variables hold garbage (whatever bit pattern happened to be in memory). It is an error to assume that it will always be initialized to zero.Assuming Uninitialized memory will be zeroedvoid myfunct() {   char array[10];   char *p = malloc(10);Automatic (temporary variables) are not automatically initialized to zero. Heap allocations using malloc are not automatically initialized to zero.Logic and Program flow mistakesEqual vs. equalityint answer = 3; // Will print out the answer.if (answer = 42) { printf(\"I've solved the answer! It's %d\", answer);}Undeclared or incorrectly prototyped functionstime_t start = time();The system function ‘time’ actually takes a parameter (a pointer to some memory that can receive the time_t structure). The compiler did not catch this error because the programmer did not provide a valid function prototype by including time.hExtra Semicolonsfor(int i = 0; i &lt; 5; i++) ; printf(\"I'm printed once\");while(x &lt; 10); x++ ; // X is never incrementedHowever, the following code is perfectly OK.for(int i = 0; i &lt; 5; i++){    printf(\"%d\\n\", i);;;;;;;;;;;;;}It is OK to have this kind of code, because the C language uses semicolons (;) to separate statements. If there is no statement in between semicolons, then there is nothing to do and the compiler moves on to the next statementTopics      C Strings representation        C Strings as pointers        char p[]vs char* p        Simple C string functions (strcmp, strcat, strcpy)        sizeof char        sizeof x vs x*        Heap memory lifetime        Calls to heap allocation        Deferencing pointers        Address-of operator        Pointer arithmetic        String duplication        String truncation        double-free error        String literals        Print formatting.        memory out of bounds errors        static memory        fileio POSIX vs. C library        C io fprintf and printf        POSIX file IO (read, write, open)        Buffering of stdout  Questions/Exercises      What does the following print out?    int main(){fprintf(stderr, \"Hello \");fprintf(stdout, \"It's a small \");fprintf(stderr, \"World\\n\");fprintf(stdout, \"place\\n\");return 0;}            What are the differences between the following two declarations? What does sizeof return for one of them?    char str1[] = \"bhuvan\";char *str2 = \"another one\";            What is a string in c?        Code up a simple my_strcmp. How about my_strcat, my_strcpy, or my_strdup? Bonus: Code the functions while only going through the strings once.        What should the following usually return?    int *ptr;sizeof(ptr);sizeof(*ptr);            What is malloc? How is it different than calloc. Once memory is malloced how can I use realloc?        What is the &amp; operator? How about *?        Pointer Arithmetic. Assume the following addresses. What are the following shifts?    char** ptr = malloc(10); //0x100ptr[0] = malloc(20); //0x200ptr[1] = malloc(20); //0x300                      ptr + 2                    ptr + 4                    ptr[0] + 4                    ptr[1] + 2000                    *((int)(ptr + 1)) + 3                  How do we prevent double free errors?        What is the printf specifier to print a string, int, or char?        Is the following code valid? If so, why? Where is output located?    char *foo(int var){static char output[20];snprintf(output, 20, \"%d\", var);return output;}            Write a function that accepts a string and opens that file prints out the file 40 bytes at a time but every other print reverses the string (try using POSIX API for this).        What are some differences between the POSIX filedescriptor model and C’s FILE* (ie what function calls are used and which is buffered)? Does POSIX use C’s FILE* internally or vice versa?  "
  },{
    "title": "Introduction",
    "url": " /coursebook/Introduction",
   "content": "  Introduction[1][]  IntroductionHey Everyone! Thanks for checking out the coursebook. This project is very much still in beta but has been released to everyone, so we can iterate on it this semester and fill in any gaps for the coming semesters. A few things to note      This book strives for being more readable than compatible with the lectures, meaning that one lecture can go over many different sections of the book at various levels of resolution. The book is meant to be read through and make sense on a read through.        This is still a collaborative effort, if you have thoughts, comments, suggestions, and fixes: please let us know!  "
  },{
    "title": "Interprocess Communication",
    "url": " /coursebook/Ipc",
   "content": "  Interprocess Communication          MMU and Translating Addresses                  Terminology          Multi-level page tables          Page Table Disadvantages                    Advanced Frames and Page Protections                  Read-only bit          Dirty bit          Execution bit          Page Faults                    Pipes                  Pipe Gotchas          Why is my pipe hanging?          Race condition with named pipes          What is filling up the pipe? What happens when the pipe becomes full?          Are pipes process safe?          The lifetime of pipes          Want to use pipes with printf and scanf? Use fdopen!          When do I need two pipes?                    Named Pipes                  How do I create named pipes?          Two types of files          How do I tell how large a file is?          But try not to do this          What happens if a child process closes a filestream using fclose or close?          How about mmap for files?          For every mmap                    [1][]  Interprocess CommunicationEpigraphIn very simple embedded systems and early computers, processes directly access memory i.e. “Address 1234” corresponds to a particular byte stored in a particular part of physical memory. For example the IBM 709 had to read and write directly to a tape with no level of abstraction . Even in systems after that, it was hard to adopt virtual memory because virtual memory required the whole fetch cycle to be altered through hardware – a change many manufacturers still thought was expensive. In the PDP-10, a workaround was used by using different registers for each process and then virtual memory was added later. In modern systems, this is no longer the case. Instead each process is isolated, and there is a translation process between the address of a particular CPU instruction or piece of data of a process and the actual byte of physical memory (“RAM”). Memory addresses no longer map to physical addresses; the process runs inside virtual memory. Virtual memory not only keeps processes safe (because one process cannot directly read or modify another process’s memory) it also allows the system to efficiently allocate and re-allocate portions of memory to different processes. The modern process of translating memory is as follows.      A process makes a memory request        The circuit first checks the Translation Lookaside Buffer (TLB) if the address page is cached into memory. It skips to the reading from/writing to phase if found otherwise the request goes to the MMU.        The Memory Management Unit (MMU) performs the address translation. If the translation succeeds (more on that later), the page get pulled from RAM – conceptually the entire page isn’t loaded up. The result is cached in the TLB.        The CPU performs the operation by either reading from the physical address or writing to the address.  MMU and Translating AddressesThe Memory Management Unit is part of the CPU, and it converts a virtual memory address into a physical address. There is a sort of pseudocode associated with the MMU.      Receive address        Try to translate address according to the programmed scheme        If the translation fails, report an invalid address        Otherwise,                  If the page exists in memory, check if the process has permissions to perform the operation on the page meaning the process has access to the page, and it is reading from the page/writing to a page that is not marked as read only.                              If so then provide the address, cache the results in the TLB                                Otherwise trigger a hardware interrupt. The kernel will most likely send a SIGSEGV or a Segmentation Violation.                                      If the page doesn’t exist in memory, generate an Interrupt.                              The kernel could realize that this page could either be not allocated or on disk. If it fits the mapping, allocate the page and try the operation again.                                Otherwise, this is an invalid access and the kernel will most likely send a SIGSEGV to the process.                              Imagine you had a 32 bit machine, meaning pointers are 32 bits. They can address (2^{32}) different locations or 4GB of memory where one address is one byte. Imagine we had a large table - here’s the clever part - stored in memory! For every possible address (all 4 billion of them) we will store the ‘real’ i.e.  physical address. Each physical address will need 4 bytes (to hold the 32 bits). This scheme would require 16 billion bytes to store all of entries. Oops - our lookup scheme would consume all of the memory that we could possibly buy for our 4GB machine. We need to do better than this. Our lookup table better be smaller than the memory we have otherwise we will have no space left for our actual programs and operating system data. The solution is to chunk memory into small regions called ‘pages’ and ‘frames’ and use a lookup table for each page.A page is a block of virtual memory. A typical block size on Linux operating system is 4KB or (2^{12}) addresses, though you can find examples of larger blocks. So rather than talking about individual bytes we can talk about blocks of 4KBs, each block is called a page. We can also number our pages (“Page 0” “Page 1” etc). Let’s do a sample calculation of how many pages are there assume page size of 4KB.For a 32 bit machine, (2^{32}) address / (2^{12}) (address/page) = (2^{20}) pages.For a 64 bit machine, (2^{64}) / (2^{12}) = (2^{52}), which is roughly (10^{15}) pages.TerminologyA frame (or sometimes called a ‘page frame’) is a block of physical memory or RAM (=Random Access Memory). This kind of memory is occasionally called ‘primary storage’ in contrast with lower, secondary storage such as spinning disks that have lower access times. A frame is the same number of bytes as a virtual page. If a 32 bit machine has (2^{32} B) of RAM, then there will be the same number of them in the addressable space of the machine. It’s unlikely that a 64 bit machine will ever have (2^{64}) bytes of RAM.A page table is a mapping between a page to the frame. For example Page 1 might be mapped to frame 45, page 2 mapped to frame 30. Other frames might be currently unused or assigned to other running processes, or used internally by the operating system.A simple page table could be imagined as an array.int frame_num = table[page_num]; For a 32 bit machine with 4KB pages, each entry needs to hold a frame number - i.e. 20 bits because we calculated there are (2^{20}) frames. That’s 2.5 bytes per entry! In practice, we’ll round that up to 4 bytes per entry and find a use for those spare bits. With 4 bytes per entry x (2^{20}) entries = 4 MB of physical memory are required to hold the page table For a 64 bit machine with 4KB pages, each entry needs 52 bits. Let’s round up to 64 bits (8 bytes) per entry. With (2^{52}) entries thats (2^{55}) bytes (roughly 40 peta bytes…) Oops our page table is too large In 64 bit architectures memory addresses are sparse, so we need a mechanism to reduce the page table size, given that most of the entries will never be used.An offset take a particular page and looks up a byte by adding it to the start of the page. Remember our page table maps pages to frames, but each page is a block of contiguous addresses. How do we calculate which particular byte to use inside a particular frame? The solution is to re-use the lowest bits of the virtual memory address directly. For example, suppose our process is reading the following address- VirtualAddress = 11110000111100001111000010101010 (binary)On a machine with page size 256 Bytes, then the lowest 8 bits (10101010) will be used as the offset. The remaining upper bits will be the page number (111100001111000011110000).Multi-level page tablesMulti-level pages are one solution to the page table size issue for 64 bit architectures. We’ll look at the simplest implementation - a two level page table. Each table is a list of pointers that point to the next level of tables, not all sub-tables need to exist. An example, two level page table for a 32 bit architecture is shown below-VirtualAddress = 11110000111111110000000010101010 (binary)                 |-Index1-||        ||          | 10 bit Directory index                           |-Index2-||          | 10 bit Sub-table index                                     |--offset--| 12 bit offset (passed directly to RAM)In the above scheme, determining the frame number requires two memory reads: The topmost 10 bits are used in a directory of page tables. If 2 bytes are used for each entry, we only need 2KB to store this entire directory. Each subtable will point to physical frames (i.e. required 4 bytes to store the 20 bits). However, for processes with only tiny memory needs, we only need to specify entries for low memory address (for the heap and program code) and high memory addresses (for the stack). Each subtable is 1024 entries x 4 bytes i.e. 4KB for each subtable.Thus the total memory overhead for our multi-level page table has shrunk from 4MB (for the single level implementation) to 3 frames of memory (12KB) ! Here’s why: We need at least one frame for the high level directory and two frames for just two sub-tables. One sub-table is necessary for the low addresses (program code, constants and possibly a tiny heap), the other sub-table is for higher addresses used by the environment and stack. In practice, real programs will likely need more sub-table entries, as each subtable can only reference 1024*4KB = 4MB of address space but the main point still stands - we have significantly reduced the memory overhead required to perform page table look ups.Page Table DisadvantagesYes - Significantly ! (But thanks to clever hardware, usually no…) Compared to reading or writing memory directly. For a single page table, our machine is now twice as slow! (Two memory accesses are required) For a two-level page table, memory access is now three times as slow. (Three memory accesses are required)To overcome this overhead, the MMU includes an associative cache of recently-used virtual-page-to-frame lookups. This cache is called the TLB (“translation lookaside buffer”). Everytime a virtual address needs to be translated into a physical memory location, the TLB is queried in parallel to the page table. For most memory accesses of most programs, there is a significant chance that the TLB has cached the results. However if a program does not have good cache coherence (for example is reading from random memory locations of many different pages) then the TLB will not have the result cache and now the MMU must use the much slower page table to determine the physical frame.This may be how one splits up a multi level page table.Advanced Frames and Page ProtectionsFrames be shared between processes! In addition to storing the frame number, the page table can be used to store whether a process can write or only read a particular frame. Read only frames can then be safely shared between multiple processes. For example, the C-library instruction code can be shared between all processes that dynamically load the code into the process memory. Each process can only read that memory. Meaning that if you try to write to a read-only page in memory you will get a SEGFAULT. That is why sometimes memory accesses segfault and sometimes they don’t, it all depends on if your hardware says that you can access.In addition, processes can share a page with a child process using the mmap system call. mmap is an interesting call because instead of tying each virtual address to a physical frame, it ties it to something else. That something else can be a file, a GPU unit, or any other memory mapped operation that you can think of! Writing to the memory address may write through to the device or the write may be paused by the operating system but this is a very powerful abstraction because often the operating system is able to perform optimizations (multiple processes memory mapping the same file can have the kernel create one mapping). In addition, it is common to store at least read-only, modification and execution information.Read-only bitThe read-only bit marks the page as read-only. Attempts to write to the page will cause a page fault. The page fault will then be handled by the Kernel. Two examples of the read-only page include sharing the c runtime library between multiple processes (for security you wouldn’t want to allow one process to modify the library); and Copy-On-Write where the cost of duplicating a page can be delayed until the first write occurs.Dirty bitPage Table The dirty bit allows for a performance optimization. A page on disk that is paged in to physical memory, then read from, and subsequently paged out again does not need to be written back to disk, since the page hasn’t changed. However, if the page was written to after it’s paged in, its dirty bit will be set, indicating that the page must be written back to the backing store. This strategy requires that the backing store retain a copy of the page after it is paged in to memory. When a dirty bit is not used, the backing store need only be as large as the instantaneous total size of all paged-out pages at any moment. When a dirty bit is used, at all times some pages will exist in both physical memory and the backing store.Execution bitThe execution bit defines whether bytes in a page can be executed as CPU instructions. By disabling a page, it prevents code that is maliciously stored in the process memory (e.g. by stack overflow) from being easily executed. (further reading: background)Page FaultsA page fault is when a running program tries to access some virtual memory in its address space that is not mapped to physical memory. Page faults will also occur in other situations. There are three types of Page Faults      Minor If there is no mapping yet for the page, but it is a valid address. This could be memory asked for by sbrk(2) but not written to yet meaning that the operating system can wait for the first write before allocating space. The OS simply makes the page, loads it into memory, and moves on.        Major If the mapping to the page is not in memory but on disk. What this will do is swap the page into memory and swap another page out. If this happens frequently enough, your program is said to thrash the MMU.        Invalid When you try to write to a non-writable memory address or read to a non-readable memory address. The MMU generates an invalid fault and the OS will usually generate a SIGSEGV meaning segmentation violation meaning that you wrote outside the segment that you could write to.  PipesInter process communication is any way for one process to talk to another process. You’ve already seen one form of this virtual memory! A piece of virtual memory can be shared between parent and child, leading to communication. You may want to wrap that memory in pthread_mutexattr_setpshared(&amp;attrmutex, PTHREAD_PROCESS_SHARED); mutex (or a process wide mutex) to prevent race conditions. There are more standard ways of IPC, like pipes! Consider if you type the following into your terminal.$ ls -1 | cut -d'.' -f1 | uniq | sort | tee dir_contentsWhat does the following code do? Well it ls’s the current directory (the -1 means that it outputs one entry per line). The cut command then takes everything before the first period. Uniq makes sure all the lines are uniq, sort sorts them and tee outputs to a file. The important part is that bash creates 5 separate processes and connects their standard outs/stdins with pipes the trail lookssomething like this.(0) ls (1)------&gt;(0) cut (1)-------&gt;(0) uniq (1)------&gt;(0) sort (1)------&gt;(0) tee (1)The numbers in the pipes are the file descriptors for each process and the arrow represents the redirect or where the output of the pipe is going. A POSIX pipe is almost like its real counterpart - you can stuff bytes down one end and they will appear at the other end in the same order. Unlike real pipes however, the flow is always in the same direction, one file descriptor is used for reading and the other for writing. The pipe system call is used to create a pipe. These file descriptors can be used with read and with write. A common method of using pipes is to create the pipe before forking in order to communicate with a child processint filedes[2];pipe (filedes);pid_t child = fork();if (child &gt; 0) { /* I must be the parent */    char buffer[80];    int bytesread = read(filedes[0], buffer, sizeof(buffer));    // do something with the bytes read    } else {    write(filedes[1], \"done\", 4);}One can use pipes inside of the same process, but there tends to be no added benefit. Here’s an example program that sends a message to itself:#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;int main() {    int fh[2];    pipe(fh);    FILE *reader = fdopen(fh[0], \"r\");    FILE *writer = fdopen(fh[1], \"w\");    // Hurrah now I can use printf rather than using low-level read() write()    printf(\"Writing...\\n\");    fprintf(writer,\"%d %d %d\\n\", 10, 20, 30);    fflush(writer);        printf(\"Reading...\\n\");    int results[3];    int ok = fscanf(reader,\"%d %d %d\", results, results + 1, results + 2);    printf(\"%d values parsed: %d %d %d\\n\", ok, results[0], results[1], results[2]);        return 0;}The problem with using a pipe in this fashion is that writing to a pipe can block meaning the pipe only has a limited buffering capacity. If the pipe is full the writing process will block! The maximum size of the buffer is system dependent; typical values from 4KB upto 128KB.int main() {    int fh[2];    pipe(fh);    int b = 0;    #define MESG \"...............................\"    while(1) {        printf(\"%d\\n\",b);        write(fh[1], MESG, sizeof(MESG))        b+=sizeof(MESG);    }    return 0;}Pipe GotchasHere’s a complete example that doesn’t work! The child reads one byte at a time from the pipe and prints it out - but we never see the message! Can you see why?#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;signal.h&gt;int main() {    int fd[2];    pipe(fd);    //You must read from fd[0] and write from fd[1]    //One way to remember this: water(data stream) in a pipe always flows from a higher place(1) to a lower place(0)    printf(\"Reading from %d, writing to %d\\n\", fd[0], fd[1]);    pid_t p = fork();    if (p &gt; 0) {        /* I have a child therefore I am the parent*/        write(fd[1],\"Hi Child!\",9);        /*don't forget your child*/        wait(NULL);    } else {        char buf;        int bytesread;        // read one byte at a time.        while ((bytesread = read(fd[0], &amp;buf, 1)) &gt; 0) {            putchar(buf);        }    }    return 0;}The parent sends the bytes H,i,(space),C...! into the pipe (this may block if the pipe is full). The child starts reading the pipe one byte at a time. In the above case, the child process will read and print each character. However it never leaves the while loop! When there are no characters left to read it simply blocks and waits for moreThe call putchar writes the characters out but we never flush the stdout buffer. i.e. We have transferred the message from one process to another but it has not yet been printed. To see the message we could flush the buffer e.g. fflush(stdout) (or printf(\\n) if the output is going to a terminal). A better solution would also exit the loop by checking for an end-of-message marker,        while ((bytesread = read(fd[0], &amp;buf, 1)) &gt; 0) {            putchar(buf);            if (buf == '!') break; /* End of message */        }Processes receive the signal SIGPIPE when no process is listening! From the pipe(2) man page -If all file descriptors referring to the read end of a pipe have been closed, then a write(2) will cause a SIGPIPE signal to be generated for the calling process. Tip: Notice only the writer (not a reader) can use this signal. To inform the reader that a writer is closing their end of the pipe, you could write your own special byte (e.g. 0xff) or a message ( Bye!)Here’s an example of catching this signal that does not work! Can you see why?#include &lt;stdio.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;signal.h&gt;void no_one_listening(int signal) {    write(1, \"No one is listening!\\n\", 21);}int main() {    signal(SIGPIPE, no_one_listening);    int filedes[2];        pipe(filedes);    pid_t child = fork();    if (child &gt; 0) {         /* I must be the parent. Close the listening end of the pipe */        /* I'm not listening anymore!*/        close(filedes[0]);    } else {        /* Child writes messages to the pipe */        write(filedes[1], \"One\", 3);        sleep(2);        // Will this write generate SIGPIPE ?        write(filedes[1], \"Two\", 3);        write(1, \"Done\\n\", 5);    }    return 0;}The mistake in above code is that there is still a reader for the pipe! The child still has the pipe’s first file descriptor open and remember the specification? All readers must be closedWhen forking, It is common practice to close the unnecessary (unused) end of each pipe in the child and parent process. For example the parent might close the reading end and the child might close the writing end (and vice versa if you have two pipes)Why is my pipe hanging?Reads and writes hang on Named Pipes until there is at least one reader and one writer, take this1$ mkfifo fifo1$ echo Hello &gt; fifo# This will hang until I do this on another terminal or another process2$ cat fifoHelloAny open is called on a named pipe the kernel blocks until another process calls the opposite open. Meaning, echo calls open(.., O_RDONLY) but that blocks until cat calls open(.., O_WRONLY), then the programs are allowed to continue.Race condition with named pipesWhat is wrong with the following program?//Program 1int main(){    int fd = open(\"fifo\", O_RDWR | O_TRUNC);    write(fd, \"Hello!\", 6);    close(fd);    return 0;}//Program 2int main() {    char buffer[7];    int fd = open(\"fifo\", O_RDONLY);    read(fd, buffer, 6);    buffer[6] = '\\0';    printf(\"%s\\n\", buffer);    return 0;}This may never print hello because of a race condition. Since you opened the pipe in the first process under both permissions, open won’t wait for a reader because you told the operating system that you are a reader! Sometimes it looks like it works because the execution of the code looks something like this.      Process 1: open(O_RDWR) &amp; write()        Process 2: open(O_RDONLY) &amp; read()        Process 1: close() &amp; exit()        Process 2: print() &amp; exit()        Process 1: open(O_RDWR) &amp; write()        Process 1: close() &amp; exit()        Process 2: open(O_RDONLY) (Blocks indefinitely)  What is filling up the pipe? What happens when the pipe becomes full?A pipe gets filled up when the writer writes too much to the pipe without the reader reading any of it. When the pipes become full, all writes fail until a read occurs. Even then, a write may partial fail if the pipe has a little bit of space left but not enough for the entire messageTo avoid this, usually two things are done. Either increase the size of the pipe. Or more commonly, fix your program design so that the pipe is constantly being read from.Are pipes process safe?Yes! Pipe write are atomic up to the size of the pipe. Meaning that if two processes try to write to the same pipe, the kernel has internal mutexes with the pipe that it will lock, do the write, and return. The only gotcha is when the pipe is about to become full. If two processes are trying to write and the pipe can only satisfy a partial write, that pipe write is not atomic – be careful about that!The lifetime of pipesUnnamed pipes (the kind we’ve seen up to this point) live in memory (do not take up any disk space) and are a simple and efficient form of inter-process communication (IPC) that is useful for streaming data and simple messages. Once all processes have closed, the pipe resources are freed.Want to use pipes with printf and scanf? Use fdopen!POSIX file descriptors are simple integers 0,1,2,3… At the C library level, C wraps these with a buffer and useful functions like printf and scanf, so we that we can easily print or parse integers, strings etc. If you already have a file descriptor then you can ‘wrap’ it yourself into a FILE pointer using fdopen :#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;int main() {    char *name=\"Fred\";    int score = 123;    int filedes = open(\"mydata.txt\", \"w\", O_CREAT, S_IWUSR | S_IRUSR);    FILE *f = fdopen(filedes, \"w\");    fprintf(f, \"Name:%s Score:%d\\n\", name, score);    fclose(f);For writing to files this is unnecessary - just use fopen which does the same as open and fdopen However for pipes, we already have a file descriptor - so this is great time to use fdopenHere’s a complete example using pipes that almost works! Can you spot the error? Hint: The parent never prints anything!#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;int main() {    int fh[2];    pipe(fh);    FILE *reader = fdopen(fh[0], \"r\");    FILE *writer = fdopen(fh[1], \"w\");    pid_t p = fork();    if (p &gt; 0) {        int score;        fscanf(reader, \"Score %d\", &amp;score);        printf(\"The child says the score is %d\\n\", score);    } else {        fprintf(writer, \"Score %d\", 10 + 10);        fflush(writer);    }    return 0;}Note the unnamed pipe resource will disappear once both the child and parent have exited. In the above example the child will send the bytes and the parent will receive the bytes from the pipe. However, no end-of-line character is ever sent, so fscanf will continue to ask for bytes because it is waiting for the end of the line i.e. it will wait forever! The fix is to ensure we send a newline character, so that fscanf will return.change:   fprintf(writer, \"Score %d\", 10 + 10);to:       fprintf(writer, \"Score %d\\n\", 10 + 10);If you want your bytes to be sent to the pipe immediately, you’ll need to fflush! At the beginning of this course we assumed that file streams are always line buffered i.e. the C library will flush its buffer everytime you send a newline character. Actually this is only true for terminal streams - for other filestreams the C library attempts to improve performance by only flushing when it’s internal buffer is full or the file is closed.When do I need two pipes?If you need to send data to and from a child asynchronously, then two pipes are required (one for each direction). Otherwise the child would attempt to read its own data intended for the parent (and vice versa)!Named PipesHow do I create named pipes?An alternative to unamed pipes is named pipes created using mkfifo.From the command line: mkfifo From C: int mkfifo(const char pathname, mode_t mode);You give it the path name and the operation mode, it will be ready to go! Named pipes take up no space on the disk. What the operating system is essentially telling you when you have a named pipe is that it will create an unnamed pipe that refers to the named pipe, and that’s it! There is no additional magic. This is just for programming convenience if processes are started without forking (meaning that there would be no way to get the file descriptor to the child process for an unnamed pipe)Two types of filesOn linux, there are two abstractions with files. The first is the linux fd level abstraction.      open takes a path to a file and creates a file descriptor entry in the process table. If the file is not available to you, it errors out.        read takes a number of bytes that the kernel has received and reads them into a user space buffer. If the file is not open in read mode, this will break.        write outputs a number of bytes to a file descriptor. If the file is not open in write mode, this will break. This may be buffered internally.        close removes a file descriptor from a process’ file descriptors. This always succeeds on a valid file descriptor.        lseek takes a file descriptor and moves it to a certain position. Can fail if the seek is out of bounds.        fcntl is the catch all function for file descriptors. You can do everything with this function. Set file locks, read, write, edit permissions, etc …        …  And so on. The linux interface is very powerful and expressive, but sometimes we need portability (for example if we are writing for a mac or windows). This is where C’s abstraction comes into play. On different operating systems, C uses the low level functions to create a wrapper around files you can use everywhere, meaning that C on linux uses the above calls.      fopen opens a file and returns an object. null is returned if you don’t have permission for the file.        fread reads a certain number of bytes from a file. An error is returned if already at the end of file when which you must call feof() in order to check.        fgetc/fgets        fscanf        fwrite        fprintf        fclose        fflush  But you don’t get the expressiveness that linux gives you with system calls you can convert back and forth between them with int fileno(FILE* stream) and FILE* fdopen(int fd...)Another important aspect to note is the C files are buffered meaning that their contents may not be written right away by default. You can can change that with C options.How do I tell how large a file is?For files less than the size of a long, using fseek and ftell is a simple way to accomplish this:Move to the end of the file and find out the current position.fseek(f, 0, SEEK_END);long pos = ftell(f);This tells us the current position in the file in bytes - i.e. the length of the file!fseek can also be used to set the absolute position.fseek(f, 0, SEEK_SET); // Move to the start of the file fseek(f, posn, SEEK_SET);  // Move to 'posn' in the file.All future reads and writes in the parent or child processes will honor this position. Note writing or reading from the file will change the current position.See the man pages for fseek and ftell for more information.But try not to do thisNote: This is not recommended in the usual case because of a quirk with the C language. That quirk is that longs only need to be 4 Bytes big meaning that the maximum size that ftell can return is a little under 2 Gigabytes (which we know nowadays our files could be hundreds of gigabytes or even terabytes on a distributed file system). What should we do instead? Use stat! We will cover stat in a later part but here is some code that will tell you the size of the filestruct stat buf;if(stat(filename, &amp;buf) == -1){    return -1;}return (ssize_t)buf.st_size;buf.st_size is of type off_t which is big enough for large files.What happens if a child process closes a filestream using fclose or close?Closing a file stream is unique to each process. Other processes can continue to use their own file-handle. Remember, everything is copied over when a child is created, even the relative positions of the files.How about mmap for files?One of the general uses for mmap is to map a file to memory. This does not mean that the file is malloc’ed to memory right away. Take the following code for example.int fd = open(...); //File is 2 Pageschar* addr = mmap(..fd..);addr[0] = 'l';The kernel may say, “okay I see that you want to mmap the file into memory, so I’ll reserve some space in your address space that is the length of the file”. That means when you write to addr[0] that you are actually writing to the first byte of the file. The kernel can actually do some optimizations too. Instead of loading the file into memory, it may only load pages at a time because if the file is 1024 pages; you may only access 3 or 4 pages making loading the entire file a waste of time. That is why page faults are so powerful! They let the operating system take control of how much you use your files.For every mmapRemember that once you are done mmapping that you munmap to tell the operating system that you are no longer using the pages allocated, so the OS can write it back to disk and give you the addresses back in case you need to malloc later."
  },{
    "title": "Memory Allocators",
    "url": " /coursebook/Malloc",
   "content": "  Memory Allocators          Introduction      C Memory Allocation Functions                  Heaps and sbrk                    Intro to Allocating                  Placement Strategies          Placement Startegy pros and cons                    Memory Allocator Tutorial                  Implementing malloc          Alignment and rounding up considerations          Implementing free          Performance          Explicit Free Lists Allocators                    Case study: Buddy Allocator (an example of a segregated list)      Further Reading      Topics      Questions/Exercises      [1][]  Memory AllocatorsIntroductionMemory allocation is very important! Allocating and de-allocating heap memory is one of the most common operations in any application. The heap at the system level is contiguous series of addresses that the program can expand or contract and use as its own accord. In POSIX, this is called the system break. We use sbrk to move the system break. Most programs don’t interact directly with this call, they use a memory allocation system around it to handle chunking up and keeping track of which memory is allocated and which is free’d.We will mainly be looking into how the c standard library does memory allocations and that c-api for it. Just know that there are other ways of dividing up memory like with mmap or other allocation schemes and methods like jemalloc.C Memory Allocation Functions      malloc(size_t bytes) is a C library call and is used to reserve a contiguous block of memory. Unlike stack memory, the memory remains allocated until free is called with the same pointer. If malloc fails to reserve any more memory then it returns NULL. Robust programs should check the return value. If your code assumes malloc succeeds and it does not, then your program will likely crash (segfault) when it tries to write to address 0. Also, malloc may not zero out memory because of performance – check your code to make sure that you are not using unitialized values.        realloc(void *space, size_t bytes) allows you to resize an existing memory allocation that was previously allocated on the heap (via malloc, calloc, or realloc). The most common use of realloc is to resize memory used to hold an array of values. There are two gotchas with realloc. One, a new pointer may be returned. Two, it can fail. A naive but readable version of realloc is suggested below with sample usage.    void * realloc(void * ptr, size_t newsize) {  // Simple implementation always reserves more memory  // and has no error checking  void *result = malloc(newsize);   size_t oldsize =  ... //(depends on allocator's internal data structure)  if (ptr) memcpy(result, ptr, newsize &lt; oldsize ? newsize : oldsize);  free(ptr);  return result;}int main() {  // 1  int *array = malloc(sizeof(int) * 2);  array[0] = 10; array[1] = 20;  // Ooops need a bigger array - so use realloc..  array = realloc(array, 3 * sizeof(int));  array[2] = 30;       }        The above code is not robust. If realloc fails then the array is a memory leak. Robust code checks for the return value and only reassigns the original pointer if not null.        calloc(size_t nmemb, size_t size) initializes memory contents to zero and also takes two arguments: the number of items and the size in bytes of each item. An advanced discussion of these limitations is in this article. Programmers often use calloc rather than explicitly calling memset after malloc, to set the memory contents to zero. Note calloc(x,y) is identical to calloc(y,x), but you should follow the conventions of the manual. A naive implementation of calloc is below.    void *calloc(size_t n, size_t size) {  size_t total = n * size; // Does not check for overflow!  void *result = malloc(total);        if (!result) return NULL;          // If we're using new memory pages   // just allocated from the system by calling sbrk  // then they will be zero so zero-ing out is unnecessary,  return memset(result, 0, total);}            free takes a pointer to the start of a piece of memory and makes it available for use in the subsequent calls to the other allocation functions. This is important because we don’t want every process in our address space to take an enormous amount of memory. Once we are done using memory, we stop using it with free. A simple usage is below.    int *ptr = malloc(sizeof(*ptr));do_something(ptr);free(ptr);        If you use a piece of memory after it is freed - that is undefined behavior.  Heaps and sbrkThe heap is part of the process memory and it does not have a fixed size. Heap memory allocation is performed by the C library when you call malloc (calloc, realloc) and free. By calling sbrk the C library can increase the size of the heap as your program demands more heap memory. As the heap and stack (one for each thread) need to grow, we put them at opposite ends of the address space. So for typical architectures the heap will grow upwards and the stack grows downwards.Truthiness: Modern operating system memory allocators no longer need sbrk - instead they can request independent regions of virtual memory and maintain multiple memory regions. For example, gigabyte requests may be placed in a different memory region than small allocation requests. However this detail is an unwanted complexity. The problems of fragmentation and allocating memory efficiently still apply, so we will ignore this implementation nicety here and will write as if the heap is a single region. If we write a multi-threaded program (more about that later) we will need multiple stacks (one per thread) but there’s only ever one heap.Programs don’t need to call brk or sbrk typically (though calling sbrk(0) can be interesting because it tells you where your heap currently ends). Instead programs use malloc,calloc,realloc and free which are part of the C library. The internal implementation of these functions may call sbrk when additional heap memory is required.void *top_of_heap = sbrk(0);malloc(16384);void *top_of_heap2 = sbrk(0);printf(\"The top of heap went from %p to %p \\n\", top_of_heap, top_of_heap2);// Example output: The top of heap went from 0x4000 to 0xa000An important fact we just glossed over earlier is memory that was newly obtained by the operating system must be zeroed out. If the operating system did not zero out contents of physical RAM it might be possible for one process to learn about the memory of another process that had previously used the memory. This would be a security leak. Unfortunately this means that for malloc requests before any memory has been freed and simple programs (which end up using newly reserved memory from the system) the memory is often zero. Then programmers mistaken write C programs that assume malloc’d memory will always be zero.char* ptr = malloc(300);// contents is probably zero because we get brand new memory// so beginner programs appear to work!// strcpy(ptr, \"Some data\"); // work with the datafree(ptr);// laterchar *ptr2 = malloc(300); // Contents might now contain existing data and is probably not zeroIntro to AllocatingLet’s try to write Malloc. Here is a first attempt at it – the naive version.void* malloc(size_t size)// Ask the system for more bytes by extending the heap space. // sbrk Returns -1 on failure   void *p = sbrk(size);    if(p == (void *) -1) return NULL; // No space left   return p;}void free() {/* Do nothing */}Above is the simpliest implementation of malloc, there are a few drawbacks though.      System calls are slow (compared to library calls). We should reserve a large amount of memory and only occasionally ask for more from the system.        No reuse of freed memory. Our program never re-uses heap memory - it just keeps asking for a bigger heap.  If this allocator was used in a typical program, the process would quickly exhaust all available memory. Instead we need an allocator that can efficiently use heap space and only ask for more memory when necessary. There are programs that may use this type of allocator. Consider a video game allocating objects to load the next scene. It is considerably faster to do the above and just throw the entire block of memory away than it is to do the following placement strategies.Placement StrategiesDuring program execution, memory is allocated and de-allocated (freed), so there will be gaps (holes) in the heap memory that can be re-used for future memory requests. The memory allocator needs to keep track of which parts of the heap are currently allocated and which are parts are available. Suppose our current heap size is 64K, though not all of it is in use because some earlier malloc’d memory has already been freed by the program:            16KB      10KB      1KB      1KB      30KB      4KB      2KB                  free      allocated      free      allocated      free      allocated      free      If a new malloc request for 2KB is executed (malloc(2048)), where should malloc reserve the memory? It could use the last 2KB hole, which happens to be the perfect size! Or it could split one of the other two free holes. These choices represent different placement strategies. Whichever hole is chosen, the allocator will need to split the hole into two: The newly allocated space (which will be returned to the program) and a smaller hole (if there is spare space left over). A perfect-fit strategy finds the smallest hole that is of sufficient size (at least 2KB):            16KB      10KB      1KB      1KB      30KB      4KB      2KB                  free      allocated      free      allocated      free      allocated      HERE!      A worst-fit strategy finds the largest hole that is of sufficient size (so break the 30KB hole into two):            16KB      10KB      1KB      1KB      2KB      28KB      4KB      2KB                  free      allocated      free      allocated      HERE!      free      allocated      free      A first-fit strategy finds the first available hole that is of sufficient size (break the 16KB hole into two):            2KB      14KB      10KB      1KB      1KB      30KB      4KB      2KB                  HERE!      free      allocated      free      allocated      free      allocated      free      To introduce another concept, external fragmentation is that even though we have enough memory in the heap, it may be divided up in a way that wear are not able to give the full amount. In the example below, of the 64KB of heap memory, 17KB is allocated, and 47KB is free. However the largest available block is only 30KB because our available unallocated heap memory is fragmented into smaller pieces.            16KB      10KB      1KB      1KB      30KB      4KB      2KB                  free      allocated      free      allocated      free      allocated      free      Placement Startegy pros and consThe challenges of writing a heap allocator are      Need to minimize fragmentation (i.e. maximize memory utilization)        Need high performance        Fiddly implementation (lots of pointer manipulation using linked lists and pointer arithmetic)        Both fragmentation and performance depend on the application allocation profile, which can be evaluated but not predicted and in practice, under-specific usage conditions, a special-purpose allocator can often out-perform a general purpose implementation.        The allocator doesn’t know the program’s memory allocation requests in advance. Even if we did, this is the Knapsack problem which is known to be NP hard!  Different strategies affect the fragmentation of heap memory in non-obvious ways, which only are discovered by mathematical analysis or careful simulations under real-world conditions (for example simulating the memory allocation requests of a database or webserver). For example, best-fit at first glance appears to be an excellent strategy. However, if we can not find a perfectly-sized hole then this placement creates many tiny unusable holes, leading to high fragmentation. It also requires a scan of all possible holes. Since Worst-fit targets the largest unallocated space, it is a poor choice if large allocations are required. Worst fit also has to scan the entire heapFirst fit has the advantage that it will not evaluate all possible placements and therefore be faster. In practice first-fit and next-fit are often common placement strategy. Hybrid approaches and many other alternatives exist.Memory Allocator TutorialA memory allocator needs to keep track of which bytes are currently allocated and which are available for use. This page introduces the implementation and conceptual details of building an allocator, or the actual code that implements malloc and free.Conceptually we are thinking about creating linked lists and lists of blocks! We are writing integers and pointers into memory that we already control, so we can later consistently hop from one address to the next. This internal information represents some overhead. Even if we had requested 1024 KB of contiguous memory from the system, we will not be able to provide all of it to the running program.We can think of our heap memory as a list of blocks where each block is either allocated or unallocated. Rather than storing an explicit list of pointers we store information about the block’s size as part of the block. Thus there is conceptually a list of free blocks, but it is implicit in the form of block size information that we store as part of each block.We could navigate from one block to the next block just by adding the block’s size. For example if you have a pointer p that points to the start of a block, then next_block with be at ((char )p) +  (size_t ) p, if you are storing the size of the blocks in bytes. The cast to char  ensures that pointer arithmetic is calculated in bytes. The cast to size_t  ensures the memory at p is read as a size value and would be necessarily if p was a void  or char  type.The calling program never sees these values; they are internal to the implementation of the memory allocator. As an example, suppose your allocator is asked to reserve 80 bytes (malloc(80)) and requires 8 bytes of internal header data. The allocator would need to find an unallocated space of at least 88 bytes. After updating the heap data it would return a pointer to the block. However, the returned pointer does not point to the start of the block because that’s where the internal size data is stored! Instead we would return the start of the block + 8 bytes. In the implementation, remember that pointer arithmetic depends on type. For example, p += 8 adds 8  sizeof(p), not necessarily 8 bytes!Implementing mallocThe simplest implementation uses first fit: Start at the first block, assuming it exists, and iterate until a block that represents unallocated space of sufficient size is found, or we’ve checked all the blocks. If no suitable block is found, it’s time to call sbrk() again to sufficiently extend the size of the heap. A fast implementation might extend it a significant amount so that we will not need to request more heap memory in the near future.When a free block is found, it may be larger than the space we need. If so, we will create two entries in our implicit list. The first entry is the allocated block, the second entry is the remaining space. There are two simple ways to note if a block is in use or available. The first is to store it as a byte in the header information along with the size and the second to encode it as the lowest bit in the size! Thus block size information would be limited to only even values:// Assumes p is a reasonable pointer type, e.g. 'size_t *'.isallocated = (*p) &amp; 1;realsize = (*p) &amp; ~1;  // mask out the lowest bitAlignment and rounding up considerationsMany architectures expect multi-byte primitives to be aligned to some multiple of 2 (4, 16, etc). For example, it’s common to require 4-byte types to be aligned to 4-byte boundaries and 8-byte types on 8-byte boundaries. If multi-byte primitives are not stored on a reasonable boundary for example starting at an odd address then the performance can be significantly impacted because it may require two memory read requests instead of one. On some architectures the penalty is even greater - the program will crash with a bus error. Most of you experienced this in your architecture classes if there was no memory protection.As malloc does not know how the user will use the allocated memory, the pointer returned to the program needs to be aligned for the worst case, which is architecture dependent.From glibc documentation, the glibc malloc uses the following heuristic  The block that malloc gives you is guaranteed to be aligned so that it can hold any type of data. On GNU systems, the address is always a multiple of eight on most systems, and a multiple of 16 on 64-bit systems.” For example, if you need to calculate how many 16 byte units are required, don’t forget to round up.int s = (requested_bytes + tag_overhead_bytes + 15) / 16The additional constant ensures incomplete units are rounded up. Note, real code is more likely to symbol sizes e.g. sizeof(x) - 1, rather than coding numerical constant 15.Here’s a great article on memory alignment, if you are further interestedAnother added effect is could be internal fragmentation happens when the block you give them is larger than their allocation size. Let’s say that we have a free block of size 16B (not including metadata). If they allocate 7 bytes, you may want to round up to 16B and just return the entire block. This gets very sinister when you implementing coalescing and splitting (next section). If you don’t implement either, then you may end up returning a block of size 64B for a 7B allocation! There is a lot of overhead for that allocation which is what we are trying to avoid.Implementing freeWhen free is called we need to re-apply the offset to get back to the ‘real’ start of the block – to where we stored the size information. A naive implementation would simply mark the block as unused. If we are storing the block allocation status in the lowest size bit, then we just need to clear the bit:*p = (*p) &amp; ~1; // Clear lowest bit However, we have a bit more work to do: If the current block and the next block (if it exists) are both free we need to coalesce these blocks into a single block. Similarly, we also need to check the previous block, too. If that exists and represents an unallocated memory, then we need to coalesce the blocks into a single large block.To be able to coalesce a free block with a previous free block we will also need to find the previous block, so we store the block’s size at the end of the block, too. These are called “boundary tags” (ref Knuth73). These are Knuth’s solution to the coalescing problem both ways. As the blocks are contiguous, the end of one blocks sits right next to the start of the next block. So the current block (apart from the first one) can look a few bytes further back to lookup the size of the previous block. With this information you can now jump backwards!PerformanceWith the above description it’s possible to build a memory allocator. It’s main advantage is simplicity - at least simple compared to other allocators! Allocating memory is a worst-case linear time operation – search linked lists for a sufficiently large free block. De-allocation is constant time (no more than 3 blocks will need to be coalesced into a single block). Using this allocator it is possible to experiment with different placement strategies. For example, you could start searching from where you last free’d a block, or where you last allocated from. If you do store pointers to blocks, you need to be very careful that they always remain valid Particularly when you malloc, free, calloc, realloc, coalesce, split, etc.Explicit Free Lists AllocatorsBetter performance can be achieved by implementing an explicit doubly-linked list of free nodes. In that case, we can immediately traverse to the next free block and the previous free block. This can halve the search time, because the linked list only includes unallocated blocks. A second advantage is that we now have some control over the ordering of the linked list. For example, when a block is free’d, we could choose to insert it into the beginning of the linked list rather than always between its neighbors. This is discussed below.Where do we store the pointers of our linked list? A simple trick is to realize that the block itself is not being used and store the next and previous pointers as part of the block (though now you have to ensure that the free blocks are always sufficiently large to hold two pointers). We still need to implement Boundary Tags, so we can correctly free blocks and coalesce them with their two neighbors. Consequently, explicit free lists require more code and complexity. With explicit linked lists a fast and simple ‘Find-First’ algorithm is used to find the first sufficiently large link. However, since the link order can be modified, this corresponds to different placement strategies. For example if the links are maintained from largest to smallest, then this produces a ‘Worst-Fit’ placement strategy.Explicit linked list insertion policyThe newly free’d block can be inserted easily into two possible positions: at the beginning or in address order (by using the boundary tags to first find the neighbors). Inserting at the beginning creates a LIFO (last-in, first-out) policy: The most recently free’d spaces will be reused. Studies suggest fragmentation is worse than using address order.Inserting in address order (“Address ordered policy”) inserts free’d blocks so that the blocks are visited in increasing address order. This policy required more time to free a block because the boundary tags (size data) must be used to find the next and previous unallocated blocks. However, there is less fragmentation.Case study: Buddy Allocator (an example of a segregated list)A segregated allocator is one that divides the heap into different areas that are handled by different sub-allocators dependent on the size of the allocation request. Sizes are grouped into classes (e.g. powers of two) and each size is handled by a different sub-allocator and each size maintains its own free list.A well known allocator of this type is the buddy allocator . We’ll discuss the binary buddy allocator which splits allocation into blocks of size 2^n (n = 1, 2, 3, …) times some base unit number of bytes, but others also exist (e.g. Fibonacci split). The basic concept is simple: If there are no free blocks of size 2^n, go to the next level and steal that block and split it into two. If two neighboring blocks of the same size become unallocated, they can be coalesced back together into a single large block of twice the size.Buddy allocators are fast because the neighboring blocks to coalesce with can be calculated from the free’d block’s address, rather than traversing the size tags. Ultimate performance often requires a small amount of assembler code to use a specialized CPU instruction to find the lowest non-zero bit.The main disadvantage of the Buddy allocator is that they suffer from internal fragmentation, because allocations are rounded up to the nearest block size. For example, a 68-byte allocation will require a 128-byte block.Further ReadingThere are many other allocation schemes. One of three allocators used internally by the Linux Kernel. See the man page!      SLUB (wikipedia)        See Foundations of Software Technology and Theoretical Computer Science 1999 proceedings (Google books,page 85)        Wikipedia’s buddy memory allocation page  Topics      Best Fit        Worst Fit        First Fit        Buddy Allocator        Internal Fragmentation        External Fragmentation        sbrk        Natural Alignment        Boundary Tag        Coalescing        Splitting        Slab Allocation/Memory Pool  Questions/Exercises      What is Internal Fragmentation? When does it become an issue?        What is External Fragmentation? When does it become an issue?        What is a Best Fit placement strategy? How is it with External Fragmentation? Time Complexity?        What is a Worst Fit placement strategy? Is it any better with External Fragmentation? Time Complexity?        What is the First Fit Placement strategy? It’s a little bit better with Fragmentation, right? Expected Time Complexity?        Let’s say that we are using a buddy allocator with a new slab of 64kb. How does it go about allocating 1.5kb?        When does the 5 line sbrk implementation of malloc have a use?        What is natural alignment?        What is Coalescing/Splitting? How do they increase/decrease fragmentation? When can you coalesce or split?        How do boundary tags work? How can they be used to coalesce or split?  "
  },{
    "title": "Networking",
    "url": " /coursebook/Networking",
   "content": "  Networking          The OSI Model      Layer 3: The Internet Protocol                  Extra: In-depth IPv4 Specification          Routing          Fragmentation/Reassembly          IP Multicast          What’s the deal with IPv6?          What’s My Address?                    Layer 4: TCP and Client                  Note on network orders          TCP Client          Sending some data                    Layer 4: TCP Server                  Server code example                    Layer 4: UDP                  UDP Attributes          UDP Client          UDP Server                    Layer 7: HTTP      Nonblocking IO                  epoll                    Remote Procedure Calls                  What is Privilege Separation?          What is stub code? What is marshalling?          What is server stub code? What is unmarshalling?          How do you send an int? float? a struct? A linked list? A graph?          What is an Interface Description Language (IDL)?          Complexity and challenges of RPC vs local calls?          Transferring large amounts of structured data                    Topics      Questions      [1][]  NetworkingNetworking has become arguably the most important use of computers in the past 10-20 years. Most of us nowadays can’t stand a place without wifi or any connectivity, so it is crucial as programmers that you have an understanding of networking and how to program to communicate across networks. Although it may sound complicated, POSIX has defined nice standards that make connecting to the outside world easy. POSIX also lets you peer underneath the hood and optimize all the little parts of each connection to write highly performant programs.The OSI ModelThe Open Source Interconnection 7 layer model (OSI Model) is a sequence of segments that define standards for both infrastructure and protocols for forms of radio communication, in our case the internet. The 7 layer model is as follows      Layer 1: The physical layer. These are the actual waves that carry the bauds across the wire. As an aside, bits don’t cross the wire because in most mediums you can alter two characterstics of a wave – the amplitude and the frequency – and get more bits per clock cycle.        Layer 2: The link layer. This is how each of the agents react to certain events (error detection, noisy channels, etc). This is where Ethernet and WiFi live.        Layer 3: The network layer. This is the heart of the internet. The bottom two protocols deal with communication between two different computers that are directly connected. This layer deals with routing packets from one endpoint to another.        Layer 4: The transport layer. This layer specifies how the slices of data are received. The bottom three layers make no guarantee about the order that packets are received and what happens when a packet is dropped. Using different protocols, this layer can.        Layer 5: The session layer. This layer makes sure that if a connection in the previous layers is dropped, a new connection in the lower layers can be established, and it looks like a nothing happened to the end user.        Layer 6: The presentation layer. This layer deals with encryption, compression, and data translation. For example, portability between different operating systems like translating newlines to windows newlines.        Layer 7: The application layer. The application layer is where many different protocols live. HTTP and FTP are both defined at this level. This is typically where we define protocols across the internet. As programmers, we only go lower when we think we can create algorithms that are more suited to our needs than all of the below.  Just to be clear this is not a networking class. We won’t go over most of these layers in depth. We will focus on some aspects of layers 3, 4, and 7 because they are essential to know if you are going to be doing something with the internet, which at some point in your career you will be. As for another definition, a protocol is a set of specifications put forward by the Internet Engineering Task Force that govern how implementers of protocol have their program or circuit behave under specific circumastnces.Layer 3: The Internet ProtocolThe following is the 30 second introduction to internet protocol (IP), the primary way to send datagrams of information from one machine to another. “IP4”, or more precisely, IPv4 is version 4 of the Internet Protocol that describes how to send packets of information across a network from one machine to another. Roughly 95% of all packets on the Internet today are IPv4 packets. Citation Needed A significant limitation of IPv4 is that source and destination addresses are limited to 32 bits. IPv4 was designed at a time when the idea of 4 billion devices connected to the same network was unthinkable or at least not worth making the packet size larger. IPv4 address are written typically in a sequence of four octets delimited by periods “255.255.255.0” for example.Each IPv4 datagram includes a very small header - typically 20 octets, that includes a source and destination address. Conceptually the source and destination addresses can be split into two: a network number the upper bits and the lower bits represent a particular host number on that network.A newer packet protocol IPv6 solves many of the limitations of IPv4 like making routing tables simpler and 128 bit addresses. However, less than 5% of web traffic is IPv6 based. Citation Needed We write IPv6 addresses in a sequence of eight, four hexadecimal delimiters like “1F45:0000:0000:0000:0000:0000:0000:0000”. Since that can get unruly, we can omit the zeros “1F45::”. A machine can have an IPv6 address and an IPv4 address.There are special IP Addresses. One such in IPv4 is 127.0.0.1, IPv6 as 0:0:0:0:0:0:0:1 or ::1 also known as localhost. Packets sent to 127.0.0.1 will never leave the machine; the address is specified to be the same machine. There are a lot of others that are denoted by certain octets being zeros or 255, the maximum value. You won’t need to know all the terminology, just keep in mind that the actual number of IP addresses that a machine can have globally over the internet is smaller than the number of “raw” addresses. For the purposes of the class, you need to know at this layer that IP deals with routing, fragmenting, and reassembling upper level protocols. A more in-depth aside follows.Extra: In-depth IPv4 SpecificationThe internet protocol deals with routing, fragmentation, and reassembly of fragments. Datagrams are formatted as such      The first octet is the version number, either 4 or 6        The next octet is how long the header is. Although it may seem that the header is constant size, you can include optional parameters to augment the path taken or other instructions        The next two octets specify the total length of the datagram. This means this is the header, the data, footer, and padding. This is given in multiple of octets, meaning that a value of 20 means 20 octets.        The next two are Identification number. IP handles taking packets that are too big to be sent over the phsyical wire and chunks them up. As such, this number identifies what datagram this originally belonged to.        The next octet is various bit flags that can be set.        The next octet and half is fragment number. If this packet was fragmented, this is the number this fragment represents        The next octet is time to live. So this is the number of “hops” (travels over a wire) a packet is allowed to go. This is set because different routing protocols could cuase packets to go in circles, the packets must be dropped at some point.        The next octet is the protocol number. Although protocols between different layers of the OCI model are supposed to be black boxes, this is included, so that hardware can peer into the underlying protocol efficiently. Take for example IP over IP (yes you can do that!). Your ISP wraps IPv4 packets sent from your computer to the ISP in another IP layer and sends the packet off to be delivered to the website. On the reverse trip the packet is “unwrapped” and the original IP datagram is sent to your computer. This was done because we ran out of IP addresses, and this adds additional overhead but it is a necessary fix. Other common protocols are TCP, UDP, etc.        The next two octets is an internet checksum. This is a CRC that is calculated to make sure that a wide variety of bit errors are detected.        Source address is what people generally refer to as the IP address. There is no verification of this, so one host can pretend to be any IP address possible        Destination address is where you want the packet to be sent to. This is crucial in the routing process as you need that to route.        After: Your data! All layer of higher order protocols are put in there        Additional options: Hosts of additional options        Footer: A bit of padding to make sure your data is a multiple of 8  RoutingThe internet protocol routing is an amazing intersection of theory and application. We can imagine the entire internet as a set of graphs. Most peers are connected to what we call “peering points” these are the WIFI routers and the ethernet ports that one finds in their house, work, or public. These peering points are then connected to a wired network of routers, switches, and servers that all route themselves. At a top level there are two types of routing      Internal Routing Protocols. Internal protocols is routing designed for within an ISP’s network. These protocols are meant to be fast and more trusting because all computers, switches, and routers are part of an ISP. communication between two routers.        External Routing Protocols. These typically happen to be ISP to ISP protocol. Certain routers are designated as border routers. These routers talk to routers from ISPs have have different policies from accepting or receiving packets. If an evil ISP is trying to dump all network traffic onto your ISP, these routers would deal with that. These protocols also deal with gathering information about the outside world to each router. In most routing protocols using link state or OSPF, a router must necessarily calculate the shortest path to the destination. This means it needs information about the “foreign” routers which is disseminated according to these protocols.  These two protocols have to interplay with each other nicely in order to make sure that packets are mostly delivered. In addition, ISPs need to be nice to each other because theoretically an ISP can handle lower load by forwarding all packets to another ISP. If everyone does that then, no packets get delivered at all which won’t make customers happy at all. So these two protocols need to be fair so the end result worksIf you want to read more about this, look at the wikipedia page for routing here Routing.Fragmentation/ReassemblyLower layers like WiFi and Ethernet have maximum transmission sizes. The reason being is      One host shouldn’t crowd the medium for too long        If an error occurs, we want some sort of “progress bar” on how far the communication has gone instead of retransmitting the stream        There are physical limitations as well, keeping a laser beam in optics working continuously may cause bit errors.  As such if the internet protocol receives a packet that is too big for the maximum size, it must chunk it up. TCP calculates how many datagrams it needs to construct a packet and ensures that they are all transmitted and reconstructed at the end receiver. The reason that we barely use this feature is that if any fragment is lost, the entire packet is lost. Meaning that, assuming the probability of receiving a packet assuming each fragment is lost with an independent percentage, the probability of successfully sending a packet drops off exponentially as packet size increases.As such, TCP slices its packets so that it fits inside on IP datagram. The only time that this applies is when sending UDP packets that are too big, but most people who are using UDP optimize and set the same packet size as well.IP MulticastA little known feature is that using the IP protocol one can send a datagram to all devices connected to a router in what is called a multicast. Multicasts can also be configured with groups, so one can efficiently slice up all connected routers and send a piece of information to all of them efficiently. To access this in a higher protocol, you need to use UDP and specify a few more options. Note that this will cause undo stress on the network, so a series of multicasts could flood the network fast.What’s the deal with IPv6?One of the big features of IPv6 is the address space. The world ran out of IP addresses a while ago and has been using hacks to get around that. With IPv6 there are enough internal and external addresses, so that unless we discover alien civilizations, we probably won’t run out. The other benefit is that these addresses are leased not bought, meaning that if something drastic happens in let’s say the internet of things and there needs to be a change in the block addressing scheme, it can be done.Another big feature is security through IPsec. IPv4 was designed with little to no security in mind. As such, now there is a key exchange similar to TLS in higher layers that allows you to encrypt communication.Another feature is simplified processing. In order to make the internet fast, IPv4 and IPv6 headers alike are actually implemented in hardware. That means that all header options are processed in circuits as they come in. The problem is that as the IPv4 spec grew to include a copious amount of headers, the hardware had to become more and more advanced to support those headers. IPv6 reorders the headers so that packets can be dropped and routed with less hardware cycles. In the case of the internet, every cycle matters when trying to route the world’s traffic.What’s My Address?To obtain a linked list of IP addresses of the current machine use getifaddrs which will return a linked list of IPv4 and IPv6 IP addresses among other interfaces as well. We can examine each entry and use getnameinfo to print the host’s IP address. The ifaddrs struct includes the family but does not include the sizeof the struct. Therefore we need to manually determine the struct sized based on the family. (family == AF_INET) ? sizeof(struct sockaddr_in) : sizeof(struct sockaddr_in6)The complete code is shown below.    int required_family = AF_INET; // Change to AF_INET6 for IPv6    struct ifaddrs *myaddrs, *ifa;    getifaddrs(&amp;myaddrs);    char host[256], port[256];    for (ifa = myaddrs; ifa != NULL; ifa = ifa-&gt;ifa_next) {        int family = ifa-&gt;ifa_addr-&gt;sa_family;        if (family == required_family &amp;&amp; ifa-&gt;ifa_addr) {            if (0 == getnameinfo(ifa-&gt;ifa_addr,                                (family == AF_INET) ? sizeof(struct sockaddr_in) :                                sizeof(struct sockaddr_in6),                                host, sizeof(host), port, sizeof(port)                                 , NI_NUMERICHOST | NI_NUMERICSERV  ))                puts(host);            }        }To get your IP Address from the command line use ifconfig (or Windows’s ipconfig) However this command generates a lot of output for each interface, so we can filter the output using grepifconfig | grep inetExample output:    inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1     inet 127.0.0.1 netmask 0xff000000     inet6 ::1 prefixlen 128     inet6 fe80::7256:81ff:fe9a:9141%en1 prefixlen 64 scopeid 0x5     inet 192.168.1.100 netmask 0xffffff00 broadcast 192.168.1.255To actually grab the IP Address of a remote website. The function getaddrinfo can convert a human readable domain name (e.g. www.illinois.edu) into an IPv4 and IPv6 address. In fact it will return a linked-list of addrinfo structs:struct addrinfo {    int              ai_flags;    int              ai_family;    int              ai_socktype;    int              ai_protocol;    socklen_t        ai_addrlen;    struct sockaddr *ai_addr;    char            *ai_canonname;    struct addrinfo *ai_next;};For example, suppose you wanted to find out the numeric IPv4 address of a webserver at www.bbc.com We do this in two stages. First use getaddrinfo to build a linked-list of possible connections. Secondly use getnameinfo to convert the binary address into a readable form.#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;netdb.h&gt;struct addrinfo hints, *infoptr; // So no need to use memset global variablesint main() {  hints.ai_family = AF_INET; // AF_INET means IPv4 only addresses  int result = getaddrinfo(\"www.bbc.com\", NULL, &amp;hints, &amp;infoptr);  if (result) {    fprintf(stderr, \"getaddrinfo: %s\\n\", gai_strerror(result));    exit(1);  }  struct addrinfo *p;  char host[256];  for(p = infoptr; p != NULL; p = p-&gt;ai_next) {    getnameinfo(p-&gt;ai_addr, p-&gt;ai_addrlen, host, sizeof(host), NULL, 0, NI_NUMERICHOST);    puts(host);  }  freeaddrinfo(infoptr);  return 0;}212.58.244.70212.58.244.71If you are wondering how the the computer maps hostnames to addresses, we will talk about that in Layer 7. Spoiler: It is a service called DNSLayer 4: TCP and ClientMost services on the Internet today use TCP because it efficiently hides the complexity of lower, packet-level nature of the Internet. TCP or Transport Control Protocol is a connection-based protocol that is built on top of IPv4 and IPv6 and therefore can be described as “TCP/IP” or “TCP over IP”. TCP creates a pipe between two machines and abstracts away the low level packet-nature of the Internet. Thus, under most conditions, bytes sent over a TCP connection will not be lost or corrupted.TCP has a number of features that set it apart from the other transport protocol UDP.      Ports With IP, you are only allowed to send packets to a machine. If you want one machine to handle multiple flows of data, you have to do it manually with IP. TCP abstracts that an gives the programmer a set of virtual sockets. Clients specify the socket that you want the packet sent to and the TCP protocol makes sure that applications that are waiting for packets on that port receive that. A process can listen for incoming packets on a particular port. However only processes with super-user (root) access can listen on ports less than 1024. Any process can listen on ports 1024 or higher. An often used port is port 80: Port 80 is used for unencrypted http requests or web pages. For example, if a web browser connects to http://www.bbc.com/ then it will be connecting to port 80.        Retransmission Packets can get dropped due to network errors or congestion. As such, they need to be retransmitted but at the same time the retransmission shouldn’t cause packets more packets to be dropped. This needs to balance the tradeoff between flooding the network and speed.        Out of order packets. Packets may get routed more favorably due to various reasons in IP. If a later packet arrives before another packet, the protocol should detect and reorder them.        Duplicate packets. Packets can arrive twice. Packets can arrive twice. As such, a protocol need to be able to differentiate between two packets given a sequence number subject to overflow.        Error correction. There is a TCP checksum that handles bit errors. This is rarely used though.        Flow Control. Flow control is performed on the receiver side. This may be done so that a slow receiver doesn’t get overwhelmed with packets. Servers especially that may handle 10000 or 10 million concurrent connection may need to tell receivers to slow down, but not disconnect due to load. There are also other prorblem of making sure the local network is not overwhelmed        Congestion control. Congestion control is performed on the sender side. Congestion control is to avoid a sender from flooding the network with too many packets. This is really important to make sure that each TCP connection is treated fairly. Meaning that two connections leaving a computer to google and youtube receive the same bandwidth and ping as each other. One can easily define a protocol that takes all the bandwidth and leaves other protocols in the dust, but this tends to be malicious because more often than not limiting a computer to a single TCP connection will yield the same result.        Connection oriented/lifecycle oriented. You can really imagine a TCP connection as a series of bytes sent through a pipe. There is a “lifecycle” to a TCP connection though. What this means is that a TCP connection has a series of states and certain packets received can or not received can move it to another state. TCP handles setting up the connection through SYN SYN-ACK ACK. This means the client will send a SYNchronization packet that tells TCP what starting sequence to start on. Then the receiver will send a SYN-ACK message acknowledging the synchronization number. Then the client will ACKnowledge that with one last packet. The connection is now open for both reading and writing on both ends TCP will send data and the receiver of the data will acknowledge that it received a packet. Then every so often if a packet is not sent, TCP will trade zero length packets to make sure the connection is still alive. At any point, the client and server can send a FIN packet meaning that the server will not transmit. This packet can be altered with bits that only close the read or write end of a particular connection. When all ends are closed then the connection is over.  There are a list of things that TCP doesn’t provide though      Security. This means that if you connect to an IP address that says that it is a certain website, TCP does not verify that this website is in fact that IP address. You could be sending packets to a malicious computer.        Encryption. Anybody can listen in on plain TCP. The packets in transport are in plain text meaning that important things like your passwords could easily be skimmed by servers and regularly are.        Session Reconnection. This is handled by a higher protocols, but if a TCP connection dies then a whole new one hast to be created and the transmission has to be started over again.        Delimiting Requests. TCP is naturally connection oriented. Applications that are communicating over TCP need to find a unique way of telling each other that this request or response is over. HTTP delimits the header through two carriage returns and uses either a length field or one keeps listening until the connection closes  Note on network ordersIntegers can be represented in least significant byte first or most-significant byte first. Either approach is reasonable as long as the machine itself is internally consistent. For network communications we need to standardize on agreed format.htons(xyz) returns the 16 bit unsigned integer ‘short’ value xyz in network byte order. htonl(xyz) returns the 32 bit unsigned integer ‘long’ value xyz in network byte order.These functions are read as ‘host to network’; the inverse functions (ntohs, ntohl) convert network ordered byte values to host-ordered ordering. So, is host-ordering little-endian or big-endian? The answer is - it depends on your machine! It depends on the actual architecture of the host running the code. If the architecture happens to be the same as network ordering then the result of these functions is just the argument. For x86 machines, the host and network ordering is different.Unless agreed otherwise whenever you read or write the low level C network structures (e.g. port and address information), remember to use the above functions to ensure correct conversion to/from a machine format. Otherwise the displayed or specified value may be incorrect.This doesn’t apply to protocols that negotiate the endianness before-hand. If two computers are CPU bound by converting the messages between network orders – this happens with JSON parsing all the time in high performance systems – it may be worth it to negotiate if they are on similar endians to send in little endian order.TCP ClientThere are three basic system calls you need to connect to a remote machine:      int getaddrinfo(const char *node, const char *service, const struct addrinfo *hints, struct addrinfo **res);    The getaddrinfo call if successful, creates a linked-list of addrinfo structs and sets the given pointer to point to the first one.    In addition, you can use the hints struct to only grab certain entries like certain IP protocols etc. The addrinfo structure that is passed into getaddrinfo to define the kind of connection you’d like. For example, to specify stream-based protocols over IPv6:    struct addrinfo hints;memset(&amp;hints, 0, sizeof(hints));hints.ai_family = AF_INET6; // Only want IPv6 (use AF_INET for IPv4)hints.ai_socktype = SOCK_STREAM; // Only want stream-based connection        Error handling with getaddrinfo is a little different: The return value is the error code. To convert to a human-readable error use gai_strerror to get the equivalent short English error text.    int result = getaddrinfo(...);if(result) {    const char *mesg = gai_strerror(result);    ...}            int socket(int domain, int socket_type, int protocol);    The socket call creates an outgoing socket and returns a descriptor that can be used with read and write. In this sense it is the network analog of open that opens a file stream - except that we haven’t connected the socket to anything yet!    Socket creates a socket with domain  AF_INET for IPv4 or AF_INET6 for IPv6, socket_type is whether to use UDP or TCP or other socket type, protocol is an optional choice of protocol configuration (for our examples this we can just leave this as 0 for default). This call creates a socket object in the kernel with which one can communicate with the outside world/network. You can use the result of getaddressinfo to fill in the socket parameters, or provide them manually.    The socket call returns an integer - a file descriptor - and, for TCP clients, you can use it like a regular file descriptor i.e. you can use read and write to receive or send packets.    TCP sockets are similar to pipes except that they allow full duplex communication i.e. you can send and receive data in both directions independently.        connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen);    Finally the connect call attempts the connection to the remote machine. We pass the original socket descriptor and also the socket address information which is stored inside the addrinfo structure. There are different kinds of socket address structures which can require more memory. So in addition to passing the pointer, the size of the structure is also passed. To help identify errors and mistakes it is good practice to check the return value of all networking calls, including connect    // Pull out the socket address info from the addrinfo struct:connect(sockfd, p-&gt;ai_addr, p-&gt;ai_addrlen)            (Optional) To clean up code call freeaddrinfo(struct addrinfo *ai) on the first level addrinfo struct.  There is an old function gethostbyname is deprecated; it’s the old way convert a host name into an IP address. The port address still needs to be manually set using htons function. It’s much easier to write code to support IPv4 AND IPv6 using the newer getaddrinfoThis is all that is needed to create a simple TCP client - however network communications offers many different levels of abstraction and several attributes and options that can be set at each level of abstraction. For example we haven’t talked about setsockopt which can manipulate options for the socket. For more information see this guide.Sending some dataOnce we have a successful connection we can read or write like any old file descriptor. Keep in mind if you are connected to a website, you want to conform to the HTTP protocol specification in order to get any sort of meaningful results back. There are libraries to do this, usually you don’t connect at the socket level because there are other libraries or packages around it. The number of bytes read or written may be smaller than expected. Thus it is important to check the return value of read and write. A simple HTTP client that sends a request to compliant URL is below.#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;sys/types.h&gt; #include &lt;sys/socket.h&gt; #include &lt;netdb.h&gt;#include &lt;errno.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;#ifndef _GNU_SOURCE#define _GNU_SOURCE#endiftypedef struct _host_info {    char *hostname;    char *port;    char *resource;} host_info;host_info *get_info(char *uri);void free_info(host_info *info);host_info *send_request(host_info *info);ssize_t min(ssize_t a, ssize_t b) {    return a &lt; b ? a : b;}host_info *get_info(char *uri) {    const char *http = \"http://\";    int http_len = strlen(http);    int uri_len = strlen(uri);    if (uri_len &lt; http_len &amp;&amp; !strncmp(uri, http, min(strlen(http), uri_len))) {        fprintf(stderr, \"The uri must start with \\\"%s\\\"\", http);        exit(1);    } else {        uri += http_len;        uri_len -= http_len;    }    char *hostname = malloc(uri_len+1);    char *port = malloc(6);    char *ptr = hostname;    while(*uri &amp;&amp; *uri != '/' &amp;&amp; *uri != ':') {        *ptr++ = *uri++;    }    *ptr = '\\0';    if(*uri == ':') {        ptr = port;        uri++;        while(*uri != '/') {            *ptr++ = *uri++;        }        *ptr = '\\0';    } else {        free(port);        port = strdup(\"80\");    }    char *resource = NULL;    int len = strlen(uri);    if (len == 0) {        // Empty means get the index        resource = strdup(\"/\");    } else {        resource = strdup(uri);    }    host_info *info = malloc(sizeof(*info));    info-&gt;hostname = hostname;    info-&gt;port = port;    info-&gt;resource = resource;      return info;}void free_info(host_info *info) {    free(info-&gt;hostname);    free(info-&gt;port);    free(info-&gt;resource);    free(info);}static void send_get_request(FILE *sock_file, host_info *info) {    char *buffer;    asprintf(&amp;buffer,         \"GET %s HTTP/1.0\\r\\n\"        \"Connection: close\\r\\n\"        \"Accept: */*\\r\\n\\r\\n\",         info-&gt;resource);    int sock_fd = fileno(sock_file);    write(sock_fd, buffer, strlen(buffer));    free(buffer);}static void connect_to_address(int sock_fd, host_info *info) {    struct addrinfo current, *result;    memset(&amp;current, 0, sizeof(struct addrinfo));    current.ai_family = AF_INET;    current.ai_socktype = SOCK_STREAM;    int s = getaddrinfo(info-&gt;hostname, info-&gt;port, &amp;current, &amp;result);    if (s != 0) {        fprintf(stderr, \"getaddrinfo: %s\\n\", gai_strerror(s));        exit(1);    }    if(connect(sock_fd, result-&gt;ai_addr, result-&gt;ai_addrlen) == -1){        perror(\"connection error\");        exit(1);    }    freeaddrinfo(result);}host_info *send_request(host_info *info) {    int sock_fd = socket(AF_INET, SOCK_STREAM, 0);    if (sock_fd == -1) {        perror(\"socket\");        exit(1);    }    int optval = 1;    int retval = setsockopt(sock_fd, SOL_SOCKET, SO_REUSEADDR, &amp;optval,                sizeof(optval));    if(retval == -1) {        perror(\"setsockopt\");        exit(1);    }    connect_to_address(sock_fd, info);    // Open so you can use getline    FILE *sock_file = fdopen(sock_fd, \"r+\");    setvbuf(sock_file, NULL, _IONBF, 0);    send_get_request(sock_file, info);    host_info *ret = NULL;    if (is_redirect(sock_file)) {        ret = handle_redirect(sock_file);    } else {        handle_okay(sock_file);    }    fclose(sock_file);    return ret;}int main(int argc, char *argv[]) {    if(argc != 2) {        fprintf(stderr, \"Usage: %s http://hostname[:port]/path\\n\", *argv);        return 1;       }    char *uri = argv[1];    host_info *info = get_info(uri);    do {        host_info *temp = send_request(info);        free_info(info);        info = NULL;        if (temp) {            info = temp;        }    } while(info);    return 0;}The example above demonstrates a request to the server using Hypertext Transfer Protocol. A web page (or other resources) are requested using the following request:GET / HTTP/1.0There are four parts the method e.g. GET,POST,…); the resource (e.g. / /index.html /image.png); the proctocol “HTTP/1.0” and two new lines ( r n r n)The server’s first response line describes the HTTP version used and whether the request is successful using a 3 digit response code:HTTP/1.1 200 OKIf the client had requested a non existing file, e.g. GET /nosuchfile.html HTTP/1.0 Then the first line includes the response code is the well-known 404 response code:HTTP/1.1 404 Not FoundLayer 4: TCP ServerThe four system calls required to create a TCP server are: socket, bind listen and accept. Each has a specific purpose and should be called in roughly the above order      int socket(int domain, int socket_type, int protocol)    To create a endpoint for networking communication. A new socket by itself is not particularly useful. Though we’ve specified either a packet or stream-based connections, it is not bound to a particular network interface or port. Instead socket returns a network descriptor that can be used with later calls to bind, listen and accept.    As one gotcha, these sockets must be declared passive. Passive server sockets do not actively try to connect to another host; instead they wait for incoming connections. Additionally, server sockets are not closed when the peer disconnects. Instead the client communicates with a separate active socket on the server that is specific to that connection.    Since a TCP connection is defined by the sender address and port along with a receiver address and port, a particular server port there can be one passive server socket but multiple active sockets: one for each currently open connection. The server’s operating system maintains a lookup table that associates a unique tuple with active sockets, so that incoming packets can be correctly routed to the correct socket.        int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);    The bind call associates an abstract socket with an actual network interface and port. It is possible to call bind on a TCP client. The port information used by bind can be set manually many older IPv4-only C code examples do this, or be created using getaddrinfo    By default a port is not immediately released when the server socket is closed. Instead, the port enters a “TIMED-WAIT” state. This can lead to significant confusion during development because the timeout can make valid networking code appear to fail.    To be able to immediately re-use a port, specify SO_REUSEPORT before binding to the port.      int optval = 1;  setsockopt(sfd, SOL_SOCKET, SO_REUSEPORT, &amp;optval, sizeof(optval));  bind(...);              Here’s an extended stackoverflow introductory discussion of SO_REUSEPORT.        int listen(int sockfd, int backlog);    The listen call specifies the queue size for the number of incoming, unhandled connections i.e. that have not yet been assigned a network descriptor by accept Typical values for a high performance server are 128 or more.        int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);    Once the server socket has been initialized the server calls accept to wait for new connections. Unlike socket bind and listen, this call will block. i.e. if there are no new connections, this call will block and only return when a new client connects. The returned TCP socket is associated with a particular tuple (client IP, client port, server IP, server port) and will be used for all future incoming and outgoing TCP packets that match this tuple.    Note the accept call returns a new file descriptor. This file descriptor is specific to a particular client. It is common programming mistake to use the original server socket descriptor for server I/O and then wonder why networking code has failed.    The accept system call can optionally provide information about the remote client, by passing in a sockaddr struct. Different protocols have differently variants of the struct sockaddr, which are different sizes. The simplest struct to use is the sockaddr_storage which is sufficiently large to represent all possible types of sockaddr. Notice that C does not have any model of inheritance. Therefore we need to explicitly cast our struct to the ‘base type’ struct sockaddr.      struct sockaddr_storage clientaddr;  socklen_t clientaddrsize = sizeof(clientaddr);  int client_id = accept(passive_socket,          (struct sockaddr *) &amp;clientaddr,          &amp;clientaddrsize);              We’ve already seen getaddrinfo that can build a linked list of addrinfo entries (and each one of these can include socket configuration data). What if we wanted to turn socket data into IP and port addresses? Enter getnameinfo that can be used to convert a local or remote socket information into a domain name or numeric IP. Similarly the port number can be represented as a service name (e.g. “http” for port 80). In the example below we request numeric versions for the client IP address and client port number.      socklen_t clientaddrsize = sizeof(clientaddr);  int client_id = accept(sock_id, (struct sockaddr *) &amp;clientaddr, &amp;clientaddrsize);  char host[256], port[256];  getnameinfo((struct sockaddr *) &amp;clientaddr,        clientaddrsize, host, sizeof(host), port, sizeof(port),        NI_NUMERICHOST | NI_NUMERICSERV);              Discuss NI_MAXHOST and NI_MAXSERV, and NI_NUMERICHOST        (optional but highly recommended) int close(int fd) and int shutdown(int fd, int how)    Use the shutdown call when you no longer need to read any more data from the socket, write more data, or have finished doing both. When you shutdown a socket for further writing (or reading) that information is also sent to the other end of the connection. For example if you shutdown the socket for further writing at the server end, then a moment later, a blocked read call could return 0 to indicate that no more bytes are expected.    Use close when your process no longer needs the socket file descriptor.    If you fork-ed after creating a socket file descriptor, all processes need to close the socket before the socket resources can be re-used. If you shutdown a socket for further read then all process are be affected because you’ve changed the socket, not just the file descriptor.    Well written code will shutdown a socket before calling close it.  There are a few gotchas to creating a server.      Using the socket descriptor of the passive server socket (described above)        Not specifying SOCK_STREAM requirement for getaddrinfo        Not being able to re-use an existing port.        Not initializing the unused struct entries        The bind call will fail if the port is currently in use. Ports are per machine – not per process or user. In other words, you cannot use port 1234 while another process is using that port. Worse, ports are by default ‘tied up’ after a process has finished.  Server code exampleA working simple server example is shown below. Note this example is incomplete - for example it does not close either socket descriptor, or free up memory created by getaddrinfo#include &lt;string.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;netdb.h&gt;#include &lt;unistd.h&gt;#include &lt;arpa/inet.h&gt;int main(int argc, char **argv){    int s;    int sock_fd = socket(AF_INET, SOCK_STREAM, 0);    struct addrinfo hints, *result;    memset(&amp;hints, 0, sizeof(struct addrinfo));    hints.ai_family = AF_INET;    hints.ai_socktype = SOCK_STREAM;    hints.ai_flags = AI_PASSIVE;    s = getaddrinfo(NULL, \"1234\", &amp;hints, &amp;result);    if (s != 0) {            fprintf(stderr, \"getaddrinfo: %s\\n\", gai_strerror(s));            exit(1);    }    if (bind(sock_fd, result-&gt;ai_addr, result-&gt;ai_addrlen) != 0) {        perror(\"bind()\");        exit(1);    }    if (listen(sock_fd, 10) != 0) {        perror(\"listen()\");        exit(1);    }        struct sockaddr_in *result_addr = (struct sockaddr_in *) result-&gt;ai_addr;    printf(\"Listening on file descriptor %d, port %d\\n\", sock_fd, ntohs(result_addr-&gt;sin_port));    printf(\"Waiting for connection...\\n\");    int client_fd = accept(sock_fd, NULL, NULL);    printf(\"Connection made: client_fd=%d\\n\", client_fd);    char buffer[1000];    int len = read(client_fd, buffer, sizeof(buffer) - 1);    buffer[len] = '\\0';    printf(\"Read %d chars\\n\", len);    printf(\"===\\n\");    printf(\"%s\\n\", buffer);    return 0;}Layer 4: UDPUDP is a connectionless protocol that is built on top of IPv4 and IPv6. It’s very simple to use: Decide the destination address and port and send your data packet! However the network makes no guarantee about whether the packets will arrive. Packets (aka Datagrams) may be dropped if the network is congested. Packets may be duplicated or arrive out of order.Between two distant data-centers it’s typical to see 3% packet loss. Citation Needed A typical use case for UDP is when receiving up to date data is more important than receiving all of the data. For example, a game may send continuous updates of player positions. A streaming video signal may send picture updates using UDPUDP Attributes      Unreliable Datagram Protocol Packets sent through UDP are not guaranteed to reach their destination. The probability that the packet gets delivered goes down over time.        Simple The UDP protocol is supposed to have much less fluff than TCP. Meaning that for TCP there are a lot of configurable parameters and a lot of edge cases in the implementation. UDP is just fire and forget.        Stateless/Transaction The UDP protocol does not keep a “state” of the connection. This makes the protocol more simple and let’s the protocol represent simple transactions like requesting or responding to queries. There is also less overhead to sending a UDP message because there is no three way handshake.        Manual Flow/Congestion Control You have to manually manage the flow and congestion control which is a double edged sword. On one hand you have full control over everything, but on the other hand TCP has decades of optimization, meaning your protocol for its use cases needs to be more efficient that that to be more beneficial to use it.        Multicast This is one thing that you can only do with UDP. This means that you can send a message to every peer connected to a particular router that is part of a particular group.  UDP ClientUDP Clients are pretty versatile below is a simple client that sends a packet to a server specified through the command line. Note that this client sends a packet and doesn’t wait for acknowledgement. It fires and forgets. The example below also uses gethostbyname because some legacy functionality still works pretty well for setting up a client.    struct sockaddr_in addr;    memset(&amp;addr, 0, sizeof(addr));    addr.sin_family = AF_INET;    addr.sin_port = htons((uint16_t)port);  struct hostent *serv = gethostbyname(hostname);  if (!serv) {    perror(\"gethostbyname\");    exit(1);  } The previous code grabs an entry hostent that matches by hostname. Even though this isn’t portable, it definitely gets the job done. The full example follows.#include &lt;stdint.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/time.h&gt;#include &lt;assert.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;sys/time.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;netdb.h&gt;#include &lt;pthread.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;#include &lt;sys/mman.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;int connectToUDP(int port, char *hostname, struct sockaddr_in *ipaddr) {    int sockfd = socket(AF_INET, SOCK_DGRAM, 0);    if (sockfd &lt; 0) {        perror(\"socket\");    }    int optval = 1;    // Let them reuse    setsockopt(sockfd, SOL_SOCKET, SO_REUSEPORT, &amp;optval, sizeof(optval));    struct sockaddr_in addr;    memset(&amp;addr, 0, sizeof(addr));    addr.sin_family = AF_INET;    addr.sin_port = htons((uint16_t)port);  struct hostent *serv = gethostbyname(hostname);  if (!serv) {    perror(\"gethostbyname\");    exit(1);  }   memcpy(&amp;addr.sin_addr.s_addr, serv-&gt;h_addr, serv-&gt;h_length);  if (ipaddr) {    memcpy(ipaddr, &amp;addr, sizeof(*ipaddr));  }    // Timeouts for resending acks and whatnot    struct timeval tv;    tv.tv_sec = 0;    tv.tv_usec = SOCKET_TIMEOUT;    setsockopt(sockfd, SOL_SOCKET, SO_RCVTIMEO, &amp;tv, sizeof(tv));    return sockfd;}int main(int argc, char **argv) {  char *hostname = argv[1];  int port = strtoll(argv[2], NULL, 10);  struct sock_addr_in ipaddr;  int port_fd = connectToUDP(port, hostname, &amp;ipaddr, 0)   char *to_send = \"Hello!\"  int send_ret = sendto(port_fd, to_send, packet_size, 0,             (struct sockaddr *)&amp;ipaddr,             sizeof(ipaddr));  return 0;}UDP ServerThere are a variety of function calls available to send UDP sockets. We will use the newer getaddrinfo to help set up a socket structure. Remember that UDP is a simple packet-based (‘data-gram’) protocol ; there is no connection to set up between the two hosts. First, initialize the hints addrinfo struct to request an IPv6, passive datagram socket.memset(&amp;hints, 0, sizeof(hints));hints.ai_family = AF_INET6; // use AF_INET instead for IPv4hints.ai_socktype =  SOCK_DGRAM;hints.ai_flags =  AI_PASSIVE;Next, use getaddrinfo to specify the port number (we don’t need to specify a host as we are creating a server socket, not sending a packet to a remote host).getaddrinfo(NULL, \"300\", &amp;hints, &amp;res);sockfd = socket(res-&gt;ai_family, res-&gt;ai_socktype, res-&gt;ai_protocol);bind(sockfd, res-&gt;ai_addr, res-&gt;ai_addrlen);The port number is less than 1024, so the program will need root privileges. We could have also specified a service name instead of a numeric port value.So far the calls have been similar to a TCP server. For a stream-based service we would call listen and accept. For our UDP-serve we can just start waiting for the arrival of a packet on the socket-struct sockaddr_storage addr;int addrlen = sizeof(addr);// ssize_t recvfrom(int socket, void* buffer, size_t buflen, int flags, struct sockaddr *addr, socklen_t * address_len);byte_count = recvfrom(sockfd, buf, sizeof(buf), 0, &amp;addr, &amp;addrlen);The addr struct will hold sender (source) information about the arriving packet. Note the sockaddr_storage type is a sufficiently large enough to hold all possible types of socket addresses (e.g. IPv4, IPv6 and other socket types). The full UDP server code is below.#include &lt;string.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;netdb.h&gt;#include &lt;unistd.h&gt;#include &lt;arpa/inet.h&gt;int main(int argc, char **argv){    int s;    struct addrinfo hints, *res;    memset(&amp;hints, 0, sizeof(hints));    hints.ai_family = AF_INET6; // INET for IPv4    hints.ai_socktype =  SOCK_DGRAM;    hints.ai_flags =  AI_PASSIVE;    getaddrinfo(NULL, \"300\", &amp;hints, &amp;res);    int sockfd = socket(res-&gt;ai_family, res-&gt;ai_socktype, res-&gt;ai_protocol);    if (bind(sockfd, res-&gt;ai_addr, res-&gt;ai_addrlen) != 0) {        perror(\"bind()\");        exit(1);    }    struct sockaddr_storage addr;    int addrlen = sizeof(addr);    while(1){        char buf[1024];        ssize_t byte_count = recvfrom(sockfd, buf, sizeof(buf), 0, &amp;addr, &amp;addrlen);        buf[byte_count] = '\\0';        printf(\"Read %d chars\\n\", byte_count);        printf(\"===\\n\");        printf(\"%s\\n\", buf);    }    return 0;}Layer 7: HTTPLayer 7 of the OSI layer deals with application level interfaces. Meaning that you can ignore everything below this layer and treat an internet as a way of communicating with another computer than can be secure and the session may reconnect. Common layer 7 protocols are the following      HTTP(S) - Hyper Text Transfer Protocol. Sends arbitrary data and executes remote actions on a web server.        FTP - File Transfer Protocol. Transfers a file from one computer to another        TFTP - Trivial File Transfer Protocol. Same as above but using UDP.        DNS - Domain Name Service. Translates hostnames to IP addresses        SMTP - Simple Mail Transfer Protocol. Allows one to send plain text emails to an email server        SSH - Secure SHell. Allows one computer to connect to another computer and execute commands remotely.        Bitcoin - Decentralized crypto currency        BitTorrent - Peer to peer file sharing protocol        NTP - Network Time Protocol. This protocol helps keep your computer’s clock synced with the outside world  Remember when we were talking before about converting a website to an IP address? A system called “DNS” (Domain Name Service) is used. If a machine does not hold the answer locally then it sends a UDP packet to a local DNS server. This server in turn may query other upstream DNS servers.DNS by itself is fast but not secure. DNS requests are not encrypted and susceptible to ‘man-in-the-middle’ attacks. For example, a coffee shop internet connection could easily subvert your DNS requests and send back different IP addresses for a particular domain. The way this is usually subverted is that after the IP address is obtained then a connection is usually made over HTTPS. HTTPS uses what is called the TLS (formerly known as SSL) to secure transmissions and verify the IP address is who they say they are.DNS works like this in a nutshell      Send a UDP packet to your DNS server        If that DNS server has the packet cached return the result        If not ask higher level DNS servers for the answer. Cache and send the result        If either packet is not answered from within a guessed timeout, resend the request.  Nonblocking IONormally, when you call read(), if the data is not available yet it will wait until the data is ready before the function returns. When you’re reading data from a disk, that delay may not be long, but when you’re reading from a slow network connection it may take a long time for that data to arrive, if it ever arrives.POSIX lets you set a flag on a file descriptor such that any call to read() on that file descriptor will return immediately, whether it has finished or not. With your file descriptor in this mode, your call to read() will start the read operation, and while it’s working you can do other useful work. This is called “nonblocking” mode, since the call to read() doesn’t block.To set a file descriptor to be nonblocking:// fd is my file descriptorint flags = fcntl(fd, F_GETFL, 0);fcntl(fd, F_SETFL, flags | O_NONBLOCK);For a socket, you can create it in nonblocking mode by adding SOCK_NONBLOCK to the second argument to socket():fd = socket(AF_INET, SOCK_STREAM | SOCK_NONBLOCK, 0);When a file is in nonblocking mode and you call read(), it will return immediately with whatever bytes are available. Say 100 bytes have arrived from the server at the other end of your socket and you call read(fd, buf, 150). Read will return immediately with a value of 100, meaning it read 100 of the 150 bytes you asked for. Say you tried to read the remaining data with a call to read(fd, buf+100, 50), but the last 50 bytes still hadn’t arrived yet. read() would return -1 and set the global error variable errno to either EAGAIN or EWOULDBLOCK. That’s the system’s way of telling you the data isn’t ready yet.write() also works in nonblocking mode. Say you want to send 40,000 bytes to a remote server using a socket. The system can only send so many bytes at a time. Common systems can send about 23,000 bytes at a time. In nonblocking mode, write(fd, buf, 40000) would return the number of bytes it was able to send immediately, or about 23,000. If you called write() right away again, it would return -1 and set errno to EAGAIN or EWOULDBLOCK. That’s the system’s way of telling you it’s still busy sending the last chunk of data, and isn’t ready to send more yet.There are a few ways to check that your IO has finished. Let’s see how to do it using select and epoll.int select(int nfds,            fd_set *readfds,            fd_set *writefds,           fd_set *exceptfds,            struct timeval *timeout);Given three sets of file descriptors, select() will wait for any of those file descriptors to become ‘ready’.      readfds - a file descriptor in readfds is ready when there is data that can be read or EOF has been reached.        writefds - a file descriptor in writefds is ready when a call to write() will succeed.        exceptfds - system-specific, not well-defined. Just pass NULL for this.  select() returns the total number of file descriptors that are ready. If none of them become ready during the time defined by timeout, it will return 0. After select() returns, the caller will need to loop through the file descriptors in readfds and/or writefds to see which ones are ready. As readfds and writefds act as both input and output parameters, when select() indicates that there are file descriptors which are ready, it would have overwritten them to reflect only the file descriptors which are ready. Unless it is the caller’s intention to call select() only once, it would be a good idea to save a copy of readfds and writefds before calling it.fd_set readfds, writefds;FD_ZERO(&amp;readfds);FD_ZERO(&amp;writefds);for (int i=0; i &lt; read_fd_count; i++)  FD_SET(my_read_fds[i], &amp;readfds);for (int i=0; i &lt; write_fd_count; i++)  FD_SET(my_write_fds[i], &amp;writefds);struct timeval timeout;timeout.tv_sec = 3;timeout.tv_usec = 0;int num_ready = select(FD_SETSIZE, &amp;readfds, &amp;writefds, NULL, &amp;timeout);if (num_ready &lt; 0) {  perror(\"error in select()\");} else if (num_ready == 0) {  printf(\"timeout\\n\");} else {  for (int i=0; i &lt; read_fd_count; i++)    if (FD_ISSET(my_read_fds[i], &amp;readfds))      printf(\"fd %d is ready for reading\\n\", my_read_fds[i]);  for (int i=0; i &lt; write_fd_count; i++)    if (FD_ISSET(my_write_fds[i], &amp;writefds))      printf(\"fd %d is ready for writing\\n\", my_write_fds[i]);}For more information on select()epollepoll is not part of POSIX, but it is supported by Linux. It is a more efficient way to wait for many file descriptors. It will tell you exactly which descriptors are ready. It even gives you a way to store a small amount of data with each descriptor, like an array index or a pointer, making it easier to access your data associated with that descriptor.To use epoll, first you must create a special file descriptor with epoll_create(). You won’t read or write to this file descriptor; you’ll just pass it to the other epoll_xxx functions and call close() on it at the end.    epfd = epoll_create(1);For each file descriptor you want to monitor with epoll, you’ll need to add it to the epoll data structures using epoll_ctl() with the EPOLL_CTL_ADD option. You can add any number of file descriptors to it.struct epoll_event event;event.events = EPOLLOUT;  // EPOLLIN==read, EPOLLOUT==writeevent.data.ptr = mypointer;epoll_ctl(epfd, EPOLL_CTL_ADD, mypointer-&gt;fd, &amp;event)To wait for some of the file descriptors to become ready, use epoll_wait(). The epoll_event struct that it fills out will contain the data you provided in event.data when you added this file descriptor. This makes it easy for you to look up your own data associated with this file descriptor.int num_ready = epoll_wait(epfd, &amp;event, 1, timeout_milliseconds);if (num_ready &gt; 0) {  MyData *mypointer = (MyData*) event.data.ptr;  printf(\"ready to write on %d\\n\", mypointer-&gt;fd);}Say you were waiting to write data to a file descriptor, but now you want to wait to read data from it. Just use epoll_ctl() with the EPOLL_CTL_MOD option to change the type of operation you’re monitoring.event.events = EPOLLOUT;event.data.ptr = mypointer;epoll_ctl(epfd, EPOLL_CTL_MOD, mypointer-&gt;fd, &amp;event);To unsubscribe one file descriptor from epoll while leaving others active, use epoll_ctl() with the EPOLL_CTL_DEL option.epoll_ctl(epfd, EPOLL_CTL_DEL, mypointer-&gt;fd, NULL);To shut down an epoll instance, close its file descriptor.close(epfd);In addition to nonblocking read() and write(), any calls to connect() on a nonblocking socket will also be nonblocking. To wait for the connection to complete, use select() or epoll to wait for the socket to be writable. There are definitely reasons to use epoll over select but due to to interface, there are fundamental problems with doing so.Blogpost about select being brokenRemote Procedure CallsRemote Procedure Call. RPC is the idea that we can execute a procedure (function) on a different machine. In practice the procedure may execute on the same machine, however it may be in a different context - for example under a different user with different permissions and different lifecycle.What is Privilege Separation?The remote code will execute under a different user and with different privileges from the caller. In practice the remote call may execute with more or fewer privileges than the caller. This in principle can be used to improve the security of a system (by ensuring components operate with least privilege). Unfortunately, security concerns need to be carefully assessed to ensure that RPC mechanisms cannot be subverted to perform unwanted actions. For example, an RPC implementation may implicitly trust any connected client to perform any action, rather than a subset of actions on a subset of the data.What is stub code? What is marshalling?The stub code is the necessary code to hide the complexity of performing a remote procedure call. One of the roles of the stub code is to marshall the necessary data into a format that can be sent as a byte stream to a remote server.// On the outside 'getHiscore' looks like a normal function call// On the inside the stub code performs all of the work to send and receive the data to and from the remote machine.int getHighScore(char* game) {  // Marshall the request into a sequence of bytes:  char* buffer;  asprintf(&amp;buffer,\"getHiscore(%s)!\", name);  // Send down the wire (we do not send the zero byte; the '!' signifies the end of the message)  write(fd, buffer, strlen(buffer) );  // Wait for the server to send a response  ssize_t bytesread = read(fd, buffer, sizeof(buffer));  // Example: unmarshal the bytes received back from text into an int  buffer[bytesread] = 0; // Turn the result into a C string  int score= atoi(buffer);  free(buffer);  return score;}What is server stub code? What is unmarshalling?The server stub code will receive the request, unmarshall the request into a valid in-memory data call the underlying implementation and send the result back to the caller.How do you send an int? float? a struct? A linked list? A graph?To implement RPC you need to decide (and document) which conventions you will use to serialize the data into a byte sequence. Even a simple integer has several common choices:      Signed or unsigned?        ASCII, Unicode Text Format 8, some other encoding?        Fixed number of bytes or variable depending on magnitude        Little or Big endian binary format?  To marshall a struct, decide which fields need to be serialized. It may not be necessary to send all data items (for example, some items may be irrelevant to the specific RPC or can be re-computed by the server from the other data items present).To marshall a linked list it is unnecessary to send the link pointers- just stream the values. As part of unmarshalling the server can recreate a linked list structure from the byte sequence.By starting at the head node/vertex, a simple tree can be recursively visited to create a serialized version of the data. A cyclic graph will usually require additional memory to ensure that each edge and vertex is processed exactly once.What is an Interface Description Language (IDL)?Writing stub code by hand is painful, tedious, error prone, difficult to maintain and difficult to reverse engineer the wire protocol from the implemented code. A better approach is specify the data objects, messages and services and automatically generate the client and server code.A modern example of an Interface Description Language is Google’s Protocol Buffer .proto files.Complexity and challenges of RPC vs local calls?Remote Procedure Calls are significantly slower (10x to 100x) and more complex than local calls. An RPC must marshall data into a wire-compatible format. This may require multiple passes through the data structure, temporary memory allocation and transformation of the data representation.Robust RPC stub code must intelligently handle network failures and versioning. For example, a server may have to process requests from clients that are still running an early version of the stub code.A secure RPC will need to implement additional security checks (including authentication and authorization), validate data and encrypt communication between the client and host.Transferring large amounts of structured dataLet’s examine three methods of transferring data using 3 different formats - JSON, XML and Google Protocol Buffers. JSON and XML are text-based protocols. Examples of JSON and XML messages are below.&lt;ticket&gt;&lt;price currency='dollar'&gt;10&lt;/price&gt;&lt;vendor&gt;travelocity&lt;/vendor&gt;&lt;/ticket&gt;{ 'currency':'dollar' , 'vendor':'travelocity', 'price':'10' }Google Protocol Buffers is an open-source efficient binary protocol that places a strong emphasis on high throughput with low CPU overhead and minimal memory copying. Implementations exist for multiple languages including Go, Python, C++ and C. This means client and server stub code in multiple languages can be generated from the .proto specification file to marshall data to and from a binary stream.Google Protocol Buffers reduces the versioning problem by ignoring unknown fields that are present in a message. See the introduction to Protocol Buffers for more information.Topics      IPv4 vs IPv6        TCP vs UDP        Packet Loss/Connection Based        Get address info        DNS        TCP client calls        TCP server calls        shutdown        recvfrom        epoll vs select        RPC  Questions      What is IPv4? IPv6? What are the differences between them?        What is TCP? UDP? Give me advantages and disadvantages of both of them. When would I use one and not the other?        Which protocol is connection less and which one is connection based?        What is DNS? What is the route that DNS takes?        What does socket do?        What are the calls to set up a TCP client?        What are the calls to set up a TCP server?        What is the difference between a socket shutdown and closing?        When can you use read and write? How about recvfrom and sendto?        What are some advantages to epoll over select? How about select over epoll?        What is a remote procedure call? When should I use it?        What is marshalling/unmarshalling? Why is HTTP not an RPC?  "
  },{
    "title": "Post Mortems",
    "url": " /coursebook/Post_Mortems",
   "content": "  Post Mortems          Shell Shock      Heartbleed      Dirty Cow      Meltdown      Spectre      Mars Pathfinder      Mars Again      Year 2038      Northeast Blackout of 2003      Apple IOS Unicode Handling      Apple SSL Verification      Sony Rootkit Installation      Civilization and Ghandi      The Woes of Shell Scripting      Appnexus Double Free      ATT Cascading Failures - 1990      [1][]  Post MortemsThis chapter is meant to serve as a big “why are we learning all of this”. In all of your previous classes, you were learning what to do. How to program a data structure, how to code a for loop, how to prove something. This is the first class that is largely focused on what not to do. As a result we draw experience from our past in very real ways. Sit back and scroll through this chapter as we tell you about problems of programmers past. Even if you are dealing in something much higher level like web-development, everything relates back to the system.Shell ShockRequired: Appendix/ShellThis was a back door into most shells. The bug allowed an attacher to exploit an environment variable to execute arbitrary code.  &gt; env x='() { :;}; echo vulnerable' bash -c \"echo this is a test\"  vulnerable...This meant that in any system that uses environment variables and doesn’t sanitize their input (hint no one sanitized environment variable input because they saw it as safe) you can execute whatever code you want on others machines including setting up a webserver.Lessons Learned: On production machines make sure that there is a minimal operating system (something like BusyBox with DietLibc) so that you can understand most of the code in the systems and their effectiveness. Put in multiple layers of abstraction and checks to make sure that data isn’t leaked. For example the above is a problem insofar as getting information back to the attackers if it is allowed to communicate with them. This means that you can harden your machine ports by disallowing connections on all but a few ports. In addition, you can harden your system to never perform exec calls to perform tasks (i.e. perform an exec call to update a value) and instead do it in C or your favorite programming language of choice. Although you don’t have flexibility, you have peace of mind what you allow users to do.HeartbleedRequired: Intro to CTo put it simply, there was no bounds on buffer checking. The SSL Heartbeat is super simple. A server sends a string of a certain length, and the second server is supposed to send the string of the length back. The problem is someone can maliciously change the size of the request to larger than what they sent (i.e. send “cat” but request 500 bytes) and get crucial information like passwords, SSL keys etc from the server. There is a Relevant XKCD on it.Lessons Learned: Check your buffers! Know the difference between a buffer and a string.Dirty CowRequired: Processes/Virtual MemoryDirty CowA process usually has access to a set of read-only mappings of memory that if they try to write to they get a segfault. Dirty COW is a vulnerability where a bunch of threads try to access the same piece of memory at the same time hoping that one of the threads flips the NX bit and the writable bit. After that, an attacker can modify the page. This can be done to the effective user id bit and the process can pretend it was running as root and spawn a root shell, allowing access to the system from a normal shell. Naturally this is undesirable.Lessons Learned: Spinlocks in the kernel are hard.MeltdownThere is an example of this in the background sectionSpectreSame hereMars PathfinderRequired sections: Synchronization and a bit of SchedulingPathfinder LinkThe mars pathfinder was a mission that tried to collect climate data on Mars. The finder use a signle bus to communicate with different parts. Since this was 1997, the hardware itself didn’t have advanced features like efficient locking so it was up to the operating system developers to regulate that with mutexes. The architecture was pretty simple. There was a thread that controlled data along the information bus, communications thread, and data collection thread in with high, regular, and low priorities with respect to scheduling. The other caveat is that if an interrupt happened at some interval and a task is running and a task is to be scheduled, the task that has the higher priority wins.The pattern that caused everything to start failing was the data collection thread starts writing to the bus, the information bus thread is waiting on the data and then out of nowhere the communication thread comes in. The communication thread would preempt the other lower priority thread while the lower priority thread still held the mutex. This means when the regular priority thread tried to lock the bus, you’d get deadlock. After some time the system would reset, but ideally this isn’t a good thing to leave to chance.Moral of the lesson? Don’t have the applications themselves deal with the synchronization. Define a module that handles mutex locking and have module communicate with each others through files, IPC, etc etc.Mars AgainRequired Sections: MallocMarsThe short of it is they ran out of memory. The long of it is they ran out of memory, disk space, and swap space. The moral of the story? Make sure to write code that can handle file failures and can handle files when they close and go out of memory, so the operating system can hotswap file to free up memory. Also clean up files, assume that your tmp directory is roughly a hundreth or a thousandth of the total size and use that.Year 2038Required sections: Intro to C2038This is actual a fundamental problem that hasn’t happened yet. Unix time stamps are kept as the number of seconds from a particular day (Jan 1st 1970). This is stored as a 32 bit signed integer. In March of 2038, this number will overflow. This isn’t a problem for most modern operating system who store 64 bit signed integers which is enough to keep us going until the end of time, but it is a problem for embedded devices that we can’t change the internal hardware to. Stay tuned to see what happens.Lessons learned: Plan like your application will be huge one day.Northeast Blackout of 2003Required Sections: Synchronization2003Very simply put a race condition triggered a series of undefined events in a system that cause the blackout of most of the northeastern part of North America for quite some time. This bug also turned off or caused the backup system and the logging systems to fail so people didn’t even know of the bug for an hour. The exact bits that were flipped are not known, but patches have been put into place.Lessons Learned: Modularize your code to localize failures (i.e. keeping race conditions different between processes). If you need to synchronize among processes make sure your failure detection system is not interlaced with your system.Apple IOS Unicode HandlingRequired Sections: Intro to CCrashing your iphone with textWonder why we teach string parsing? Because it is hard thing to do even for professional software developers. This bug allowed a lot of undefined behavior when trying to parse a series of unicode characters. Apple probably knows why this happened, but my guess is that the parsing of the string happens somewhere inside the kernel and a segfault is reached. When you get a segfault in the kernel, your kernel panics, and the entire device reboots. Undefined behavior means anything though, and a lot of varied things did happen with this bug.Lessons Learned: Fuzz your kernelApple SSL VerificationRequired Sections: Intro to CApple BugDue to a stray goto in Apple’s code, a function always returned that an SSL certificate was valid. Naturally hackers were able to get away with some pretty crazy site names.Lessons Learned: Always bracket if statements, use gotos sparingly. Chances are if you need to use a goto, write another function or a switch statement with fallthroughs (still bad).Sony Rootkit InstallationRequired Sections: Intro to C/ProcessesRoot Kit ScandalPicture this. It’s 2005, limewire came a few years prior, the internet was a growing pool of illegal activities – not to say that is fixed now. In this day, Sony knew that it didn’t have the computing power to police all the interwebs or get around the various technologies people were using to get around copyright protections. So what did they do? With 22 million Music CDs, they required users to install a rootkit on their operating system, so Sony can monitor the device for unethical activities.Privacy concerns aside, and believe me there are a lot fof them, the big problem was that this rootkit is a backdoor for everyone’s systems if not programmed correctly. A rootkit is a piece of code usually installed kernel-side that keeps track of almost anything that a user does. What websites visited, what clicks or keys typed etc etc. If a hacker finds out about this and there is a way to access that API from the user space level, that means any program has the ability to find out important information about your device. Needless to say, people where not pleased.Lessons Learned: Get an antivirus and/or apparmor and make sure that an application is only requesting permissions that make sense. If you are really torn, try something like Windows sandbox or keep a Sacrificial VM around to see if installing it makese your compute horrible. Don’t trust certificates trust code.Civilization and GhandiRequired Sections: Intro to CGhandi’s AggressivenessThis is probably well known to gamers why someone as (in real life) non-violent as Ghandi was uber aggressive in the video game civilization. In the original, the game kept aggressiveness as an unsigned integer. During the course of the game the integer could be decremented and then the problem ensued because Ghandi was already at 0. This caused him to become the most aggressive character in the game.Lessons Learned: The takeway from this is Never use unsigned numbers unless you have an express written reason for it (reasons include you need to know about the overflow behavior, you are bit shifting, you are bit masking). In ever other case, cast it.The Woes of Shell ScriptingRequired Sections: Intro to C/AppendingSteamThere was a simple bug in steam that caused steam to remove all of your files in the form of something like this  &gt; ROOT=$(cd $0/; echo $PWD);  &gt; rm -rf $ROOTWell what happens if $0 or the first parameter passed into a script doesn’t exist? You move to root, and you delete your entire computer.Lessons Learned: Do parameter checks, always always always set set -e on a script and if you expect a command to fail, explicitly list it. You can also alias rm to mv and then delete the trash later.Appnexus Double FreeRequired Sections: Intro to C/MallocDouble FreeIt may not be as simple as it sounds. Appnexus uses an asynchronous garbage collector that reclaims different parts of the heap as it believes that objects are not being used anymore. The architecture is that an element is in the used list and then it is taken out to a to-be-freed list. After a certain time if that element was not used, it is freed and added to the free list. This is fine until two thread try to delete the same object at once, adding to the list twice. After less time, one of the objects was deleted, the delete was announced to other computers.Lessons Learned: Try not to make hacky software if you need to. Modularize, and set memory limits, and monitor different parts of your code and optimize by hand. There is no general catch-all garbage collector that fits everyone. Even highly tested ones like the JVM need some nudges if you want to get performance out of them.ATT Cascading Failures - 1990Required Sections: Intro to CExplanationTo be honest the explanation of the bug is really good at the link above, I recommend reading that. Basically it was a series of network delays that caused some switches (telephone) across the coutnry to think that other switches were operable when they weren’t. When the switches came back online, they realized they had a huge backlog of calls to route and began doing so. Other routing failures and restarts only compounded the problem.Lessons Learned: Not using C would’ve actually helped here because of more rigorous fuzzing (though C++ in this day and age would be worse with its language constructs). The real moral of the story is networks are random and expect any jump at any point in your code. That means writing simulations and running them with random delays to figure out bugs before they happen."
  },{
    "title": "Processes",
    "url": " /coursebook/Processes",
   "content": "  Processes          Processes      Process Contents                  Memory Layout          Other Contents                    Intro to Fork                  A word of warning          What does fork do?          What is a fork bomb?          POSIX Fork Detailings                    Waiting and Execing                  Exit statuses          Zombies and Orphans          Extra: How can I asynchronously wait for my child using SIGCHLD?                    exec                  Shortcuts                    The fork-exec-wait Pattern                  Environment Variables                    Further Reading                  Topics                    Questions/Exercises      [1][]  ProcessesIn the beginning, there is a kernel. The operating system kernel is a special piece of software. This is the piece of software that is loaded up before all of your other programs even consider getting booted up. What the kernel does is the following, abbreviated      The operating system executes ROM or read only code        The operating system then executes a boot_loader or EFI extensions nowadays        The boot_loader loads your kernels        Your kernel executes init to bootstrap itself from nothing        The kernel executes start up scripts        The kernel executes userland scripts, and you get to use your computer!  You don’t need to know the specifics of the booting process, but there it is. When you are executing in user space the kernel provides some important operations that programs don’t have to worry about.      Scheduling Processes and threads        Handling synchronization primitives        Providing System Calls like write or read        Manages virtual memory and low level binary devices like usb drivers        Handles reading and understanding a filesystem        Handles communicating over networks        Handles communications with other processes        Dynamically linking libraries  The kernel handles all of this stuff in kernel mode. Kernel mode gets you greater power, like executing extra CPU instructions but at the cost of one failure crashes your entire computer – ouch. That is what you are going to interacting with in this class. One of the things that you have already become familiar with is that the kernel gives you file descriptors when you open text files. Here is a zine from Julia Evans that details it a bit.As the little zine shows, the Kernel keeps track of the file descriptors and what they point to. We will see later that file descriptors need not point to actual files and the OS keeps track of them for you. Also, notice that between processes file descriptors may be reused but inside of a process they are unique. File descriptors also have a notion of position. You can read a file on disk completely because the OS keeps track of the position in the file, and that belongs to your process as well.ProcessesA process an instance of a computer program that may be running. Processes have a lot of things at their disposal. At the start of each program you get one process, but each program can make more processes. In fact, your operating system starts up with only one process and all other processes are forked off of that – all of that is done under the hood when booting up. A program consists of the following.      A binary format: This tells the operating system which set of bits in the binary are what – which part is executable, which parts are constants, which libraries to include etc.        A set of machine instructions        A number denoting which instruction to start from        Constants        Libraries to link and where to fill in the address of those libraries  When your operating system starts on a linux machine, there is a process called init.d that gets created. That process is a special one handling signals, interrupts, and a persistence module for certain kernel elements. Whenever you want to make a new process, you call fork and use exec to load another program.Processes are very powerful but they are isolated! That means that by default, no process can communicate with another process. This is very important because if you have a large system (let’s say the University of Illinois Engineering Workstations) then you want some processes to have higher privileges than your average user, and one certainly doesn’t want the average user to be able to bring down the entire system either on purpose or accidentally by modifying a process.  int secrets;  secrets++;  printf(\"%d\\n\", secrets);On two different terminals, as you would guess they would both print out 1 not 2. Even if we changed the code to do something really hacky, there would be no way to change another process’ state (Okay, check out the Post-Mortems chapter if you want to break some things).Process ContentsMemory LayoutWhen a process starts, it gets its own address space. Each process gets the following.      A Stack. The stack is the place where automatic variable and function call return addresses are stored. Every time a new variable is declared, the program moves the stack pointer down to reserve space for the variable. This segment of the stack is Writable but not executable. If the stack grows too far – meaning that it either grows beyond a preset boundary or intersects the heap – you will get a stackoverflow most likely resulting in a SEGFAULT. The stack is statically allocated by default meaning that there is only a certain amount of space to which one can write        A Heap. The heap is an expanding region of memory. If you want to allocate a large object, it goes here. The heap starts at the top of the text segment and grows upward, meaning sometimes when you call malloc that it asks the operating system to push the heap boundary upward. More on that in the memory allocation chapter. This area is also writable but not executable. One can run out of heap memory if the system is constrained or if you run out of addresses. This is more common on a 32bit system.        A Data Segment This contains all of your globals. This section starts at the end of the text segment and is static in size because the amount of globals is known at compile time. There are two areas to the data usually the IBSS and the UBSS which stand for the initialized basic service set and the uninitialized data segment respectively. This section is Writable but not Executable and there isn’t anything else too fancy here.        A Text Segment. This is where all your code is stored – all the 1’s and 0’s. The program counter moves through this segment executing instructions and moving down the next instruction. It is important to note that this is the only Executable section of the code created by default. If you try to change the code while it’s running, most likely you will segfauls. There are ways around it, but we won’t be exploring those in this course. Why doesn’t it start at zero? Because of Address Space Layout Randomization. It is outside the scope of this class but know that it exists.  Other ContentsTo keep track of all these processes, your operating system gives each process a number and that process is called the PID, process ID. Processes also have a ppid which is short for parent process id. Every process has a parent, that parent could be init.d.Processes could also contain      Running State - Whether a process is getting ready, running, stopped, terminated etc.        File Descriptors - List of mappings from integers to real devices (files, usb sticks, sockets)        Permissions - What user the file is running on and what group the process belongs to. The process can then only do this admissible to the user or group like opening a file that the user has made exclusives. There are tricks to make a program not be the user who started the program i.e. sudo takes a program that a user starts and executes it as root.        Arguments - a list of strings that tell your program what parameters to run under        Environment List - a list of strings in the form NAME=VALUE that one can modify.  Intro to ForkA word of warningProcess forking is a powerful and dangerous tool. If you mess up and cause a fork bomb, you can bring down the entire system. To reduce the chances of this, limit your maximum number of processes to a small number e.g 40 by typing ulimit -u 40 into a command line. Note, this limit is only for the user, which means if you fork bomb, then you won’t be able to kill all of the processes you just created since calling killall requires your shell to fork() … ironic right? One solution is to spawn another shell instance as another user (for example root) before hand and kill processes from there. Another is to use the built in exec command to kill all the user processes (careful you only have one shot at this). Finally you could reboot the system, but you only have one shot at this with the exec function. When testing fork() code, ensure that you have either root and/or physical access to the machine involved. If you must work on fork() code remotely, remember that kill -9 -1 will save you in the event of an emergency.TL;DR: Fork can be extremely dangerous if you aren’t prepared for it. You have been warned.What does fork do?The fork system call clones the current process to create a new process. It creates a new process (the child process) by duplicating the state of the existing process with a few minor differences. The child process does not start from main. Instead it executes the next line after the fork() just as the parent process does. Just as a side remark, in older UNIX systems, the entire address space of the parent process was directly copied (regardless of whether the resource was modified or not). These days, kernel performs copy-on-write, which saves a lot of resources, while being very time efficient. Here’s a very simple example  printf(\"I'm printed once!\\n\");  fork();  // Now there are two processes running if fork succeeded  // and each process will print out the next line.  printf(\"You see this line twice!\\n\");The following program may print out 42 twice - but the fork() is after the printf!? Why?  #include &lt;unistd.h&gt; /*fork declared here*/  #include &lt;stdio.h&gt; /* printf declared here*/  int main() {    int answer = 84 &gt;&gt; 1;    printf(\"Answer: %d\", answer);    fork();    return 0;  }The printf line is executed only once however notice that the printed contents is not flushed to standard out. There’s no newline printed, we didn’t call fflush, or change the buffering mode. The output text is therefore still in process memory waiting to be sent. When fork() is executed the entire process memory is duplicated including the buffer. Thus the child process starts with a non-empty output buffer which will be flushed when the program exits.To write code that is different for the parent and child process, check the return value of fork(). If fork() returns -1, that implies something went wrong in the process of creating a new child. One should check the value stored in errno to determine what kind of error occurred; commons one include EAGAIN and ENOMEM Which are essentially try again and out of memory. Similarly, a return value of 0 indicates that we are in the child process, while a positive integer shows that we are in parent process. The positive value returned by fork() gives as the process id (pid) of the child.Another way to remember which is which is that the child process can find its parent - the original process that was duplicated - by calling getppid() - so does not need any additional return information from fork(). The parent process however can only find out the id of the new child process from the return value of fork:  pid_t id = fork();  if (id == -1) exit(1); // fork failed   if (id &gt; 0) {     // I'm the original parent and     // I just created a child process with id 'id'    // Use waitpid to wait for the child to finish  } else { // returned zero    // I must be the newly made child process  }A slightly silly example is shown below. What will it print? Try it with multiple arguments to your program.  #include &lt;unistd.h&gt;  #include &lt;stdio.h&gt;  int main(int argc, char **argv) {    pid_t id;    int status;     while (--argc &amp;&amp; (id=fork())) {      waitpid(id,&amp;status,0); /* Wait for child*/    }    printf(\"%d:%s\\n\", argc, argv[argc]);    return 0;  }Another example is below. This is the amazing parallel apparent-O(N) sleepsort is today’s silly winner. First published on 4chan in 2011. A version of this awful but amusing sorting algorithm is shown below.  int main(int c, char **v) {    while (--c &gt; 1 &amp;&amp; !fork());    int val  = atoi(v[c]);    sleep(val);    printf(\"%d\\n\", val);    return 0;  }Note: The algorithm isn’t actually O(N) because of how the system scheduler works, though you can get faster times.What is a fork bomb?A ‘fork bomb’ is when you attempt to create an infinite number of processes. This will often bring a system to a near-standstill as it attempts to allocate CPU time and memory to a very large number of processes that are ready to run. System administrators don’t like fork-bombs and may set upper limits on the number of processes each user can have or may revoke login rights because it creates a disturbance in the force for other users’ programs. You can also limit the number of child processes created by using setrlimit(). fork bombs are not necessarily malicious - they occasionally occur due to student coding errors. Below is a simple example.  while (1) fork();There may even be subtle forkbombs that occur when you are being careless while coding.  #include &lt;unistd.h&gt;  #define HELLO_NUMBER 10  int main(){    pid_t children[HELLO_NUMBER];    int i;    for(i = 0; i &lt; HELLO_NUMBER; i++){      pid_t child = fork();      if(child == -1){        break;      }      if(child == 0){ //I am the child        execlp(\"ehco\", \"echo\", \"hello\", NULL);      }      else{        children[i] = child;      }    }    int j;    for(j = 0; j &lt; i; j++){      waitpid(children[j], NULL, 0);    }    return 0;  }We misspelled ehco, so we can’t exec it. What does this mean? Instead of creating 10 processes we just created 210 processes, fork bombing our machine. How could we prevent this? Put an exit right after exec so in case exec fails we won’t end up fork bombing our machine.POSIX Fork DetailingsPOSIX determines what the standards of fork are. The commonly expected standards are detailed in the posix fork page Here are a summary of what gets inherirted\\item Fork will return a non-negative integer on success\\item A child will inherit any open file descriptors of the parent. That means if a parent reads a bit of the file and forks, the child will start at that offset. Any other flags are also carried over.\\item Pending signals are not inherited. This means that if a parent has a pending signal and creates a child, the child will not receive that signal unless another process signals the child.\\item The process will be created with one thread (more on that later, the general consensus is to not fork and pthread at the same time).\\item Since we have copy on write, read-only memory addresses are shared between processes\\item If you set up certain regions of memory, they are shared between processes.\\item Signal handlers are inherited but can be changed\\item Current working directory is inherited but can be changed\\item Environment variables are inherited but can be changedKey differences include:      The process id returned by getpid(). The parent process id returned by getppid().        The parent is notified via a signal, SIGCHLD, when the child process finishes but not vice versa.        The child does not inherit pending signals or timer alarms. For a complete list see the fork man page        The child has its own set of environment variables  Waiting and ExecingIf the parent process wants waits for the child to finish, waitpid (or wait).  pid_t child_id = fork();  if (child_id == -1) { perror(\"fork\"); exit(EXIT_FAILURE);}  if (child_id &gt; 0) {     // We have a child! Get their exit code    int status;     waitpid( child_id, &amp;status, 0 );    // code not shown to get exit status from child  } else { // In child ...    // start calculation    exit(123);  }wait is a simpler version of waitpid. wait accepts a pointer to an integer and waits on any child process. After the first one changes state wait returns. waitpid is similar to wait but it has a few differences. First, you can wait on a specific process, or you can pass in special values for the pid to do different things (check the man pages). The last parameter to waitpid is an option parameter. The options are listed below\\item WNOHANG - Return whether or not the searched process is exited\\item WNOWAIT - Wait, but leave the child waitable by another wait call\\item WEXITED - Wait for exited children\\item WSTOPPED - Wait for stopped children\\item WCONTINUED - Wait for continued childrenIt is generally poor programming practice to use signals in program logic. Signals have no timeframe of delivery and no assurance that they will be deliverd. If you need to communicate between two processes, there are other ways of doing so.Exit statuses or the value stored in the integer pointer for both of the calls above are explained below.Exit statusesTo find the return value of main() or value included in exit()), Use the Wait macros - typically you will use WIFEXITED and WEXITSTATUS . See wait/waitpid man page for more information.  int status;  pid_t child = fork();  if (child == -1) return 1; //Failed  if (child &gt; 0) { /* I am the parent - wait for the child to finish */    pid_t pid = waitpid(child, &amp;status, 0);    if (pid != -1 &amp;&amp; WIFEXITED(status)) {      int low8bits = WEXITSTATUS(status);      printf(\"Process %d returned %d\" , pid, low8bits);    }  } else { /* I am the child */    // do something interesting    execl(\"/bin/ls\", \"/bin/ls\", \".\", (char *) NULL); // \"ls .\"  }A process can only have 256 return values, the rest of the bits are informational, this is done by bit shifting. But, The kernel has an internal way of keeping track of signaled, exited, or stopped. That API is abstracted so that that the kernel developers are free to change at will. Remember that these macros only make sense if the precondition is met. Meaning that a process’ exit status won’t be defined if the process isn’t signaled. The macros will not do the checking for you, so it’s up to the programmer to make sure the logic checks out. As an example above, you should use the WIFSTOPPED to check if a process was stopped and then the WSTOPSIG to find the signal that stopped it. As such there is no need to memorize the following, this is just a high level overview of how information is stored inside the status variables. From sys/wait.h of an old Berkeley kernel:  /* If WIFEXITED(STATUS), the low-order 8 bits of the status. */  #define _WSTATUS(x) (_W_INT(x) &amp; 0177)  #define _WSTOPPED 0177    /* _WSTATUS if process is stopped */  #define WIFSTOPPED(x) (_WSTATUS(x) == _WSTOPPED)  #define WSTOPSIG(x) (_W_INT(x) &gt;&gt; 8)  #define WIFSIGNALED(x)  (_WSTATUS(x) != _WSTOPPED &amp;&amp; _WSTATUS(x) != 0)  #define WTERMSIG(x) (_WSTATUS(x))  #define WIFEXITED(x)  (_WSTATUS(x) == 0)There is an untold convention about exit codes. If the process exited normally and everything was successful, then a zero should be returned. Beyond that, there isn’t too many conventions except the ones that you place on yourself. If you know how the program you spawn is going to interact, you may be able to make more sense of the 256 error codes. You could in fact write your program to return 1 if the program went to stage 1 (like writing to a file) 2 if it did something else etc… But none of the unix programs are designed to follow that for simplicity sake.Zombies and OrphansIt is good practice to wait on your children! If you don’t wait on your children they become zombies. Zombies occur when a child terminates and then take up a spot in the kernel process table for your process. The process table keeps information about that process like pid, status, how it was killed. The only way to get rid of a zombie is to wait on your children. If you never wait on your children, and the program is long running then you may lose the ability to fork.You don’t always need to wait for your children! Your parent process can continue to execute code without having to wait for the child process. If a parent dies without waiting on its children, a process can orphan its children. Once a parent process completes, any of its children will be assigned to init - the first process with pid of 1. Thus these children would see getppid() return a value of 1. These orphans will eventually finish and for a brief moment become a zombie. The init process automatically waits for all of its children, thus removing these zombies from the system.Extra: How can I asynchronously wait for my child using SIGCHLD?Warning: This section uses signals which we have not yet fully introduced. The parent gets the signal SIGCHLD when a child completes, so the signal handler can wait on the process. A slightly simplified version is shown below.  pid_t child;  void cleanup(int signal) {    int status;    waitpid(child, &amp;status, 0);    write(1,\"cleanup!\\n\",9);  }  int main() {    // Register signal handler BEFORE the child can finish    signal(SIGCHLD, cleanup); // or better - sigaction    child = fork();    if (child == -1) { exit(EXIT_FAILURE);}    if (child == 0) { /* I am the child!*/      // Do background stuff e.g. call exec       } else { /* I'm the parent! */      sleep(4); // so we can see the cleanup      puts(\"Parent is done\");    }    return 0;  } The above example however misses a couple of subtle points.\\item More than one child may have finished but the parent will only get one SIGCHLD signal (signals are not queued)\\item SIGCHLD signals can be sent for other reasons (e.g.~a child process is temporarily stopped)\\item It uses the depricated \\keyword{signal} codeA more robust code to reap zombies is shown below.  void cleanup(int signal) {    int status;    while (waitpid((pid_t) (-1), 0, WNOHANG) &gt; 0) {    }  }execTo make the child process execute another program, use one of the exec functions after forking. The exec set of functions replaces the process image with the the process image of what is being called. This means that any lines of code after the exec call are replaced. Any other work you want the child process to do should be done before the exec call. The Wikipedia article does a great job helping you make sense of the names of the exec family. The naming schemes can be shortened mnemonically.\\item e -- An array of pointers to environment variables is explicitly passed to the new process image.\\item l -- Command-line arguments are passed individually (a list) to the function.\\item p -- Uses the PATH environment variable to find the file named in the file argument to be executed.\\item v -- Command-line arguments are passed to the function as an array (vector) of pointers.\\end{quote}An example of this code is below. This code executes \\keyword{ls}\\begin{lstlisting}[language=C]  #include &lt;unistd.h&gt;  #include &lt;sys/types.h&gt;   #include &lt;sys/wait.h&gt;  #include &lt;stdlib.h&gt;  #include &lt;stdio.h&gt;  int main(int argc, char**argv) {    pid_t child = fork();    if (child == -1) return EXIT_FAILURE;    if (child) { /* I have a child! */      int status;      waitpid(child , &amp;status ,0);      return EXIT_SUCCESS;    } else { /* I am the child */      // Other versions of exec pass in arguments as arrays      // Remember first arg is the program name      // Last arg must be a char pointer to NULL      execl(\"/bin/ls\", \"/bin/ls\",\"-alh\", (char *) NULL);      // If we get to this line, something went wrong!      perror(\"exec failed!\");    }  }Try to decode the following example  #include &lt;unistd.h&gt;  #include &lt;fcntl.h&gt; // O_CREAT, O_APPEND etc. defined here  int main() {    close(1); // close standard out    open(\"log.txt\", O_RDWR | O_CREAT | O_APPEND, S_IRUSR | S_IWUSR);    puts(\"Captain's log\");    chdir(\"/usr/include\");    // execl( executable,  arguments for executable including program name and NULL at the end)    execl(\"/bin/ls\", /* Remaining items sent to ls*/ \"/bin/ls\", \".\", (char *) NULL); // \"ls .\"    perror(\"exec failed\");    return 0; // Not expected  }There’s no error checking in the above code (we assume close,open,chdir etc works as expected).\\item \\keyword{open} -- will use the lowest available file descriptor (i.e.~1) ; so standard out now goes to the log file. \\item \\keyword{chdir} -- Change the current directory to /usr/include \\item \\keyword{execl} -- Replace the program image with /bin/ls and call its main() method \\item \\keyword{perror} -- We don't expect to get here - if we did then exec failed.\\end{lstlsting}\\subsection{Deeper Semantics}\\href{https://pubs.opengroup.org/onlinepubs/009695399/functions/exec.html}{Here is the full POSIX standard for Exec}What you need to know is the following bullet points (in addition to above).\\begin{lstlisting}\\item File descriptors are preserved after an exec. That means if you open a file, and you forget to close it, it remains open in the child.This is a problem because usually the child doesn't know about those file descriptors and they take up a slot in the file descriptor table and could possible prevent other processes from accessing the file. The one exception to this is if the file descriptor has the Close Exec flag set (more on how to set that later).\\item Various Signal semantics (don't need to worry about this yet). If you are coming back to do a re-read, exec'ed processes preserve the signal mask and the pending signal set.Shortcutssystem pre-packs the above code. Here is how to use it:  #include &lt;unistd.h&gt;  #include &lt;stdlib.h&gt;  int main(int argc, char**argv) {    system(\"ls\"); // execl(\"/bin/sh\", \"/bin/sh\", \"-c\", \"\\\\\"ls\\\\\"\")    return 0;  }The system call will fork, execute the command passed by parameter and the original parent process will wait for this to finish. This also means that system is a blocking call: The parent process can’t continue until the process started by system exits. Also, system actually creates a shell which is then given the string, which is more overhead than just using exec directly. The standard shell will use the PATH environment variable to search for a filename that matches the command. Using system will usually be sufficient for many simple run-this-command problems but can quickly become limiting for more complex or subtle problems, and it hides the mechanics of the fork-exec-wait pattern so we encourage you to learn and use fork exec and waitpid instead. Not only that, it tends to be a huge security risk. By allowing someone to access a shell version of the environment, you can reach all sorts of problems  int main(int argc, char**argv) {    char *to_exec = asprintf(\"ls %s\", argv[1]);     system(to_exec);  }Passing something along the lines of argv[1] = “; sudo su” could cause all sorts of problems.The fork-exec-wait PatternA common programming pattern is to call fork followed by exec and wait. The original process calls fork, which creates a child process. The child process then uses exec to start execution of a new program. Meanwhile the parent uses wait (or waitpid) to wait for the child process to finish.  #include &lt;unistd.h&gt;  int main() {    pid_t pid = fork();    if (pid &lt; 0) { // fork failure      exit(1);    } else if (pid &gt; 0) { // I am the parent      int status;      waitpid(pid, &amp;status, 0);    } else { // I am the child      execl(\"/bin/ls\", \"/bin/ls\", NULL);      exit(1);    }  }Environment VariablesEnvironment variables are variables that the system keeps for all processes to use. Your system has these set up right now! In Bash, you can check some of these  $ echo $HOME  /home/bhuvy  $ echo $PATH  /usr/local/sbin:/usr/bin:...How would you get these in C/C++? You can use the getenv and setenv function  char* home = getenv(\"HOME\"); // Will return /home/bhuvy  setenv(\"HOME\", \"/home/bhuvan\", 1 /*set overwrite to true*/ );Environment variables are important because they are inherited between processes and can be used the specify A standard set of behaviors Another security related concern is that environment variables cannot be read by an outside process wheras argv can be.Further ReadingRead the man pages!      fork        exec        wait  Topics      Correct use of fork, exec and waitpid        Using exec with a path        Understanding what fork and exec and waitpid do. E.g. how to use their return values.        SIGKILL vs SIGSTOP vs SIGINT.        What signal is sent when you press CTRL-C        Using kill from the shell or the kill POSIX call.        Process memory isolation.        Process memory layout (where is the heap, stack etc; invalid memory addresses).        What is a fork bomb, zombie and orphan? How to create/remove them.        getpid vs getppid        How to use the WAIT exit status macros WIFEXITED etc.  Questions/Exercises      What is the difference between execs with a p and without a p? What does the operating system        How do you pass in command line arguments to execl*? How about execv*? What should be the first command line argument by convention?        How do you know if exec or fork failed?        What is the int status pointer passed into wait? When does wait fail?        What are some differences between SIGKILL, SIGSTOP, SIGCONT, SIGINT? What are the default behaviors? Which ones can you set up a signal handler for?        What signal is sent when you press CTRL-C?        My terminal is anchored to PID = 1337 and has just become unresponsive. Write me the terminal command and the C code to send SIGQUIT to it.        Can one process alter another processes memory through normal means? Why?        Where is the heap, stack, data, and text segment? Which segments can you write to? What are invalid memory addresses?        Code me up a fork bomb in C (please don’t run it).        What is an orphan? How does it become a zombie? How do I be a good parent?        Don’t you hate it when your parents tell you that you can’t do something? Write me a program that sends SIGSTOP to your parent.        Write a function that fork exec waits an executable, and using the wait macros tells me if the process exited normally or if it was signaled. If the process exited normally, then print that with the return value. If not, then print the signal number that caused the process to terminate.  "
  },{
    "title": "Review",
    "url": " /coursebook/Review",
   "content": "  Review          C                  Memory and Strings          Q1.1          Q 1.2          Q 1.3          Q 1.4          Q 1.4          Q 1.5          Q 1.6          Q 1.7          Q 1.8          Q 1.9          Q 1.10          Printing          Q 2.1          Formatting and Printing to a file          Q 3.1          Printing to a string          Q 4.1          Input parsing          Q 5.1          Q 5.3          Heap memory                    Threading                  Q1          Q2          Q3          Q4          Q5          Q6          Q7                    Deadlock                  Q1          Q2          Q3          Q4          Q5                    IPC                  Q1          Q2          Q3          Q4                    Processes                  Q1          Q2          Q3 (Advanced)                    Mapped Memory                  Q1          Q2          Q3          Q4          Q5                    Networking                  Q1          Q2          Q3          Q4          Q5          Q6          Q7          Q8          Q9          Q10          Q11          Q12          Q13          Q14          Q15          Q16          Q17          Q18          Q19          Q20          Q21          Q22          Q23          Q24          Q25          Q26          Q 2.1          Q 2.2          Q 2.3                    Signals                  Give the names of two signals that are normally generated by the kernel          Give the name of a signal that can not be caught by a signal          Why is it unsafe to call any function (something that it is not signal handler safe) in a signal handler?                    [1][]  ReviewA non-comprehensive list of topics is below.2CSP (critical section problems)HTTPSIGINTTCPTLBVirtual Memoryarraysbarrierc stringschmodclient/servercoffman conditionscondition variablescontext switchdeadlockdining philosophersepollexitfile I/Ofile system representationfork/exec/waitfprintffreeheap allocatorheap/stackinode vs namemallocmkfifommapmutexesnetwork portsopen/closeoperating system termspage faultpage tablespipespointer arithmeticpointersprinting (printf)producer/consumerprogress/mutexrace conditionsread/writereader/writerresource allocation graphsring bufferscanf bufferingschedulingselectsemaphoressignalssizeofstatstderr/stdoutsymlinksthread control (_create, _join, _exit)variable initializersvariable scopevm thrashingwait macroswrite/read with errno, EINTR and partial dataCMemory and StringsQ1.1In the example below, which variables are guaranteed to print the value of zero?int a;static int b;void func() {   static int c;   int d;   printf(\"%d %d %d %d\\n\",a,b,c,d);}Q 1.2In the example below, which variables are guaranteed to print the value of zero?void func() {   int* ptr1 = malloc( sizeof(int) );   int* ptr2 = realloc(NULL, sizeof(int) );   int* ptr3 = calloc( 1, sizeof(int) );   int* ptr4 = calloc( sizeof(int) , 1);      printf(\"%d %d %d %d\\n\",*ptr1,*ptr2,*ptr3,*ptr4);}Q 1.3Explain the error in the following attempt to copy a string.char* copy(char*src) { char*result = malloc( strlen(src) );  strcpy(result, src);  return result;}Q 1.4Why does the following attempt to copy a string sometimes work and sometimes fail?char* copy(char*src) { char*result = malloc( strlen(src) +1 );  strcat(result, src);  return result;}Q 1.4Explain the two errors in the following code that attempts to copy a string.char* copy(char*src) { char result[sizeof(src)];  strcpy(result, src);  return result;}Q 1.5Which of the following is legal?char a[] = \"Hello\"; strcpy(a, \"World\");char b[] = \"Hello\"; strcpy(b, \"World12345\", b);char* c = \"Hello\"; strcpy(c, \"World\");Q 1.6Complete the function pointer typedef to declare a pointer to a function that takes a void* argument and returns a void*. Name your type ‘pthread_callback’typedef ______________________;Q 1.7In addition to the function arguments what else is stored on a thread’s stack?Q 1.8Implement a version of char* strcat(char*dest, const char*src) using only strcpy strlen and pointer arithmeticchar* mystrcat(char*dest, const char*src) {  ? Use strcpy strlen here  return dest;}Q 1.9Implement version of size_t strlen(const char*) using a loop and no function calls.size_t mystrlen(const char*s) {}Q 1.10Identify the three bugs in the following implementation of strcpy.char* strcpy(const char* dest, const char* src) {  while(*src) { *dest++ = *src++; }  return dest;}PrintingQ 2.1Spot the two errors!fprintf(\"You scored 100%\");Formatting and Printing to a fileQ 3.1Complete the following code to print to a file. Print the name, a comma and the score to the file ‘result.txt’char* name = .....;int score = ......FILE *f = fopen(\"result.txt\",_____);if(f) {    _____}fclose(f);Printing to a stringQ 4.1How would you print the values of variables a,mesg,val and ptr to a string? Print a as an integer, mesg as C string, val as a double val and ptr as a hexadecimal pointer. You may assume the mesg points to a short C string(&lt;50 characters). Bonus: How would you make this code more robust or able to cope with?char* toString(int a, char*mesg, double val, void* ptr) {   char* result = malloc( strlen(mesg) + 50);    _____   return result;}Input parsingQ 5.1Why should you check the return value of sscanf and scanf? ## Q 5.2 Why is ‘gets’ dangerous?Q 5.3Write a complete program that uses getline. Ensure your program has no memory leaks.Heap memoryWhen would you use calloc not malloc? When would realloc be useful?(Todo - move this question to another page) What mistake did the programmer make in the following code? Is it possible to fix it i) using heap memory? ii) using global (static) memory?static int id;char* next_ticket() {  id ++;  char result[20];  sprintf(result,\"%d\",id);  return result;}ThreadingQ1Is the following code thread-safe? Redesign the following code to be thread-safe. Hint: A mutex is unnecessary if the message memory is unique to each call.static char message[20];pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;void *format(int v) {  pthread_mutex_lock(&amp;mutex);  sprintf(message, \":%d:\" ,v);  pthread_mutex_unlock(&amp;mutex);  return message;}Q2Which one of the following does not cause a process to exit? * Returning from the pthread’s starting function in the last running thread. * The original thread returning from main. * Any thread causing a segmentation fault. * Any thread calling exit. * Calling pthread_exit in the main thread with other threads still running.Q3Write a mathematical expression for the number of “W” characters that will be printed by the following program. Assume a,b,c,d are small positive integers. Your answer may use a ‘min’ function that returns its lowest valued argument.unsigned int a=...,b=...,c=...,d=...;void* func(void* ptr) {  char m = * (char*)ptr;  if(m == 'P') sem_post(s);  if(m == 'W') sem_wait(s);  putchar(m);  return NULL;}int main(int argv, char** argc) {  sem_init(s,0, a);  while(b--) pthread_create(&amp;tid, NULL, func, \"W\");   while(c--) pthread_create(&amp;tid, NULL, func, \"P\");   while(d--) pthread_create(&amp;tid, NULL, func, \"W\");   pthread_exit(NULL);   /*Process will finish when all threads have exited */}Q4Complete the following code. The following code is supposed to print alternating A and B. It represents two threads that take turns to execute. Add condition variable calls to func so that the waiting thread does not need to continually check the turn variable. Q: Is pthread_cond_broadcast necessary or is pthread_cond_signal sufficient?pthread_cond_t cv = PTHREAD_COND_INITIALIZER;pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;void* turn;void* func(void* mesg) {  while(1) {// Add mutex lock and condition variable calls ...    while(turn == mesg) {         /* poll again ... Change me - This busy loop burns CPU time! */     }    /* Do stuff on this thread */    puts( (char*) mesg);    turn = mesg;      }  return 0;}int main(int argc, char** argv){  pthread_t tid1;  pthread_create(&amp;tid1, NULL, func, \"A\");  func(\"B\"); // no need to create another thread - just use the main thread  return 0;}Q5Identify the critical sections in the given code. Add mutex locking to make the code thread safe. Add condition variable calls so that total never becomes negative or above 1000. Instead the call should block until it is safe to proceed. Explain why pthread_cond_broadcast is necessary.int total;void add(int value) { if(value &lt; 1) return; total += value;}void sub(int value) { if(value &lt; 1) return; total -= value;}Q6A non-threadsafe data structure has size() enq and deq methods. Use condition variable and mutex lock to complete the thread-safe, blocking versions.void enqueue(void* data) {  // should block if the size() would become greater than 256  enq(data);}void* dequeue() {  // should block if size() is 0  return deq();}Q7Your startup offers path planning using latest traffic information. Your overpaid intern has created a non-threadsafe data structure with two functions: shortest (which uses but does not modify the graph) and set_edge (which modifies the graph).graph_t* create_graph(char* filename); // called once// returns a new heap object that is the shortest path from vertex i to jpath_t* shortest(graph_t* graph, int i, int j); // updates edge from vertex i to jvoid set_edge(graph_t* graph, int i, int j, double time);   For performance, multiple threads must be able to call shortest at the same time but the graph can only be modified by one thread when no threads other are executing inside shortest or set_edge.Use mutex lock and condition variables to implement a reader-writer solution. An incomplete attempt is shown below. Though this attempt is threadsafe (thus sufficient for demo day!), it does not allow multiple threads to calculate shortest path at the same time and will not have sufficient throughput.path_t* shortest_safe(graph_t* graph, int i, int j) {  pthread_mutex_lock(&amp;m);  path_t* path = shortest(graph, i, j);  pthread_mutex_unlock(&amp;m);  return path;}void set_edge_safe(graph_t* graph, int i, int j, double dist) {  pthread_mutex_lock(&amp;m);  set_edge(graph, i, j, dist);  pthread_mutex_unlock(&amp;m);}  Note thread-programming synchronization problems are on a separate wiki page. This page focuses on conceptual topics. Question numbers subject to changeDeadlockQ1What do each of the Coffman conditions mean? (e.g. can you provide a definition of each one) * Hold and wait * Circular wait * No pre-emption * Mutual exclusionQ2Give a real life example of breaking each Coffman condition in turn. A situation to consider: Painters, paint and paint brushes. Hold and wait Circular wait No pre-emption Mutual exclusionQ3Identify when Dining Philosophers code causes a deadlock (or not). For example, if you saw the following code snippet which Coffman condition is not satisfied?// Get both locks or none.pthread_mutex_lock( a );if( pthread_mutex_trylock( b ) ) { /*failed*/   pthread_mutex_unlock( a );   ...}Q4How many processes are blocked?      P1 acquires R1        P2 acquires R2        P1 acquires R3        P2 waits for R3        P3 acquires R5        P1 acquires R4        P3 waits for R1        P4 waits for R5        P5 waits for R1  Q5How many of the following statements are true for the reader-writer problem?      There can be multiple active readers        There can be multiple active writers        When there is an active writer the number of active readers must be zero        If there is an active reader the number of active writers must be zero        A writer must wait until the current active readers have finished  IPCQ1What are the following and what is their purpose? * Translation Lookaside Buffer * Physical Address * Memory Management Unit * The dirty bitQ2How do you determine how many bits are used in the page offset?Q320 ms after a context switch the TLB contains all logical addresses used by your numerical code which performs main memory access 100% of the time. What is the overhead (slowdown) of a two-level page table compared to a single-level page table?Q4Explain why the TLB must be flushed when a context switch occurs (i.e. the CPU is assigned to work on a different process).  Question numbers subject to changeProcessesQ1Fill in the blanks to make the following program print 123456789. If cat is given no arguments it simply prints its input until EOF. Bonus: Explain why the close call below is necessary.int main() {  int i = 0;  while(++i &lt; 10) {    pid_t pid = fork();    if(pid == 0) { /* child */      char buffer[16];      sprintf(buffer, ______,i);      int fds[ ______];      pipe( fds);      write( fds[1], ______,______ ); // Write the buffer into the pipe      close(  ______ );      dup2( fds[0],  ______);      execlp( \"cat\", \"cat\",  ______ );      perror(\"exec\"); exit(1);    }    waitpid(pid, NULL, 0);  }  return 0;}Q2Use POSIX calls fork pipe dup2 and close to implement an autograding program. Capture the standard output of a child process into a pipe. The child process should exec the program ./test with no additional arguments (other than the process name). In the parent process read from the pipe: Exit the parent process as soon as the captured output contains the ! character. Before exiting the parent process send SIGKILL to the child process. Exit 0 if the output contained a !. Otherwise if the child process exits causing the pipe write end to be closed, then exit with a value of 1. Be sure to close the unused ends of the pipe in the parent and child processQ3 (Advanced)This advanced challenge uses pipes to get an “AI player” to play itself until the game is complete. The program tictactoe accepts a line of input - the sequence of turns made so far, prints the same sequence followed by another turn, and then exits. A turn is specified using two characters. For example “A1” and “C3” are two opposite corner positions. The string B2A1A3 is a game of 3 turns/plys. A valid response is B2A1A3C1 (the C1 response blocks the diagonal B2 A3 threat). The output line may also include a suffix -I win -You win -invalid or -draw Use pipes to control the input and output of each child process created. When the output contains a -, print the final output line (the entire game sequence and the result) and exit.Mapped MemoryQ1Write a function that uses fseek and ftell to replace the middle character of a file with an ‘X’void xout(char* filename) {  FILE *f = fopen(filename, ____ );  }Q2In an ext2 filesystem how many inodes are read from disk to access the first byte of the file /dir1/subdirA/notes.txt ? Assume the directory names and inode numbers in the root directory (but not the inodes themselves) are already in memory.Q3In an ext2 filesystem what is the minimum number of disk blocks that must be read from disk to access the first byte of the file /dir1/subdirA/notes.txt ? Assume the directory names and inode numbers in the root directory and all inodes are already in memory.Q4In an ext2 filesystem with 32 bit addresses and 4KB disk blocks, an inodes that can store 10 direct disk block numbers. What is the minimum file size required to require an single indirection table? ii) a double direction table?Q5Fix the shell command chmod below to set the permission of a file secret.txt so that the owner can read,write,and execute permissions the group can read and everyone else has no access.chmod 000 secret.txt      Wiki w/Interactive MC Questions        See        See  NetworkingQ1What is a socket?Q2What is special about listening on port 1000 vs port 2000?      Port 2000 is twice as slow as port 1000        Port 2000 is twice as fast as port 1000        Port 1000 requires root privileges        Nothing  Q3Describe one significant difference between IPv4 and IPv6Q4When and why would you use ntohs?Q5If a host address is 32 bits which IP scheme am I most likely using? 128 bits?Q6Which common network protocol is packet based and may not successfully deliver the data?Q7Which common protocol is stream-based and will resend data if packets are lost?Q8What is the SYN ACK ACK-SYN handshake?Q9Which one of the following is NOT a feature of TCP? - Packet re-ordering - Flow control - Packet re-tranmission - Simple error detection - EncryptionQ10What protocol uses sequence numbers? What is their initial value? And why?Q11What are the minimum network calls are required to build a TCP server? What is their correct order?Q12What are the minimum network calls are required to build a TCP client? What is their correct order?Q13When would you call bind on a TCP client?Q14What is the purpose of socket bind listen accept ?Q15Which of the above calls can block, waiting for a new client to connect?Q16What is DNS? What does it do for you? Which of the CS241 network calls will use it for you?Q17For getaddrinfo, how do you specify a server socket?Q18Why may getaddrinfo generate network packets?Q19Which network call specifies the size of the allowed backlog?Q20Which network call returns a new file descriptor?Q21When are passive sockets used?Q22When is epoll a better choice than select? When is select a better choice than epoll?Q23Will write(fd, data, 5000) always send 5000 bytes of data? When can it fail?Q24How does Network Address Translation (NAT) work?Q25@MCQ Assuming a network has a 20ms Transmit Time between Client and Server, how much time would it take to establish a TCP Connection? 20 ms 40 ms 100 ms 60 ms @ANS 3 Way Handshake @EXP @ENDQ26What are some of the differences between HTTP 1.0 and HTTP 1.1? How many ms will it take to transmit 3 files from server to client if the network has a 20ms transmit time? How does the time taken differ between HTTP 1.0 and HTTP 1.1?Q 2.1Writing to a network socket may not send all of the bytes and may be interrupted due to a signal. Check the return value of write to implement write_all that will repeatedly call write with any remaining data. If write returns -1 then immediately return -1 unless the errno is EINTR - in which case repeat the last write attempt. You will need to use pointer arithmetic.// Returns -1 if write fails (unless EINTR in which case it recalls write// Repeated calls write until all of the buffer is written.ssize_t write_all(int fd, const char *buf, size_t nbyte) {  ssize_t nb = write(fd, buf, nbyte);  return nb;}Q 2.2Implement a multithreaded TCP server that listens on port 2000. Each thread should read 128 bytes from the client file descriptor and echo it back to the client, before closing the connection and ending the thread.Q 2.3Implement a UDP server that listens on port 2000. Reserve a buffer of 200 bytes. Listen for an arriving packet. Valid packets are 200 bytes or less and start with four bytes 0x65 0x66 0x67 0x68. Ignore invalid packets. For valid packets add the value of the fifth byte as an unsigned value to a running total and print the total so far. If the running total is greater than 255 then exit.SignalsGive the names of two signals that are normally generated by the kernelGive the name of a signal that can not be caught by a signalWhy is it unsafe to call any function (something that it is not signal handler safe) in a signal handler?Coding QuestionsWrite brief code that uses SIGACTION and a SIGNALSET to create a SIGALRM handler."
  },{
    "title": "Scheduling",
    "url": " /coursebook/Scheduling",
   "content": "  Scheduling          Measurements                  What is preemption?          Which schedulers suffer from starvation?          Why might a process (or thread) be placed on the ready queue?                    Measures of Efficiency                  What is the Convoy Effect?          Extra: Linux Scheduling                    Scheduling Algorithms                  Shortest Job First (SJF)          Preemptive Shortest Job First (PSJF)          First Come First Served (FCFS)          Round Robin (RR)          Priority                    Topics      Questions      [1][]  SchedulingCPU Scheduling is the problem of efficiently selecting which process to run on a system’s CPU cores. In a busy system, there will be more ready-to-run processes than there are CPU cores, so the system kernel must evaluate which processes should be scheduled to run and which processes should be executed later. The system must also decide whether or not it should take a particular process and pause its execution – along with any associated threads. The balance comes from stopping processes often enough where you have a responsive computer but infrequently enough where the programs themselves are not spending a lot of cycles context switching. It is a hard balance it get right.The additional complexity of multi-threaded and multiple CPU cores are considered a distraction to this initial exposition so are ignored here. Another gotcha for non-native speakers is the dual meanings of “Time”: The word “Time” can be used in both clock and elapsed duration context. For example “The arrival time of the first process was 9:00am.” and, “The running time of the algorithm is 3 seconds”.One clarification that we will make is that our scheduling will mainly deal with short term or cpu scheduling. That means we will assume that the processes are in memory and ready to go. The other types of scheduling are long and medium term. Long term schedulers act as gatekeepers to the processing world. When a process requests another process to be executed, it can either tell tell the process yes, no, or wait. The medium term scheduler deals with the caveats of moving a process from the paused state in memory to the paused state on disk when there are too many processes or some process are known not to be used for a significant amount of CPU cycles (think about a process that only checks something once an hour).MeasurementsScheduling effects the performance of the system, specifically the latency and throughput of the system. The throughput might be measured by a system value, for example the I/O throughput - the number of bits written per second, or number of small processes that can complete per unit time. The latency might be measured by the response time – elapse time before a process can start to send a response – or wait time or turnaround time –the elapsed time to complete a task. Different schedulers offer different optimization trade-offs that may or may not be appropriate to desired use. There is no optimal scheduler for all possible environments and goals. For example ‘shortest-job-first’ will minimize total wait time across all jobs but in interactive (UI) environments it would be preferable to minimize response time at the expense of some throughput, while FCFS seems intuitively fair and easy to implement but suffers from the Convoy Effect. Arrival time is the time at which a process first arrives at the ready queue, and is ready to start executing. If a CPU is idle, the arrival time would also be the starting time of execution.What is preemption?Without preemption processes will run until they are unable to utilize the CPU any further. For example the following conditions would remove a process from the CPU and the CPU would be available to be scheduled for other processes: The process terminates due to a signal, is blocked waiting for concurrency primitive, or exits normally. Thus once a process is scheduled it will continue even if another process with a high priority (e.g. shorter job) appears on the ready queue.With preemption, the existing processes may be removed immediately if a more preferable process is added to the ready queue. For example, suppose at t=0 with a Shortest Job First scheduler there are two processes (P1 P2) with 10 and 20 ms execution times. P1 is scheduled. P1 immediately creates a new process P3, with execution time of 5 ms, which is added to the ready queue. Without preemption, P3 will run 10ms later (after P1 has completed). With preemption, P1 will be immediately evicted from the CPU and instead placed back in the ready queue, and P3 will be executed instead by the CPU.Which schedulers suffer from starvation?Any scheduler that uses a form of prioritization can result in starvation because earlier processes may never be scheduled to run (assigned a CPU). For example with SJF, longer jobs may never be scheduled if the system continues to have many short jobs to schedule. It all depends on the type of scheduler.Why might a process (or thread) be placed on the ready queue?A process is placed on the ready queue when it is able to use a CPU. Some examples include:      A process was blocked waiting for a read from storage or socket to complete and data is now available.        A new process has been created and is ready to start.        A process thread was blocked on a synchronization primitive (condition variable, semaphore, mutex lock) but is now able to continue.        A process is blocked waiting for a system call to complete but a signal has been delivered and the signal handler needs to run.  Similar examples can be generated when considering threads.Measures of EfficiencyFirst some definitions      start_time is the wall-clock start time of the process (CPU starts working on it)        end_time is the end wall-clock of the process (CPU finishes the process)        run_time is the total amount of CPU time required        arrival_time is the time the process enters the scheduler (CPU may not start working on it)  Here are measures of efficiency      Turnaround Time is the total time from when you the process arrives to when it ends. end_time - arrival_time        Response Time is the total latency (time) that it takes from when the process arrives to when the CPU actually starts working on it. start_time - arrival_time        Wait Time is the total wait time i.e. the total time that a process is on the ready queue. A common mistake is to believe it is only the initial waiting time in the ready queue. If a CPU intensive process with no I/O takes 7 minutes of CPU time to complete but required 9 minutes of wall-clock time to complete we can conclude that it was placed on the ready-queue for 2 minutes. For those 2 minutes the process was ready to run but had no CPU assigned. It does not matter when the job was waiting, the wait time is 2 minutes. end_time - arrival_time - run_time  What is the Convoy Effect?“The Convoy Effect is where I/O intensive processes are continually backed up, waiting for CPU-intensive processes that hog the CPU. This results in poor I/O performance, even for processes that have tiny CPU needs.”Suppose the CPU is currently assigned to a CPU intensive task and there is a set of I/O intensive processes that are in the ready queue. These processes require just a tiny amount of CPU time but they are unable to proceed because they are waiting for the CPU-intensive task to be removed from the processor. These processes are starved until the the CPU bound process releases the CPU. But the CPU will rarely be released (for example in the case of a FCFS scheduler, we must wait until the processes is blocked due to an I/O request). The I/O intensive processes can now finally satisfy their CPU needs, which they can do quickly because their CPU needs are small and the CPU is assigned back to the CPU-intensive process again. Thus the I/O performance of the whole system suffers through an indirect effect of starvation of CPU needs of all processes.This effect is usually discussed in the context of FCFS scheduler, however a round robin scheduler can also exhibit the Convoy effect for long time-quanta.Extra: Linux SchedulingAs of February 2016, Linux by default uses the Completely Fair Scheduler for CPU scheduling and the Budget Fair Scheduling “BFQ” for I/O scheduling. Appropriate scheduling can have a significant impact on throughput and latency. Latency is particularly important for interactive and soft-real time applications such as audio and video streaming. See the discussion and comparative benchmarks here for more information.Here is how the CFS schedules      The CPU creates a Red-Black tree with the processes virtual runtime (runtime / nice_value) and sleeper fairness flag (if the process is waiting on something give it the CPU when it is done waiting).        (Nice values are the kernel’s way of giving priority to certain processes, the lower nice value the higher priority)        The kernel chooses the lowest one based on this metric and schedules that process to run next, taking it off the queue. Since the red-black tree is self balancing this operation is guaranteed (O(log(n))) (selecting the min process is the same runtime)  Although it is called the Fair Scheduler there are a fair bit of problems.      Groups of processes that are scheduled may have imbalanced loads so the scheduler roughly distributes the load. When another CPU gets free it can only look at the average load of a group schedule not the individual cores. So the free CPU may not take the work from a CPU that is burning so long as the average is fine.        If a group of processes is running on non-adjacent cores then there is a bug. If the two cores are more than a hop away, the load balancing algorithm won’t even consider that core. Meaning if a CPU is free and a CPU that is doing more work is more than a hop away, it won’t take the work (may have been patched).        After a thread goes to sleep on a subset of cores, when it wakes up it can only be scheduled on the cores that it was sleeping on. If those cores are now busy, the thread will have to wait on them, wasting opportunities to use other idle cores.        To read more on the problems of the Fair Scheduler, read here.  Scheduling AlgorithmsFor all the examples,Process 1: Runtime 1000msProcess 2: Runtime 2000msProcess 3: Runtime 3000msProcess 4: Runtime 4000msProcess 5: Runtime 5000msShortest Job First (SJF)      P1 Arrival: 0ms        P2 Arrival: 0ms        P3 Arrival: 0ms        P4 Arrival: 0ms        P5 Arrival: 0ms  The processes all arrive at the start and the scheduler schedules the job with the shortest total CPU time. The glaring problem is that this scheduler needs to know how long this program will run over time before it ran the program.Technical Note: A realistic SJF implementation would not use the total execution time of the process but the burst time (the total CPU time including future computational execution before the process will no longer be ready to run). The expected burst time can be estimated by using an exponentially decaying weighted rolling average based on the previous burst time but for this exposition we will simplify this discussion to use the total running time of the process as a proxy for the burst time.Advantages  Shorter jobs tend to get run firstDisadvantages  Needs algorithm to be omniscientPreemptive Shortest Job First (PSJF)Preemptive shortest job first is like shortest job first but if a new job comes in with a shorter runtime than the total runtime of the current job, it is run instead. (If it is equal like our example our algorithm can choose). The scheduler uses the total runtime of the process. If you want the shortest remaining time left, that is a variant of PSJF called Shortest Remaining Time First (SRTF).      P2 at 0ms        P1 at 1000ms        P5 at 3000ms        P4 at 4000ms        P3 at 5000ms  Here’s what our algorithm does. It runs P2 because it is the only thing to run. Then P1 comes in at 1000ms, P2 runs for 2000ms, so our scheduler preemptively stops P2, and let’s P1 run all the way through (this is completely up to the algorithm because the times are equal). Then, P5 Comes in – since there are no processes running, the scheduler will run process 5. P4 comes in, and since the runtimes are equal P5, the scheduler stops P5 and runs P4. Finally P3 comes in, preempts P4, and runs to completion. Then P4 runs, then P5 runs.Advantages  Ensures shorter jobs get run firstDisadvantages  Need to know the runtime againFirst Come First Served (FCFS)      P2 at 0ms        P1 at 1000ms        P5 at 3000ms        P4 at 4000ms        P3 at 5000ms  Processes are scheduled in the order of arrival. One advantage of FCFS is that scheduling algorithm is simple: the ready queue is a just a FIFO (first in first out) queue. FCFS suffers from the Convoy effect. Here P2 Arrives, then P1 arrives, then P5, then P4, then P3. You can see the convoy effect for P5.Advantages      Simple algorithm and implementation        Context switches infrequent when there are long running processes        No starvation if all processes are guarenteed to terminate  Disadvantages      Simple algorithm and implementation        Context switches infrequent when there are long running processes  Round Robin (RR)Processes are scheduled in order of their arrival in the ready queue. However after a small time step a running process will be forcibly removed from the running state and placed back on the ready queue. This ensures that a long-running process can not starve all other processes from running. The maximum amount of time that a process can execute before being returned to the ready queue is called the time quanta. In the limit of large time quanta (where the time quanta is longer than the running time of all processes) round robin will be equivalent to FCFS.      P1 Arrival: 0ms        P2 Arrival: 0ms        P3 Arrival: 0ms        P4 Arrival: 0ms        P5 Arrival: 0ms  Quantum = 1000msHere all processes arrive at the same time. P1 is run for 1 quantum and is finished. P2 for one quantum; then, it is stopped for P3. After all other processes run for a quantum we cycle back to P2 until all the processes are finished.Advantages  Ensures some notion of fairnessDisadvantages  Large number of processes = Lots of switchingPriorityProcesses are scheduled in the order of priority value. For example, a navigation process might be more important to execute than a logging process.Topics      Scheduling Algorithms        Measures of Efficiency  Questions      What is scheduling?        What is turnaround time? Response Time? Wait time?        What is the convoy effect?        Which algorithms have the best turnaround/response/wait time on average  "
  },{
    "title": "Signals",
    "url": " /coursebook/Signals",
   "content": "  Signals          The Deep Dive of Signals      Sending Signals      Handling Signals                  Sigaction          Sigwait                    Signal Disposition      Disposition in Child Processes (No Threads)      Signals in a multithreaded program      Topics      Questions      [1][]  SignalsSignals have been a unix construct since the beginning. They are a convenient way to deliver low-priority information and for users to interact with their programs when no other form of interaction is available like using standard input. Signals allow a program to cleanup or perform an action in the case of an event. Some time, a program can choose to ignore events and that is completely fine and even supported by the standard. Crafting a program that uses signals well is tricky due ot all the rules with inheritance. As such, signals are usually kept as cleanup or termination measures.This chapter will go over how to first read information from a process that has either exited or been signaled and then it will deep dive into what are signals, how does the kernel deal with a signal, and the various ways processes can handle signals both in a single and multithreaded way.The Deep Dive of SignalsA signal is a construct provided to us by the kernel. It allows one process to asynchronously send an event (think a message) to another process. If that process wants to accept the signal, it can, and then, for most signals, can decide what to do with that signal. Here is a short list (non comprehensive) of signals. The overall process for how a kernel sends a signal as well as common use cases are below.      Before any signals are generated, the kernel sets up the default signal handlers for a process.        If still no signals have arrived, the process can install its own signal handlers. This is simple telling the kernel that when the process gets signal X it should jump to function Y.        Now is the fun part, time to deliver a signal! Signals can come from various places below. The signal is now in what we call the generated state.        As soon as the signal starts to get deliverd by the kernel, it is in the pending state.        The kernel then checks the signals disposition, which in layperson terms is whether the process is willing to accept that signal at this point. If not, then the signal is currently blocked and nothing happens.        If not, and there is no signal handler installed, the kernel executes the default action. Otherwise, the kernel delivers the signal by stopping whatever the process is doing at the current point, and jumping that process to the signal handler. If the program is multithreaded, then the process picks on thread with a signal disposition that can accept the signal and freezes the rest. The signal is now in the delivered phase.        Finally, we consider a signal caught if the process remains in tact after the signal was delivered.              Name      Default Action      Usual Use                  SIGINT      Terminate (Can be caught)      Stop a process nicely              SIGQUIT      Terminate (Can be caught)      Stop a process harshly              SIGSTOP      Stop Process (Cannot be caught)      Suspends a process              SIGCONT      Continues a process      Starts after a stop              SIGKILL      Terminate Process (Cannot be caught)      You want the process gone      Sending SignalsSignals can be genrated multiple ways. The user can send a signal. For example, you are at the terminal, and you send CTRL-C this is rarely the case in operating systems but is included in user programs for convenience. Another way is when a system event happens. For example, if you access a page that you aren’t supposed to, the hardware generates a segfault interrupt which gets intercepted by the kernel. The kernel finds the process that caused this and sends a software interrupt signal SIGSEGV. There are softer kernel events like a child being created or sometimes when the kernel wants to like when it is scheduling processes. Finally, another process can send a message when you execute kill -9 PID, it sends SIGKILL to the process. This could be used in low-stakes communication of events between process. If you are relying on signals to be the driver in your program, you should rethink your application design. There are many drawbacks to using signals for asynchronous communication that is avoided by having a dedicated thread and some form of proper Interprocess Communication.You can temporarily pause a running process by sending it a SIGSTOP signal. If it succeeds it will freeze a process, the process will not be allocated any more CPU time. To allow a process to resume execution send it the SIGCONT signal. For example, Here’s program that slowly prints a dot every second, up to 59 dots.#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;int main() {  printf(\"My pid is %d\\n\", getpid() );  int i = 60;  while(--i) {     write(1, \".\",1);    sleep(1);  }  write(1, \"Done!\",5);  return 0;}We will first start the process in the background (notice the &amp; at the end). Then send it a signal from the shell process by using the kill command.&gt;./program &amp;My pid is 403...&gt;kill -SIGSTOP 403&gt;kill -SIGCONT 403In C, you can send a signal to the child using kill POSIX call,kill(child, SIGUSR1); // Send a user-defined signalkill(child, SIGSTOP); // Stop the child process (the child cannot prevent this)kill(child, SIGTERM); // Terminate the child process (the child can prevent this)kill(child, SIGINT); // Equivalent to CTRL-C (by default closes the process)As we saw above there is also a kill command available in the shell. Another command killall works the exact same way but instead of looking up by PID, it tries to match the name of the process. ps is an important utility that can help you find the pid of a process.# First let's use ps and grep to find the process we want to send a signal to$ ps au | grep myprogramangrave  4409   0.0  0.0  2434892    512 s004  R+    2:42PM   0:00.00 myprogram 1 2 3#Send SIGINT signal to process 4409 (equivalent of `CTRL-C`)$ kill -SIGINT 4409#Send SIGKILL (terminate the process)$ kill -SIGKILL 4409$ kill -9 4409# Use kill all instead$ killall -l firefoxIn order to send a signal to the current, use raise or kill with getpid()raise(int sig); // Send a signal to myself!kill(getpid(), int sig); // Same asFor non-root processes, signals can only be sent to processes of the same user. You cant just SIGKILL my processes! man -s2 kill for more details.Handling SignalsThere are strict limitations on the executable code inside a signal handler. Most library and system calls are not async-signal-safe - they may not be used inside a signal handler because they are not re-entrant safe. Re-entrant safe means that imagine that your function can be frozen at any point and executed again, can you guarentee that your function wouldn’t fail? Let’s take the followingint func(const char *str) {  static char buffer[200];  strncpy(buffer, str, 199); # We finish this line and get recalled  printf(\"%s\\n\", buffer)}      We execute (func(“Hello”))        The string gets copied over to the buffer completely (strcmp(buffer, “Hello”) == 0)        A signal is deliverd and the function state freezes, we also stop accepting any new signals until after the handler (we do this for convenience)        We execute func(World)        Now (strcmp(buffer, “World”) == 0) and the buffer is printed out “World”.        We resume the interrupted function and now print out the buffer once again “World” instead of what the function call originally intended “Hello”  Guarenteeing that your functions are signal handler safe are not as simple as not having shared buffers. You must also think about multithreading and synchronization i.e. what happens when I double lock a mutex? You also have to make sure that each subfunction call is re-entrant safe. Suppose your original program was interrupted while executing the library code of malloc ; the memory structures used by malloc will not be in a consistent state. Calling printf (which uses malloc) as part of the signal handler is unsafe and will result in undefined behavior i.e. it is no longer a useful,predictable program. In practice your program might crash, compute or generate incorrect results or stop functioning (deadlock), depending on exactly what your program was executing when it was interrupted to execute the signal handler code. One common use of signal handlers is to set a boolean flag that is occasionally polled (read) as part of the normal running of the program. For example,int pleaseStop ; // See notes on why \"volatile sig_atomic_t\" is bettervoid handle_sigint(int signal) {  pleaseStop = 1;}int main() {  signal(SIGINT, handle_sigint);  pleaseStop = 0;  while ( ! pleaseStop) {      /* application logic here */    }  /* cleanup code here */}The above code might appear to be correct on paper. However, we need to provide a hint to the compiler and to the CPU core that will execute the main() loop. We need to prevent a compiler optimization: The expression ! pleaseStop appears to be a loop invariant meaning it will be true forever, so can be simplified to true. Secondly, we need to ensure that the value of pleaseStop is not cached using a CPU register and instead always read from and written to main memory. The sig_atomic_t type implies that all the bits of the variable can be read or modified as an atomic operation - a single uninterruptable operation. It is impossible to read a value that is composed of some new bit values and old bit values.By specifying pleaseStop with the correct type volatile sig_atomic_t, we can write portable code where the main loop will be exited after the signal handler returns. The sig_atomic_t type can be as large as an int on most modern platforms but on embedded systems can be as small as a char and only able to represent (-127 to 127) values.volatile sig_atomic_t pleaseStop;Two examples of this pattern can be found in COMP a terminal based 1Hz 4bit computer . Two boolean flags are used. One to mark the delivery of SIGINT (CTRL-C), and gracefully shutdown the program, and the other to mark SIGWINCH signal to detect terminal resize and redraw the entire display.You can also choose a handle pending signals asynchronously or synchronously. Install a signal handler to asynchronously handle signals use sigaction (or, for simple examples, signal ). To synchronously catch a pending signal use sigwait which blocks until a signal is delivered or signalfd which also blocks and provides a file descriptor that can be read() to retrieve pending signals.SigactionYou should use sigaction instead of signal because it has better defined semantics. signal on different operating system does different things which is bad sigaction is more portable and is better defined for threads if need be. To change the signal disposition of a process - i.e. what happens when a signal is delivered to your process - use sigaction You can use system call sigaction to set the current handler for a signal or read the current signal handler for a particular signal.int sigaction(int signum, const struct sigaction *act, struct sigaction *oldact);The sigaction struct includes two callback functions (we will only look at the ‘handler’ version), a signal mask and a flags field -struct sigaction {               void     (*sa_handler)(int);               void     (*sa_sigaction)(int, siginfo_t *, void *);               sigset_t   sa_mask;               int        sa_flags;}; Suppose you installed a signal handler for the alarm signal,signal(SIGALRM, myhandler);The equivalent sigaction code is:struct sigaction sa; sa.sa_handler = myhandler;sigemptyset(&amp;sa.sa_mask);sa.sa_flags = 0; sigaction(SIGALRM, &amp;sa, NULL)However, we typically may also set the mask and the flags field. The mask is a temporary signal mask used during the signal handler execution. The SA_RESTART flag will automatically restart some (but not all) system calls that otherwise would have returned early (with EINTR error). The latter means we can simplify the rest of code somewhat because a restart loop may no longer be required.sigfillset(&amp;sa.sa_mask);sa.sa_flags = SA_RESTART; /* Restart functions if  interrupted by handler */     SigwaitSigwait can be used to read one pending signal at a time. sigwait is used to synchronously wait for signals, rather than handle them in a callback. A typical use of sigwait in a multi-threaded program is shown below. Notice that the thread signal mask is set first (and will be inherited by new threads). This prevents signals from being delivered so they will remain in a pending state until sigwait is called. Also notice the same set sigset_t variable is used by sigwait - except rather than setting the set of blocked signals it is being used as the set of signals that sigwait can catch and return.One advantage of writing a custom signal handling thread (such as the example below) rather than a callback function is that you can now use many more C library and system functions that otherwise could not be safely used in a signal handler because they are not async signal-safe.Based on Sigmask Codestatic sigset_t   signal_mask;  /* signals to block         */int main (int argc, char *argv[]) {    pthread_t sig_thr_id;      /* signal handler thread ID */    sigemptyset (&amp;signal_mask);    sigaddset (&amp;signal_mask, SIGINT);    sigaddset (&amp;signal_mask, SIGTERM);    pthread_sigmask (SIG_BLOCK, &amp;signal_mask, NULL);    /* New threads will inherit this thread's mask */    pthread_create (&amp;sig_thr_id, NULL, signal_thread, NULL);    /* APPLICATION CODE */    ...}void *signal_thread (void *arg) {    int       sig_caught;    /* signal caught       */    /* Use same mask as the set of signals that we'd like to know about! */    sigwait(&amp;signal_mask, &amp;sig_caught);    switch (sig_caught)    {    case SIGINT:     /* process SIGINT  */        ...        break;    case SIGTERM:    /* process SIGTERM */        ...        break;    default:         /* should normally not happen */        fprintf (stderr, \"\\nUnexpected signal %d\\n\", sig_caught);        break;    }}Signal DispositionFor each process, each signal has a disposition which means what action will occur when a signal is delivered to the process. For example, the default disposition SIGINT is to terminate it. The signal disposition can be changed by calling signal() (which is simple but not portable as there are subtle variations in its implementation on different POSIX architectures and also not recommended for multi-threaded programs) or sigaction (discussed later). You can imagine the processes’ disposition to all possible signals as a table of function pointers entries (one for each possible signal).The default disposition for signals can be to ignore the signal, stop the process, continue a stopped process, terminate the process, or terminate the process and also dump a ‘core’ file. Note a core file is a representation of the processes’ memory state that can be inspected using a debugger.Multiple signals connot be queued. However it is possible to have signals that are in a pending state. If a signal is pending, it means it has not yet been delivered to the process. The most common reason for a signal to be pending is that the process (or thread) has currently blocked that particular signal. If a particular signal, e.g. SIGINT, is pending then it is not possible to queue up the same signal again. It is possible to have more than one signal of a different type in a pending state. For example SIGINT and SIGTERM signals may be pending (i.e. not yet delivered to the target process)Signals can be blocked (meaning they will stay in the pending state) by setting the process signal mask or, when you are writing a multi-threaded program, the thread signal mask.Disposition in Child Processes (No Threads)After forking, The child process inherits a copy of the parent’s signal dispositions and a copy of the parent’s signal mask. In other words, if you have installed a SIGINT handler before forking, then the child process will also call the handler if a SIGINT is delivered to the child. Also if SIGINT is blocked in the parent, it will be blocked in the child too. Note pending signals for the child are not inherited during forking. But after exec, both the signal mask and the signal disposition carries over to the exec-ed program . Pending signals are preserved as well. Signal handlers are reset, because the original handler code has disappeared along with the old process.To block signals use sigprocmask! With sigprocmask you can set the new mask, add new signals to be blocked to the process mask, and unblock currently blocked signals. You can also determine the existing mask (and use it for later) by passing in a non-null value for oldset.int sigprocmask(int how, const sigset_t *set, sigset_t *oldset);`From the Linux man page of sigprocmask,SIG_BLOCK: The set of blocked signals is the union of the current set and the   set argument.SIG_UNBLOCK: The signals in set are removed from the current set of blocked   signals. It is permissible to attempt to unblock a signal which is not blocked.SIG_SETMASK: The set of blocked signals is set to the argument set.The sigset type behaves as a bitmap, except functions are used rather than explicitly setting and unsetting bits using &amp; and . It is a common error to forget to initialize the signal set before modifying one bit. For example,sigset_t set, oldset;sigaddset(&amp;set, SIGINT); // Ooops!sigprocmask(SIG_SETMASK, &amp;set, &amp;oldset)Correct code initializes the set to be all on or all off. For example,sigfillset(&amp;set); // all signalssigprocmask(SIG_SETMASK, &amp;set, NULL); // Block all the signals!// (Actually SIGKILL or SIGSTOP cannot be blocked...)sigemptyset(&amp;set); // no signals sigprocmask(SIG_SETMASK, &amp;set, NULL); // set the mask to be empty againSignals in a multithreaded programThe new thread inherits a copy of the calling thread’s mask. On initialization the calling thread’s mask is the exact same as the processes mask because threads are essentially processes. After a new thread is created though, the processes signal mask turns into a gray area. Instead, the kernel likes to threat the process as a collection of thread, each of which can institute a signal mask and receive signals. In order to start setting your mask you can use,pthread_sigmask( ... ); // set my mask to block delivery of some signalspthread_create( ... ); // new thread will start with a copy of the same maskBlocking signals is similar in multi-threaded programs to single-threaded programs: * Use pthread_sigmask instead of sigprocmask * Block a signal in all threads to prevent its asynchronous deliveryThe easiest method to ensure a signal is blocked in all threads is to set the signal mask in the main thread before new threads are createdsigemptyset(&amp;set);sigaddset(&amp;set, SIGQUIT);sigaddset(&amp;set, SIGINT);pthread_sigmask(SIG_BLOCK, &amp;set, NULL);// this thread and the new thread will block SIGQUIT and SIGINTpthread_create(&amp;thread_id, NULL, myfunc, funcparam);Just as we saw with sigprocmask, pthread_sigmask includes a ‘how’ parameter that defines how the signal set is to be used:pthread_sigmask(SIG_SETMASK, &amp;set, NULL) - replace the thread's mask with given signal setpthread_sigmask(SIG_BLOCK, &amp;set, NULL) - add the signal set to the thread's maskpthread_sigmask(SIG_UNBLOCK, &amp;set, NULL) - remove the signal set from the thread's maskA signal then can delivered to any signal thread that is not blocking that signal. If the two or more threads can receive the signal then which thread will be interrupted is arbitrary! A common practice is to have one thread that can receive all signals or if there is a certain signal that requires special logic, have multiple threads for multiple signals. Even though programs from the outside can’t send signals to specific threads (unless a thread is assigned a signal), you can do that in your program with pthread_kill(pthread_t thread, int sig). In the example below, the newly created thread executing func will be interrupted by SIGINTpthread_create(&amp;tid, NULL, func, args);pthread_kill(tid, SIGINT);pthread_kill(pthread_self(), SIGKILL); // send SIGKILL to myselfAs a word of warning pthread_kill(threadid, SIGKILL) will kill the entire process. Though individual threads can set a signal mask, the signal disposition (the table of handlers/action performed for each signal) is per-process not per-thread. This means sigaction can be called from any thread because you will be setting a signal handler for all threads in the process.The linux man pages discusses signal system calls in section 2. There is also a longer article in section 7 (though not in OSX/BSD):man -s7 signalTopics      Signals        Signal Handler Safe        Signal Disposition        Signal States        Pending Signals when Forking/Exec        Signal Disposition when Forking/Exec        Raising Signals in C        Raising Signals in a multithreaded program  Questions      What is a signal?        How are signals served under UNIX? (Bonus: How about Windows?)        What does it mean that a function is signal handler safe        What is a process Signal Disposition?        How do I change the signal disposition in a single threaded program? How about multithreaded?        Why sigaction vs signal?        How do I asynchronously and synchronously catch a signal?        What happens to pending signals after I fork? Exec?        What happens to my signal disposition after I fork? Exec?  "
  },{
    "title": "Synchronization",
    "url": " /coursebook/Synchronization",
   "content": "  Synchronization          Mutex                  Lifetime          Mutex Gotchas          Simplest complete example          Mutex Implementation          Extra: Implementing a Mutex with hardware          Semaphore                    Condition Variables                  Example                    Thread Safe Data Structures                  Using semaphores                    Candidate Solutions                  Turn-based solutions          Desired properties for solutions to the Critical Section Problem?          Turn and Flag solutions                    Working Solutions                  What is Peterson’s solution?          Was Peterson’s solution the first solution?          Extra: Can I just implement Peterson’s (or Dekkers) algorithm in C or assembler?                    Implementing Counting Semaphore                  Other semaphore considerations          How do I wait for N threads to reach a certain point before continuing onto the next step?          What is the Reader Writer Problem?          Attempt #1          Attempt #2:          Attempt #3          Starving writers          Attempt #4                    Ring Buffer                  What are gotchas of implementing a Ring Buffer?          Multithreaded Correctness          Analysis          Correct implementation of a ring buffer          Food for thought                    Extra: Process Synchronization                  Interruption          Solution                    Extra: Higher Order Models of Synchronization                  Sequentially Consistent          Relaxed          Acquire/Relaxed          Consume                    External Resources      Topics      Questions      [1][]  SynchronizationSynchronization are a series of mechanisms to control what threads are allowed to perform what operation at a time. Most of the time, the threads can progress without having to communicate, but every so often two or more threads may want to access a critical section. A critical section is a section of code that can only be executed by one thread at a time, if the program is to function correctly. If two threads (or processes) were to execute code inside the critical section at the same time, it is possible that program may no longer have correct behavior.Something as simple as incrementing a variable could be a critical section. Incrementing a variable (i++) is performed in three individual steps: Copy the memory contents to the CPU register. Increment the value in the CPU. Store the new value in memory. If the memory location is only accessible by one thread (e.g. automatic variable i below) then there is no possibility of a race condition and no Critical Section associated with i. However the sum variable is a global variable and accessed by two threads. It is possible that two threads may attempt to increment the variable at the same time.#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;// Compile with -pthreadint sum = 0; //sharedvoid *countgold(void *param) {    int i; //local to each thread    for (i = 0; i &lt; 10000000; i++) {        sum += 1;    }    return NULL;}int main() {    pthread_t tid1, tid2;    pthread_create(&amp;tid1, NULL, countgold, NULL);    pthread_create(&amp;tid2, NULL, countgold, NULL);        //Wait for both threads to finish:    pthread_join(tid1, NULL);    pthread_join(tid2, NULL);        printf(\"ARRRRG sum is %d\\n\", sum);    return 0;}Typical output of the above code is ARGGGH sum is &lt;some number less than expected&gt; because there is a race condition. The code does not stop two threads from reading-writing sum at the same time. For example, both threads copy the current value of sum into CPU that runs each thread (let’s pick 123). Both threads increment one to their own copy. Both threads write back the value (124). If the threads had accessed the sum at different times then the count would have been 125. A few of the possible different orderings are below.Permittable Pattern            Thread 1      Thread 2                  Load Addr, Add 1 (i=1 locally)      …              Store (i=1 globally)      …              …      Load Addr, Add 1 (i=2 locally)              …      Store (i=2 globally)      Partial Overlap            Thread 1      Thread 2                  Load Addr, Add 1 (i=1 locally)      …              Store (i=1 globally)      Load Addr, Add 1 (i=1 locally)              …      Store (i=1 globally)      Full Overlap            Thread 1      Thread 2                  Load Addr, Add 1 (i=1 locally)      Load Addr, Add 1 (i=1 locally)              Store (i=1 globally)      Store (i=1 globally)      We would like the first pattern of the code being mutually exclusive. Which leads us to our first synchronization primitive, a Mutex.MutexTo ensure only one thread at a time can access a global variable, use a mutex – short for Mutual Exclusion. If one thread is currently inside a critical section we would like another thread to wait until the first thread is complete.pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER; // global variablepthread_mutex_lock(&amp;m); // start of Critical Section// Critical sectionpthread_mutex_unlock(&amp;m); //end of Critical SectionLifetimeThere are a few ways of initializing a mutex. You can use the macro PTHREAD_MUTEX_INITIALIZER only for global (‘static’) variables. m = PTHREAD_MUTEX_INITIALIZER is equivalent to the more general purpose pthread_mutex_init(&amp;m,NULL). The init version includes options to trade performance for additional error-checking and advanced sharing options. You can also call the init function inside of a program for a mutex located on the heap.pthread_mutex_t *lock = malloc(sizeof(pthread_mutex_t)); pthread_mutex_init(lock, NULL);//laterpthread_mutex_destroy(lock);free(lock);Once we are finished with the mutex we should also call pthread_mutex_destroy(&amp;m) too. Note, you can only destroy an unlocked mutex. Calling destroy on a destroyed lock, initializing an initialized lock, locking an already locked lock, unlocking an unlocked lock etc are undefined behavior.Things to keep in mind about init and destroy      Multiple threads init/destroy has undefined behavior        Destroying a locked mutex has undefined behavior        Basically try to keep to the pattern of one thread initializing a mutex and one and only one thread initializing a mutex.        Copying the bytes of the mutex to a new memory location and then using the copy is not supported. To reference a mutex, you have to have a pointer to that memory address.  Mutex GotchasMutexes do not lock variables. A mutex is not that smart - it works with code, not data. If I lock a mutex, the other threads will continue. It’s only when a thread attempts to lock a mutex that is already locked, will the thread have to wait. As soon as the original thread unlocks the mutex, the second (waiting) thread will acquire the lock and be able to continue. The following code creates a mutex that does effectively nothing.int a;pthread_mutex_t m1 = PTHREAD_MUTEX_INITIALIZER,                 m2 = = PTHREAD_MUTEX_INITIALIZER;// later// Thread 1pthread_mutex_lock(&amp;m1);a++;pthread_mutex_unlock(&amp;m1);// Thread 2pthread_mutex_lock(&amp;m2);a++;pthread_mutex_unlock(&amp;m2);You can create mutex before fork-ing - however the child and parent process will not share virtual memory and each one will have a mutex independent of the other. Advanced note: There are advanced options using shared memory that allow a child and parent to share a mutex if it’s created with the correct options and uses a shared memory segment. See stackoverflow exampleAs some other notes, the thread that locks a mutex is the only thread that can unlock it. Each program can have multiple mutex locks, usually one lock per data structure. If you only have one lock, then they may be significant contention for the lock between two threads that was unnecessary. For example if two threads were updating two different counters, it might not be necessary to use the same lock. However, simply creating many locks is insufficient: It’s important to be able to reason about critical sections e.g. it’s important that one thread can’t read two data structures while they are being updated and temporarily in an inconsistent state. There is a small amount of overhead of calling pthread_mutex_lock and pthread_mutex_unlock; however, this is the price you pay for correctly functioning programs!Simplest complete exampleHere is a complete example#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;// Compile with -pthread// Create a mutex this ready to be locked!pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;int sum = 0;void *countgold(void *param) {    int i;        //Same thread that locks the mutex must unlock it    //Critical section is just 'sum += 1'    //However locking and unlocking a million times    //has significant overhead        pthread_mutex_lock(&amp;m);    // Other threads that call lock will have to wait until we call unlock    for (i = 0; i &lt; 10000000; i++) {        sum += 1;    }    pthread_mutex_unlock(&amp;m);    return NULL;}int main() {    pthread_t tid1, tid2;    pthread_create(&amp;tid1, NULL, countgold, NULL);    pthread_create(&amp;tid2, NULL, countgold, NULL);    pthread_join(tid1, NULL);    pthread_join(tid2, NULL);    printf(\"ARRRRG sum is %d\\n\", sum);    return 0;}In the code above, the thread gets the lock to the counting house before entering. The critical section is only the sum+=1 so the following version is also correct.    for (i = 0; i &lt; 10000000; i++) {        pthread_mutex_lock(&amp;m);        sum += 1;        pthread_mutex_unlock(&amp;m);    }    return NULL;}This process runs slower because we lock and unlock the mutex a million times, which is expensive - at least compared with incrementing a variable. In this simple example, we didn’t really need threads - we could have added up twice! A faster multi-thread example would be to add one million using an automatic(local) variable and only then adding it to a shared total after the calculation loop has finished:    int local = 0;    for (i = 0; i &lt; 10000000; i++) {       local += 1;    }    pthread_mutex_lock(&amp;m);    sum += local;    pthread_mutex_unlock(&amp;m);    return NULL;}If I forget to unlock, you get deadlock! We will talk about deadlock a little bit later but what is the problem with this loop if called by multiple threads.while(not_stop){    //stdin may not be thread safe    pthread_mutex_lock(&amp;m);    char *line = getline(...);    if(rand() % 2) { /* randomly skip lines */         continue;    }    pthread_mutex_unlock(&amp;m);        process_line(line);}Other possible problems with synchronization.      Not unlocking a mutex due to an early return during an error condition        Resource leak (not calling pthread_mutex_destroy)        Using an unitialized mutex or using a mutex that has already been destroyed        Locking a mutex twice on a thread without unlocking first        Deadlock  Mutex ImplementationAn incorrect implementation is shown below. The unlock function simply unlocks the mutex and returns. The lock function first checks to see if the lock is already locked. If it is currently locked, it will keep checking again until another thread has unlocked the mutex.// Version 1 (Incorrect!)void lock(mutex_t *m) {  while(m-&gt;locked) { /*Locked? Nevermind - just loop and check again!*/ }  m-&gt;locked = 1;}void unlock(mutex_t *m) {  m-&gt;locked = 0;}Version 1 uses ‘busy-waiting’ (unnecessarily wasting CPU resources) however there is a more serious problem: We have a race-condition! If two threads both called lock concurrently it is possible that both threads would read m_locked as zero. Thus both threads would believe they have exclusive access to the lock and both threads will continue. Ooops!We might attempt to reduce the CPU overhead a little by calling pthread_yield() inside the loop - pthread_yield suggests to the operating system that the thread does not use the CPU for a short while, so the CPU may be assigned to threads that are waiting to run. But does not fix the race-condition. We need a better implementation - can you work how to prevent the race-condition?Extra: Implementing a Mutex with hardwareWe can use C11 Atomics to do that perfectly! A complete solution is detailed here. This is a spinlock mutex, futex implementations can be found online.typedef struct mutex_{    atomic_int_least8_t lock;    pthread_t owner;} mutex;#define UNLOCKED 0#define LOCKED 1#define UNASSIGNED_OWNER 0int mutex_init(mutex* mtx){    if(!mtx){        return 0;    }    atomic_init(&amp;mtx-&gt;lock, UNLOCKED); // Not thread safe the user has to take care of this    mtx-&gt;owner = UNASSIGNED_OWNER;    return 1;}This is the initialization code, nothing fancy here. We set the state of the mutex to unlocked and set the owner to locked.int mutex_lock(mutex* mtx){    int_least8_t zero = UNLOCKED;    while(!atomic_compare_exchange_weak_explicit            (&amp;mtx-&gt;lock,              &amp;zero,              LOCKED,             memory_order_relaxed,             memory_order_relaxed)){        zero = UNLOCKED;        sched_yield(); // Use system calls for scheduling speed    }    // We have the lock now    mtx-&gt;owner = pthread_self();    return 1;}Yikes! What does this code do? Well to start it it initializes a variable that we will keep as the unlocked state. Atomic Compare and Exchange is an instruction supported by most modern architectures (on x86 it’s lock cmpxchg). The pseudocode for this operation looks like thisint atomic_compare_exchange_pseudo(int* addr1, int* addr2, int val){    if(*addr1 == *addr2){        *addr1 = val;        return 1;    }else{        *addr2 = *addr1;        return 0;    }}Except it is all done atomically meaning in one uninterruptible operation. What does the weak part mean? Well atomic instructions are prone to spurious failures meaning that there are two versions to these atomic functions a strong and a weak part, strong guarantees the success or failure while weak may fail. We are using weak because weak is faster, and we are in a loop! That means we are okay if it fails a little bit more often because we will just keep spinning around anyway.Inside the while loop, we have failed to grab the lock! We reset zero to unlocked and sleep for a little while. When we wake up we try to grab the lock again. Once we successfully swap, we are in the critical section! We set the mutex’s owner to the current thread for the unlock method and return successful.How does this guarantee mutual exclusion, when working with atomics we are not entirely sure! But in this simple example we can because the thread that is able to successfully expect the lock to be UNLOCKED (0) and swap it to a LOCKED (1) state is considered the winner. How do we implement unlock?What is this memory order business? We were talking about memory fences earlier, here it is! We won’t go into detail because it is outside the scope of this course but not the scope of this article.int mutex_unlock(mutex* mtx){    if(unlikely(pthread_self() != mtx-&gt;owner)){        return 0; //You can't unlock a mutex if you aren't the owner    }    int_least8_t one = 1;    //Critical section ends after this atomic    mtx-&gt;owner = UNASSIGNED_OWNER;    if(!atomic_compare_exchange_strong_explicit(                &amp;mtx-&gt;lock,                 &amp;one,                 UNLOCKED,                memory_order_relaxed,                memory_order_relaxed)){        //The mutex was never locked in the first place        return 0;    }    return 1;}To satisfy the api, you can’t unlock the mutex unless you are the one who owns it. Then we unassign the mutex owner, because critical section is over after the atomic. We want a strong exchange because we don’t want to block (pthread_mutex_unlock doesn’t block). We expect the mutex to be locked, and we swap it to unlock. If the swap was successful, we unlocked the mutex. If the swap wasn’t, that means that the mutex was UNLOCKED and we tried to switch it from UNLOCKED to UNLOCKED, preserving the non blocking of unlock.SemaphoreUsing a semaphore is as easy as creating a mutex. First decide if the initial value should be zero or some other value (e.g. the number of remaining spaces in an array). Unlike pthread mutex there are not shortcuts to creating a semaphore - use sem_init#include &lt;semaphore.h&gt;sem_t s;int main() {  sem_init(&amp;s, 0, 10); // returns -1 (=FAILED) on OS X  sem_wait(&amp;s); // Could do this 10 times without blocking  sem_post(&amp;s); // Announce that we've finished (and one more resource item is available; increment count)  sem_destroy(&amp;s); // release resources of the semaphore}When using a semaphore, wait and post can be called from different threads! Unlike a mutex, the increment and decrement can be from different threads. This becomes especially useful if you want to use a semaphore to implement a mutex. A mutex is a semaphore that always waits before it posts. Some textbooks will refer to a mutex as a binary semaphore. You do have to be careful to never add more than one to a semaphore or otherwise your mutex abstraction breaks. That is usually why a mutex is used to implement a semaphore and vice versa.      Initialize the semaphore with a count of one.        Replace pthread_mutex_lock with sem_wait        Replace pthread_mutex_unlock with sem_post  sem_t s;sem_init(&amp;s, 0, 1);sem_wait(&amp;s);// Critical Sectionsem_post(&amp;s);Also, keywordsem_post is one of a handful of functions that can be correctly used inside a signal handler (pthread_mutex_unlock is not). This means we can release a waiting thread which can now make all of the calls that we were not allowed to call inside the signal handler itself e.g. printf.#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;#include &lt;signal.h&gt;#include &lt;semaphore.h&gt;#include &lt;unistd.h&gt;sem_t s;void handler(int signal) {    sem_post(&amp;s); /* Release the Kraken! */}void *singsong(void *param) {    sem_wait(&amp;s);    printf(\"I had to wait until your signal released me!\\n\");}int main() {    int ok = sem_init(&amp;s, 0, 0 /* Initial value of zero*/);     if (ok == -1) {       perror(\"Could not create unnamed semaphore\");       return 1;    }    signal(SIGINT, handler); // Too simple! See Signals chapter    pthread_t tid;    pthread_create(&amp;tid, NULL, singsong, NULL);    pthread_exit(NULL); /* Process will exit when there are no more threads */}Condition VariablesCondition variables allow a set of threads to sleep until woken up! You can wake up one thread or all threads that are sleeping. If you only wake one thread then the operating system will decide which thread to wake up. You don’t wake threads directly instead you ‘signal’ the condition variable, which then will wake up one (or all) threads that are sleeping inside the condition variable.Condition variables are also generally used with a mutex and with a loop, so when woken up they have to check a condition in a critical section. If you just need to be woken up not in a critical section, there are other ways to do this in POSIX. Threads sleeping inside a condition variable are woken up by calling pthread_cond_broadcast (wake up all) or pthread_cond_signal (wake up one). Note despite the function name, this has nothing to do with POSIX signals!Occasionally a waiting thread may appear to wake up for no reason (this is called a spurious wake)! This is not an issue because you always use wait inside a loop that tests a condition that must be true to continue.Why do spurious wakeups happen? For performance. On multi-CPU systems it is possible that a race-condition could cause a wake-up (signal) request to be unnoticed. The kernel may not detect this lost wake-up call but can detect when it might occur. To avoid the potential lost signal the thread is woken up so that the program code can test the condition again.ExampleThe call pthread_cond_wait performs three actions:      Unlock the mutex        Sleeps until pthread_cond_signal is called on the same condition variable)        Before returning, locks the mutex  Condition variables are always used with a mutex lock. Before calling wait, the mutex lock must be locked and wait must be wrapped with a loop.pthread_cond_t cv;pthread_mutex_t m;int count;// Initializepthread_cond_init(&amp;cv, NULL);pthread_mutex_init(&amp;m, NULL);count = 0;pthread_mutex_lock(&amp;m);while (count &lt; 10) {   pthread_cond_wait(&amp;cv, &amp;m); /* Remember that cond_wait unlocks the mutex before blocking (waiting)! *//* After unlocking, other threads can claim the mutex. *//* When this thread is later woken it will *//* re-lock the mutex before returning */}pthread_mutex_unlock(&amp;m);//later clean up with pthread_cond_destroy(&amp;cv); and mutex_destroy // In another thread increment count:while (1) {  pthread_mutex_lock(&amp;m);  count++;  pthread_cond_signal(&amp;cv);  /* Even though the other thread is woken up it cannot not return */  /* from pthread_cond_wait until we have unlocked the mutex. This is */  /* a good thing! In fact, it is usually the best practice to call */  /* cond_signal or cond_broadcast before unlocking the mutex */  pthread_mutex_unlock(&amp;m);}Thread Safe Data StructuresTo paraphrase Wikipedia,An operation (or set of operations) is atomic or uninterruptible if it appears to the rest of the system to occur instantaneously. Without locks, only simple CPU instructions (“read this byte from memory”) are atomic (indivisible). On a single CPU system or inside kernels, one could temporarily disable interrupts (so a sequence of operations cannot be interrupted) but in practice atomicity is achieved by using synchronization primitives, typically a mutex lock.Incrementing a variable (i++) is not atomic because it requires three distinct steps: Copying the bit pattern from memory into the CPU; performing a calculation using the CPU’s registers; copying the bit pattern back to memory. During this increment sequence, another thread or process can still read the old value and other writes to the same memory would also be over-written when the increment sequence completes.As such, we can use the tools in the previous section in order to make our data structures thread safe. For the most part, we will be using mutexes because they carry more semantic meaning than a binary semaphore. Note, this is just an introduction - writing high-performance thread-safe data structures requires its own book!// A simple fixed-sized stack (version 1)#define STACK_SIZE 20int count;double values[STACK_SIZE];void push(double v) {     values[count++] = v; }double pop() {    return values[--count];}int is_empty() {    return count == 0;}Version 1 of the stack is not thread-safe because if two threads call push or pop at the same time then the results or the stack can be inconsistent. For example, imagine if two threads call pop at the same time then both threads may read the same value, both may read the original count value.To turn this into a thread-safe data structure we need to identify the critical sections of our code i.e. which section(s) of the code must only have one thread at a time. In the above example the push,pop and is_empty functions access the same variables (i.e. memory) and all critical sections for the stack. While push (and pop) is executing, the datastructure is an inconsistent state (for example the count may not have been written to, so may still contain the original value). By wrapping these methods with a mutex we can ensure that only one thread at a time can update (or read) the stack. A candidate ‘solution’ is shown below. Is it correct? If not, how will it fail?// An attempt at a thread-safe stack (version 2)#define STACK_SIZE 20int count;double values[STACK_SIZE];pthread_mutex_t m1 = PTHREAD_MUTEX_INITIALIZER;pthread_mutex_t m2 = PTHREAD_MUTEX_INITIALIZER;void push(double v) {     pthread_mutex_lock(&amp;m1);    values[count++] = v;    pthread_mutex_unlock(&amp;m1);}double pop() {    pthread_mutex_lock(&amp;m2);    double v = values[--count];    pthread_mutex_unlock(&amp;m2);    return v;}int is_empty() {    pthread_mutex_lock(&amp;m1);    return count == 0;    pthread_mutex_unlock(&amp;m1);}Version 2 contains at least one error. Take a moment to see if you can the error(s) and work out the consequence(s).If three threads called push() at the same time the lock m1 ensures that only one thread at time manipulates the stack (two threads will need to wait until the first thread completes (calls unlock), then a second thread will be allowed to continue into the critical section and finally the third thread will be allowed to continue once the second thread has finished).A similar argument applies to concurrent calls (calls at the same time) to pop. However version 2 does not prevent push and pop from running at the same time because push and pop use two different mutex locks. The fix is simple in this case - use the same mutex lock for both the push and pop functions.The code has a second error; is_empty returns after the comparison and will not unlock the mutex. However the error would not be spotted immediately. For example, suppose one thread calls is_empty and a second thread later calls push. This thread would mysteriously stop. Using debugger you can discover that the thread is stuck at the lock() method inside the push method because the lock was never unlocked by the earlier is_empty call. Thus an oversight in one thread led to problems much later in time in an arbitrary other thread.// An attempt at a thread-safe stack (version 3)int count;double values[count];pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;void push(double v) {   pthread_mutex_lock(&amp;m);   values[count++] = v;  pthread_mutex_unlock(&amp;m);}double pop() {  pthread_mutex_lock(&amp;m);  double v = values[--count];  pthread_mutex_unlock(&amp;m);  return v;}int is_empty() {  pthread_mutex_lock(&amp;m);  int result= count == 0;  pthread_mutex_unlock(&amp;m);  return result;}Version 3 is thread-safe. We have ensured mutual exclusion for all of the critical sections.      is_empty is thread-safe but its result may already be out-of date i.e. the stack may no longer be empty by the time the thread gets the result!        There is no protection against underflow (popping on an empty stack) or overflow (pushing onto an already-full stack)  The latter point can be fixed using counting semaphores. The implementation assumes a single stack. A more general purpose version might include the mutex as part of the memory struct and use pthread_mutex_init to initialize the mutex. For example,// Support for multiple stacks (each one has a mutex)typedef struct stack {    int count;    pthread_mutex_t m;     double *values;} stack_t;stack_t* stack_create(int capacity) {    stack_t *result = malloc(sizeof(stack_t));    result-&gt;count = 0;    result-&gt;values = malloc(sizeof(double) * capacity);    pthread_mutex_init(&amp;result-&gt;m, NULL);    return result;}void stack_destroy(stack_t *s) {    free(s-&gt;values);    pthread_mutex_destroy(&amp;s-&gt;m);    free(s);}// Warning no underflow or overflow checks!void push(stack_t *s, double v) {     pthread_mutex_lock(&amp;s-&gt;m);     s-&gt;values[(s-&gt;count)++] = v;     pthread_mutex_unlock(&amp;s-&gt;m);}double pop(stack_t *s) {     pthread_mutex_lock(&amp;s-&gt;m);     double v = s-&gt;values[--(s-&gt;count)];     pthread_mutex_unlock(&amp;s-&gt;m);     return v;}int is_empty(stack_t *s) {     pthread_mutex_lock(&amp;s-&gt;m);     int result = s-&gt;count == 0;     pthread_mutex_unlock(&amp;s-&gt;m);    return result;}int main() {    stack_t *s1 = stack_create(10 /* Max capacity*/);    stack_t *s2 = stack_create(10);    push(s1, 3.141);    push(s2, pop(s1));    stack_destroy(s2);    stack_destroy(s1);}Using semaphoresWe can also use a counting semaphore to keep track of how many spaces remain and another semaphore to keep to track the number of items in the stack. We will call these two semaphores ‘sremain’ and ‘sitems’. Remember sem_wait will wait if the semaphore’s count has been decremented to zero (by another thread calling sem_post).// Sketch #1sem_t sitems;sem_t sremain;void stack_init(){  sem_init(&amp;sitems, 0, 0);  sem_init(&amp;sremain, 0, 10);}double pop() {  // Wait until there's at least one item  sem_wait(&amp;sitems);  ...void push(double v) {  // Wait until there's at least one space  sem_wait(&amp;sremain);  ...Sketch #2 has implemented the post too early. Another thread waiting in push can erroneously attempt to write into a full stack (and similarly a thread waiting in the pop() is allowed to continue too early).// Sketch #2 (Error!)double pop() {  // Wait until there's at least one item  sem_wait(&amp;sitems);  sem_post(&amp;sremain); // error! wakes up pushing() thread too early  return values[--count];}void push(double v) {  // Wait until there's at least one space  sem_wait(&amp;sremain);  sem_post(&amp;sitems); // error! wakes up a popping() thread too early  values[count++] = v;}Sketch 3 implements the correct semaphore logic but can you spot the error?// Sketch #3 (Error!)double pop() {  // Wait until there's at least one item  sem_wait(&amp;sitems);  double v= values[--count];  sem_post(&amp;sremain);  return v;}void push(double v) {  // Wait until there's at least one space  sem_wait(&amp;sremain);  values[count++] = v;  sem_post(&amp;sitems); }Sketch 3 correctly enforces buffer full and buffer empty conditions using semaphores. However there is no mutual exclusion: Two threads can be in the critical section at the same time, which would corrupt the data structure (or least lead to data loss). The fix is to wrap a mutex around the critical section:// Simple single stack - see above example on how to convert this into a multiple stacks.// Also a robust POSIX implementation would check for EINTR and error codes of sem_wait.// PTHREAD_MUTEX_INITIALIZER for statics (use pthread_mutex_init() for stack/heap memory)pthread_mutex_t m= PTHREAD_MUTEX_INITIALIZER; int count = 0;double values[10];sem_t sitems, sremain;void init() {  sem_init(&amp;sitems, 0, 0);  sem_init(&amp;sremains, 0, 10); // 10 spaces}double pop() {  // Wait until there's at least one item  sem_wait(&amp;sitems);  pthread_mutex_lock(&amp;m); // CRITICAL SECTION  double v= values[--count];  pthread_mutex_unlock(&amp;m);  sem_post(&amp;sremain); // Hey world, there's at least one space  return v;}void push(double v) {  // Wait until there's at least one space  sem_wait(&amp;sremain);  pthread_mutex_lock(&amp;m); // CRITICAL SECTION  values[count++] = v;  pthread_mutex_unlock(&amp;m);  sem_post(&amp;sitems); // Hey world, there's at least one item}// Note a robust solution will need to check sem_wait's result for EINTR (more about this later)Candidate SolutionsAs already discussed, there are critical parts of our code that can only be executed by one thread at a time. We describe this requirement as ‘mutual exclusion’; only one thread (or process) may have access to the shared resource. In multi-threaded programs, we can wrap a critical section with mutex lock and unlock calls:pthread_mutex_lock() - one thread allowed at a time! (others will have to wait here)... Do Critical Section stuff here!pthread_mutex_unlock() - let other waiting threads continueHow would we implement these lock and unlock calls? Can we create an algorithm that assures mutual exclusion?pthread_mutex_lock(p_mutex_t *m)     { while(m-&gt;lock) {}; m-&gt;lock = 1;}pthread_mutex_unlock(p_mutex_t *m)   { m-&gt;lock = 0; }At first glance, the code appears to work; if one thread attempts to locks the mutex, a later thread must wait until the lock is cleared. However this implementation does not satisfy Mutual Exclusion. Let’s take a close look at this ‘implementation’ from the point of view of two threads running around the same time.Thread ascii art or x86To simplify the discussion we consider only two threads. Note, these arguments work for threads and processes and the classic CS literature discusses these problem in terms of two processes that need exclusive access (i.e. mutual exclusion) to a critical section or shared resource. Raising a flag represents a thread/process’s intention to enter the critical section.Remember that the psuedo-code outlined below is part of a larger program; the thread or process will typically need to enter the critical section many times during the lifetime of the process. So imagine each example as wrapped inside a loop where for a random amount of time the thread or process is working on something else.Is there anything wrong with candidate solution described below?// Candidate #1wait until your flag is loweredraise my flag// Do Critical Section stufflower my flag Answer: Candidate solution #1 also suffers a race condition i.e. it does not satisfy Mutual Exclusion because both threads/processes could read each other’s flag value (=lowered) and continue.This suggests we should raise the flag before checking the other thread’s flag - which is candidate solution #2 below.// Candidate #2raise my flagwait until your flag is lowered// Do Critical Section stufflower my flag Candidate #2 satisfies mutual exclusion - it is impossible for two threads to be inside the critical section at the same time. However this code suffers from deadlock! Suppose two threads wish to enter the critical section at the same time:            Time      Thread 1      Thread 2                  1      Raise Flag                     2             Raise Flag              3      Wait      Wait      Ooops both threads / processes are now waiting for the other one to lower their flags. Neither one will enter the critical section as both are now stuck forever! This suggests we should use a turn-based variable to try to resolve who should proceed.Turn-based solutionsThe following candidate solution #3 uses a turn-based variable to politely allow one thread and then the other to continue// Candidate #3wait until my turn is myid// Do Critical Section stuffturn = youridCandidate #3 satisfies mutual exclusion (each thread or process gets exclusive access to the Critical Section), however both threads/processes must take a strict turn-based approach to using the critical section; i.e. they are forced into an alternating critical section access pattern. For example, if thread 1 wishes to read a hashtable every millisecond but another thread writes to a hashtable every second, then the reading thread would have to wait another 999ms before being able to read from the hashtable again. This ‘solution’ is not effective, because our threads should be able to make progress and enter the critical section if no other thread is currently in the critical section.Desired properties for solutions to the Critical Section Problem?There are three main desirable properties that we desire in a solution the critical section problem * Mutual Exclusion - the thread/process gets exclusive access; others must wait until it exits the critical section. * Bounded Wait - if the thread/process has to wait, then it should only have to wait for a finite, amount of time (infinite waiting times are not allowed!). The exact definition of bounded wait is that there is an upper (non-infinite) bound on the number of times any other process can enter its critical section before the given process enters. * Progress - if no thread/process is inside the critical section, then the thread/process should be able to proceed (make progress) without having to wait.With these ideas in mind let’s examine another candidate solution that uses a turn-based flag only if two threads both required access at the same time.Turn and Flag solutionsIs the following a correct solution to CSP?\\\\ Candidate #4raise my flagif your flag is raised, wait until my turn// Do Critical Section stuffturn = youridlower my flagOne instructor and another CS faculty member initially thought so! However, analyzing these solutions is tricky. Even peer-reviewed papers on this specific subject contain incorrect solutions! At first glance it appears to satisfy Mutual Exclusion, Bounded Wait and Progress: The turn-based flag is only used in the event of a tie (so Progress and Bounded Wait is allowed) and mutual exclusion appears to be satisfied. However…. Perhaps you can find a counter-example?Candidate #4 fails because a thread does not wait until the other thread lowers their flag. After some thought (or inspiration) the following scenario can be created to demonstrate how Mutual Exclusion is not satisfied.Imagine the first thread runs this code twice (so the the turn flag now points to the second thread). While the first thread is still inside the Critical Section, the second thread arrives. The second thread can immediately continue into the Critical Section!            Time      Turn      Thread # 1      Thread # 2                  1      2      Raise my flag                     2      2      If your flag is raise, wait until my turn      Raise my flag              3      2      // Do Critical Section Stuff      If your flag is raised, wait until my turn (TRUE!)              4      2      // Do Critical Section Stuff      Do Critical Section Stuff - OOPS      Working SolutionsWhat is Peterson’s solution?Peterson published his novel and surprisingly simple solution in a 2 page paper in 1981. A version of his algorithm is shown below that uses a shared variable turn:\\\\ Candidate #5raise my flagturn = your_idwait until your flag is lowered and turn is yourid// Do Critical Section stufflower my flagThis solution satisfies Mutual Exclusion, Bounded Wait and Progress. If thread #2 has set turn to 2 and is currently inside the critical section. Thread #1 arrives, sets the turn back to 1 and now waits until thread 2 lowers the flag.Link to Peterson’s original article pdf: G. L. Peterson: “Myths About the Mutual Exclusion Problem”, Information Processing Letters 12(3) 1981, 115–116Was Peterson’s solution the first solution?No, Dekkers Algorithm (1962) was the first provably correct solution. A version of the algorithm is below.raise my flagwhile-loop(your flag is raised) :   if it's your turn to win :     lower my flag     wait while your turn     raise my flag// Do Critical Section stuffset your turn to winlower my flagNotice how the process’s flag is always raised during the critical section no matter if the loop is iterated zero, once or more times. Further the flag can be interpreted as an immediate intent to enter the critical section. Only if the other process has also raised the flag will one process defer, lower their intent flag and wait.Extra: Can I just implement Peterson’s (or Dekkers) algorithm in C or assembler?Yes - and with a bit searching it is possible even today to find it in production for specific simple mobile processors: Peterson’s algorithm is used to implement low-level Linux Kernel locks for the Tegra mobile processor (a system-on-chip ARM process and GPU core by Nvidia) https://android.googlesource.com/kernel/tegra.git/+/android-tegra-3.10/arch/arm/mach-tegra/sleep.S#58However in general, CPUs and C compilers can re-order CPU instructions or use CPU-core-specific local cache values that are stale if another core updates the shared variables. Thus a simple pseudo-code to C implementation is too naive for most platforms. You can stop reading now.Oh… you decided to keep reading. Well, here be dragons! Don’t say we didn’t warn you. Consider this advanced and gnarly topic but (spoiler alert) a happy ending.Consider the following code,while(flag2 ) { /* busy loop - go around again */An efficient compiler would infer that flag2 variable is never changed inside the loop, so that test can be optimized to while(true) Using volatile goes someway to prevent compiler optimizations of this kind.Independent instructions can be re-ordered by an optimizing compiler or at runtime by an out-of-order execution optimization by the CPU. These sophisticated optimizations if the code requires variables to be modified and checked and a precise order.A related challenge is that CPU cores include a data cache to store recently read or modified main memory values. Modified values may not be written back to main memory or re-read from memory immediately. Thus data changes, such as the state of a flag and turn variable in the above examples, may not be shared between two CPU codes.But there is happy ending. Fortunately, modern hardware addresses these issues using ‘memory fences’ (also known as memory barrier) CPU instructions to ensure that main memory and the CPUs’ cache is in a reasonable and coherent state. Higher level synchronization primitives, such as pthread_mutex_lock are will call these CPU instructions as part of their implementation. Thus, in practice, surrounding critical section with a mutex lock and unlock calls is sufficient to ignore these lower-level problems.Further reading: we suggest the following web post that discusses implementing Peterson’s algorithm on an x86 process and the linux documentation on memory barriers.      http://bartoszmilewski.com/2008/11/05/who-ordered-memory-fences-on-an-x86/        http://lxr.free-electrons.com/source/Documentation/memory-barriers.txt  Implementing Counting Semaphore      We can implement a counting semaphore using condition variables.        Each semaphore needs a count, a condition variable and a mutex    typedef struct sem_t {  int count;   pthread_mutex_t m;  pthread_condition_t cv;} sem_t;      Implement sem_init to initialize the mutex and condition variableint sem_init(sem_t *s, int pshared, int value) {    if (pshared) {         errno = ENOSYS /* 'Not implemented'*/;         return -1;    }    s-&gt;count = value;    pthread_mutex_init(&amp;s-&gt;m, NULL);    pthread_cond_init(&amp;s-&gt;cv, NULL);    return 0;}Our implementation of sem_post needs to increment the count. We will also wake up any threads sleeping inside the condition variable. Notice we lock and unlock the mutex so only one thread can be inside the critical section at a time.sem_post(sem_t *s) {  pthread_mutex_lock(&amp;s-&gt;m);  s-&gt;count++;  pthread_cond_signal(&amp;s-&gt;cv); /* See note */  /* A woken thread must acquire the lock, so it will also have to wait until we call unlock*/  pthread_mutex_unlock(&amp;s-&gt;m);}Our implementation of sem_wait may need to sleep if the semaphore’s count is zero. Just like sem_post we wrap the critical section using the lock (so only one thread can be executing our code at a time). Notice if the thread does need to wait then the mutex will be unlocked, allowing another thread to enter sem_post and waken us from our sleep!Notice that even if a thread is woken up, before it returns from pthread_cond_wait it must re-acquire the lock, so it will have to wait a little bit more (e.g. until sem_post finishes).sem_wait(sem_t *s) {  pthread_mutex_lock(&amp;s-&gt;m);  while (s-&gt;count == 0) {      pthread_cond_wait(&amp;s-&gt;cv, &amp;s-&gt;m); /*unlock mutex, wait, relock mutex*/  }  s-&gt;count--;  pthread_mutex_unlock(&amp;s-&gt;m);}Wait sem_post keeps calling pthread_cond_signal won’t that break sem_wait? Answer: No! We can’t get past the loop until the count is non-zero. In practice this means sem_post would unnecessary call pthread_cond_signal even if there are no waiting threads. A more efficient implementation would only call pthread_cond_signal when necessary i.e.  /* Did we increment from zero to one- time to signal a thread sleeping inside sem_post */  if (s-&gt;count == 1) /* Wake up one waiting thread!*/     pthread_cond_signal(&amp;s-&gt;cv);Other semaphore considerations      Real semaphores implementation include a queue and scheduling concerns to ensure fairness and priority e.g. wake up the highest-priority longest sleeping thread.        Also, an advanced use of sem_init allows semaphores to be shared across processes. Our implementation only works for threads inside the same process.  How do I wait for N threads to reach a certain point before continuing onto the next step?Suppose we wanted to perform a multi-threaded calculation that has two stages, but we don’t want to advance to the second stage until the first stage is completed.We could use a synchronization method called a barrier. When a thread reaches a barrier, it will wait at the barrier until all the threads reach the barrier, and then they’ll all proceed together.Think of it like being out for a hike with some friends. You agree to wait for each other at the top of each hill (and you make a mental note how many are in your group). Say you’re the first one to reach the top of the first hill. You’ll wait there at the top for your friends. One by one, they’ll arrive at the top, but nobody will continue until the last person in your group arrives. Once they do, you’ll all proceed.Pthreads has a function pthread_barrier_wait() that implements this. You’ll need to declare a pthread_barrier_t variable and initialize it with pthread_barrier_init(). pthread_barrier_init() takes the number of threads that will be participating in the barrier as an argument. Here’s an example.Now let’s implement our own barrier and use it to keep all the threads in sync in a large calculation.double data[256][8192]1 Threads do first calculation (use and change values in data)2 Barrier! Wait for all threads to finish first calculation before continuing3 Threads do second calculation (use and change values in data)The thread function has four main parts-void *calc(void *arg) {  /* Do my part of the first calculation */  /* Am I the last thread to finish? If so wake up all the other threads! */  /* Otherwise wait until the other threads has finished part one */  /* Do my part of the second calculation */}Our main thread will create the 16 threads and we will divide each calculation into 16 separate pieces. Each thread will be given a unique value (0,1,2,..15), so it can work on its own block. Since a (void*) type can hold small integers, we will pass the value of i by casting it to a void pointer.#define N (16)double data[256][8192] ;int main() {    pthread_t ids[N];    for(int i = 0; i &lt; N; i++)          pthread_create(&amp;ids[i], NULL, calc, (void *) i);Note, we will never dereference this pointer value as an actual memory location - we will just cast it straight back to an integer:void *calc(void *ptr) {// Thread 0 will work on rows 0..15, thread 1 on rows 16..31  int x, y, start = N * (int) ptr;  int end = start + N;   for(x = start; x &lt; end; x++) for (y = 0; y &lt; 8192; y++) { /* do calc #1 */ }After calculation 1 completes, we need to wait for the slower threads (unless we are the last thread!). So, keep track of the number of threads that have arrived at our barrier aka ‘checkpoint’:// Global: int remain = N;// After calc #1 code:remain--; // We finishedif (remain ==0) {/*I'm last!  -  Time for everyone to wake up! */ }else {  while (remain != 0) { /* spin spin spin*/ }}However the above code has a race condition (two threads might try to decrement remain) and the loop is a busy loop. We can do better! Let’s use a condition variable and then we will use a broadcast/signal functions to wake up the sleeping threads.A reminder, that a condition variable is similar to a house! Threads go there to sleep (pthread_cond_wait). You can choose to wake up one thread (pthread_cond_signal) or all of them (pthread_cond_broadcast). If there are no threads currently waiting then these two calls have no effect.A condition variable version is usually very similar to a busy loop incorrect solution - as we will show next. First, let’s add a mutex and condition global variables and don’t forget to initialize them in main …//global variablespthread_mutex_t m;pthread_cond_t cv;main() {  pthread_mutex_init(&amp;m, NULL);  pthread_cond_init(&amp;cv, NULL);We will use the mutex to ensure that only one thread modifies remain at a time. The last arriving thread needs to wake up all sleeping threads - so we will use pthread_cond_broadcast(&amp;cv) not pthread_cond_signalpthread_mutex_lock(&amp;m);remain--; if (remain ==0) { pthread_cond_broadcast(&amp;cv); }else {  while(remain != 0) { pthread_cond_wait(&amp;cv, &amp;m); }}pthread_mutex_unlock(&amp;m);When a thread enters pthread_cond_wait, it releases the mutex and sleeps. At some point in the future, it will be awoken. Once we bring a thread back from its sleep, before returning it must wait until it can lock the mutex. Notice that even if a sleeping thread wakes up early, it will check the while loop condition and re-enter wait if necessary.The above barrier is not reusable Meaning that if we stick it into any old calculation loop there is a good chance that the code will encounter a condition where the barrier either deadlocks or a thread races ahead one iteration faster. Think about how you can make the above barrier reusable, meaning that if mutliple threads call barrier_wait in a loop then one can guarantee that they are on the same iteration.What is the Reader Writer Problem?Imagine you had a key-value map data structure which is used by many threads. Multiple threads should be able to look up (read) values at the same time provided the data structure is not being written to. The writers are not so gregarious - to avoid data corruption, only one thread at a time may modify (write) the data structure (and no readers may be reading at that time).This is an example of the Reader Writer Problem. Namely how can we efficiently synchronize multiple readers and writers such that multiple readers can read together but a writer gets exclusive access?An incorrect attempt is shown below (“lock” is a shorthand for pthread_mutex_lock):Attempt #1At least our first attempt does not suffer from data corruption (readers must wait while a writer is writing and vice versa)! However readers must also wait for other readers. So let’s try another implementation..Attempt #2:Our second attempt suffers from a race condition - imagine if two threads both called read and write (or both called write) at the same time. Both threads would be able to proceed! Secondly, we can have multiple readers and multiple writers, so lets keep track of the total number of readers or writers. Which brings us to attempt #3,Attempt #3Remember that pthread_cond_wait performs Three actions. Firstly it atomically unlocks the mutex and then sleeps (until it is woken by pthread_cond_signal or pthread_cond_broadcast). Thirdly the awoken thread must re-acquire the mutex lock before returning. Thus only one thread can actually be running inside the critical section defined by the lock and unlock() methods.Implementation #3 below ensures that a reader will enter the cond_wait if there are any writers writing.read() {    lock(&amp;m)    while (writing)        cond_wait(&amp;cv, &amp;m)    reading++;/* Read here! */    reading--    cond_signal(&amp;cv)    unlock(&amp;m)}However only one reader a time can read because candidate #3 did not unlock the mutex. A better version unlocks before reading :read() {    lock(&amp;m);    while (writing)        cond_wait(&amp;cv, &amp;m)    reading++;    unlock(&amp;m)/* Read here! */    lock(&amp;m)    reading--    cond_signal(&amp;cv)    unlock(&amp;m)}Does this mean that a writer and read could read and write at the same time? No! First of all, remember cond_wait requires the thread re-acquire the mutex lock before returning. Thus only one thread can be executing code inside the critical section (marked with **) at a time!read() {    lock(&amp;m);**  while (writing)**      cond_wait(&amp;cv, &amp;m)**  reading++;    unlock(&amp;m)/* Read here! */    lock(&amp;m)**  reading--**  cond_signal(&amp;cv)    unlock(&amp;m)}Writers must wait for everyone. Mutual exclusion is assured by the lock.write() {    lock(&amp;m);**  while (reading || writing)**      cond_wait(&amp;cv, &amp;m);**  writing++;**** /* Write here! */**  writing--;**  cond_signal(&amp;cv);    unlock(&amp;m);}Candidate #3 above also uses pthread_cond_signal ; this will only wake up one thread. For example, if many readers are waiting for the writer to complete then only one sleeping reader will be awoken from their slumber. The reader and writer should use cond_broadcast so that all threads should wake up and check their while-loop condition.Starving writersCandidate #3 above suffers from starvation. If readers are constantly arriving then a writer will never be able to proceed (the ‘reading’ count never reduces to zero). This is known as starvation and would be discovered under heavy loads. Our fix is to implement a bounded-wait for the writer. If a writer arrives they will still need to wait for existing readers however future readers must be placed in a “holding pen” and wait for the writer to finish. The “holding pen” can be implemented using a variable and a condition variable (so that we can wake up the threads once the writer has finished).Our plan is that when a writer arrives, and before waiting for current readers to finish, register our intent to write (by incrementing a counter ‘writer’). Sketched below -write() {    lock()    writer++    while (reading || writing)    cond_wait    unlock()  ...}And incoming readers will not be allowed to continue while writer is nonzero. Notice ‘writer’ indicates a writer has arrived, while ‘reading’ and ‘writing’ counters indicate there is an active reader or writer.read() {    lock()    // readers that arrive *after* the writer arrived will have to wait here!    while(writer)    cond_wait(&amp;cv,&amp;m)    // readers that arrive while there is an active writer    // will also wait.    while (writing)         cond_wait(&amp;cv,&amp;m)    reading++    unlock  ...}Attempt #4Below is our first working solution to the Reader-Writer problem. Note if you continue to read about the “Reader Writer problem” then you will discover that we solved the “Second Reader Writer problem” by giving writers preferential access to the lock. This solution is not optimal. However it satisfies our original problem (N active readers, single active writer, avoids starvation of the writer if there is a constant stream of readers).Can you identify any improvements? For example, how would you improve the code so that we only woke up readers or one writer?int writers; // Number writer threads that want to enter the critical section (some or all of these may be blocked)int writing; // Number of threads that are actually writing inside the C.S. (can only be zero or one)int reading; // Number of threads that are actually reading inside the C.S.// if writing !=0 then reading must be zero (and vice versa)reader() {    lock(&amp;m)    while (writers)        cond_wait(&amp;turn, &amp;m)    // No need to wait while(writing here) because we can only exit the above loop    // when writing is zero    reading++    unlock(&amp;m)  // perform reading here    lock(&amp;m)    reading--    cond_broadcast(&amp;turn)    unlock(&amp;m)}writer() {    lock(&amp;m)      writers++      while (reading || writing)           cond_wait(&amp;turn, &amp;m)      writing++      unlock(&amp;m)      // perform writing here      lock(&amp;m)      writing--      writers--      cond_broadcast(&amp;turn)      unlock(&amp;m)  }Ring BufferA ring buffer is a simple, usually fixed-sized, storage mechanism where contiguous memory is treated as if it is circular, and two index counters keep track of the current beginning and end of the queue. As array indexing is not circular, the index counters must wrap around to zero when moved past the end of the array. As data is added (enqueued) to the front of the queue or removed (dequeued) from tail of the queue, the current items in the buffer form a train that appears to circle the trackA simple (single-threaded) implementation is shown below. Note enqueue and dequeue do not guard against underflow or overflow - it’s possible to add an item when when the queue is full and possible to remove an item when the queue is empty. For example if we added 20 integers (1,2,3…) to the queue and did not dequeue any items then values 17,18,19,20 would overwrite the 1,2,3,4. We won’t fix this problem right now, instead when we create the multi-threaded version we will ensure enqueue-ing and dequeue-ing threads are blocked while the ring buffer is full or empty respectively.void *buffer[16];int in = 0, out = 0;void enqueue(void *value) { /* Add one item to the front of the queue*/  buffer[in] = value;  in++; /* Advance the index for next time */  if (in == 16) in = 0; /* Wrap around! */}void *dequeue() { /* Remove one item to the end of the queue.*/  void *result = buffer[out];  out++;  if (out == 16) out = 0;  return result;}What are gotchas of implementing a Ring Buffer?It’s very tempting to write the enqueue or dequeue method in the following compact form (N is the capacity of the buffer e.g. 16):void enqueue(void *value)  b[ (in++) % N ] = value;}This method would appear to work (pass simple tests etc) but contains a subtle bug. With enough enqueue operations (a bit more than two billion) the int value of in will overflow and become negative! The modulo (or ‘remainder’) operator ``% preserves the sign. Thus you might end up writing into b[-14] for example!A compact form is correct uses bit masking provided N is 2^x (16,32,64,…)b[ (in++) &amp; (N-1) ] = value;This buffer does not yet prevent buffer underflow or overflow. For that, we’ll turn to our multi-threaded attempt that will block a thread until there is space or there is at least one item to remove.Multithreaded CorrectnessThe following code is an incorrect implementation. What will happen? Will enqueue and/or dequeue block? Is mutual exclusion satisfied? Can the buffer underflow? Can the buffer overflow? For clarity pthread_mutex is shortened to p_m and we assume sem_wait cannot be interrupted.#define N 16void *b[N]int in = 0, out = 0p_m_t locksem_t s1,s2void init() {     p_m_init(&amp;lock, NULL)    sem_init(&amp;s1, 0, 16)    sem_init(&amp;s2, 0, 0)}enqueue(void *value) {    p_m_lock(&amp;lock)    // Hint: Wait while zero. Decrement and return    sem_wait( &amp;s1 )      b[ (in++) &amp; (N-1) ] = value    // Hint: Increment. Will wake up a waiting thread     sem_post(&amp;s1)     p_m_unlock(&amp;lock)}void *dequeue(){    p_m_lock(&amp;lock)    sem_wait(&amp;s2)    void *result = b[(out++) &amp; (N-1) ]    sem_post(&amp;s2)    p_m_unlock(&amp;lock)    return result}AnalysisBefore reading on, see how many mistakes you can find. Then determine what would happen if threads called the enqueue and dequeue methods.      The enqueue method waits and posts on the same semaphore (s1) and similarly with equeue and (s2) i.e. we decrement the value and then immediately increment the value, so by the end of the function the semaphore value is unchanged!        The initial value of s1 is 16, so the semaphore will never be reduced to zero - enqueue will not block if the ring buffer is full - so overflow is possible.        The initial value of s2 is zero, so calls to dequeue will always block and never return!        The order of mutex lock and sem_wait will need to be swapped (however this example is so broken that this bug has no effect!) ## Checking a multi-threaded implementation for correctness (Example 1)  The following code is an incorrect implementation. What will happen? Will enqueue and/or dequeue block? Is mutual exclusion satisfied? Can the buffer underflow? Can the buffer overflow? For clarity pthread_mutex is shortened to p_m and we assume sem_wait cannot be interrupted.void *b[16]int in = 0, out = 0p_m_t locksem_t s1, s2void init() {    sem_init(&amp;s1,0,16)    sem_init(&amp;s2,0,0)}enqueue(void *value){ sem_wait(&amp;s2) p_m_lock(&amp;lock) b[ (in++) &amp; (N-1) ] = value p_m_unlock(&amp;lock) sem_post(&amp;s1)}void *dequeue(){  sem_wait(&amp;s1)  p_m_lock(&amp;lock)  void *result = b[(out++) &amp; 15]  p_m_unlock(&amp;lock)  sem_post(&amp;s2)  return result;}Analysis      The initial value of s2 is 0. Thus enqueue will block on the first call to sem_wait even though the buffer is empty!        The initial value of s1 is 16. Thus dequeue will not block on the first call to sem_wait even though the buffer is empty - oops Underflow! The dequeue method will return invalid data.        The code does not satisfy Mutual Exclusion; two threads can modify in or out at the same time! The code appears to use mutex lock. Unfortunately the lock was never initialized with pthread_mutex_init() or PTHREAD_MUTEX_INITIALIZER - so the lock may not work (pthread_mutex_lock may simply do nothing)  Correct implementation of a ring bufferThe pseudo-code (pthread_mutex shortened to p_m etc) is shown below.As the mutex lock is stored in global (static) memory it can be initialized with PTHREAD_MUTEX_INITIALIZER.If we had allocated space for the mutex on the heap, then we would have used pthread_mutex_init(ptr, NULL)#include &lt;pthread.h&gt;#include &lt;semaphore.h&gt;// N must be 2^i#define N (16)void *b[N]int in = 0, out = 0p_m_t lock = PTHREAD_MUTEX_INITIALIZERsem_t countsem, spacesemvoid init() {  sem_init(&amp;countsem, 0, 0)  sem_init(&amp;spacesem, 0, 16)}The enqueue method is shown below. Notice: * The lock is only held during the critical section (access to the data structure). * A complete implementation would need to guard against early returns from sem_wait due to POSIX signals.enqueue(void *value){ // wait if there is no space left: sem_wait( &amp;spacesem ) p_m_lock(&amp;lock) b[ (in++) &amp; (N-1) ] = value p_m_unlock(&amp;lock) // increment the count of the number of items sem_post(&amp;countsem)}The dequeue implementation is shown below. Notice the symmetry of the synchronization calls to enqueue. In both cases the functions first wait if the count of spaces or count of items is zero.void *dequeue(){  // Wait if there are no items in the buffer  sem_wait(&amp;countsem)  p_m_lock(&amp;lock)  void *result = b[(out++) &amp; (N-1)]  p_m_unlock(&amp;lock)  // Increment the count of the number of spaces  sem_post(&amp;spacesem)  return result}Food for thought      What would happen if the order of pthread_mutex_unlock and sem_post calls were swapped?        What would happen if the order of sem_wait and pthread_mutex_lock calls were swapped?  Extra: Process SynchronizationYou thought that you were using different processes, so you don’t have to synchronize? Think again! You may not have race conditions within a process but what if your process needs to interact with the system around it? Let’s consider a motivating examplevoid write_string(const char *data) {    int fd = open(\"my_file.txt\", O_WRONLY);    write(fd, data, strlen(data));    close(fd);}int main() {    if(!fork()) {        write_string(\"key1: value1\");        wait(NULL);    } else {        write_string(\"key2: value2\");    }    return 0;}If none of the system calls fail then we should get something that looks like this (given the file was empty to begin with).key1: value1key2: value2orkey2: value2key1: value1InterruptionBut, there is a hidden nuance. Most system calls can be interrupted meaning that the operating system can stop an ongoing system call because it needs to stop the process. So barring fork wait open and close from failing – they typically go to completion – what happens if write fails? If write fails and no bytes are written, we can get something like key1: value1 or key2: value2. This is data loss which is incorrect but won’t corrupt the file. What happens if write gets interrupted after a partial write? We get all sorts of madness. For example,key2: key1: value1SolutionSo what should we do? We should use a shared mutex! Consider the following code.pthread_mutex_t * mutex = NULL;pthread_mutexattr_t attr;void write_string(const char *data) {    pthread_mutex_lock(mutex);    int fd = open(\"my_file.txt\", O_WRONLY);    int bytes_to_write = strlen(data), written = 0;    while(written &lt; bytes_to_write) {        written += write(fd, data + written, bytes_to_write - written);    }    close(fd);    pthread_mutex_unlock(mutex);}int main() {    pthread_mutexattr_init(&amp;attr);    pthread_mutexattr_setpshared(&amp;attr, PTHREAD_PROCESS_SHARED);    pmutex = mmap (NULL, sizeof(pthread_mutex_t),                 PROT_READ|PROT_WRITE, MAP_SHARED|MAP_ANON, -1, 0);    pthread_mutex_init(pmutex, &amp;attrmutex);    if(!fork()) {        write_string(\"key1: value1\");        wait(NULL);        pthread_mutex_destroy(pmutex);        pthread_mutexattr_destroy(&amp;attrmutex);         munmap((void *)pmutex, sizeof(*pmutex));    } else {        write_string(\"key2: value2\");    }    return 0;}What the code does in main is initialize a process shared mutex using a piece of shared memory. You will find out what this call to mmap does later – just assume for the time being that it create memory that is shared between processes. We can initialize a pthread_mutex_t in that special piece of memory and use it as normal. To counter write failing, we have put the write call inside a while loop that keeps writing so long as there are bytes left to write. Now if all the other system calls function, there should be more more race conditions.Most programs try to avoid this problem entirely by writing to separate files, but it is good to know that there are mutexes across processes, and they are useful. You can use all of the primitives that you were taught previously! Barriers, semaphores, and condition variables can all be initialized on a shared piece of memory and used in similar ways to their multithreading counterparts. You don’t need to know the implementation, just need to know that mutexes and other synchronization primitives can be shared across processes and some of the benefits.      You don’t have to worry about arbitrary memory addresses becoming race condition candidates. This means that only areas that you specifically mmap or outside system resources like files are ever in danger.        You get the nice isolation of a processes so if one process fails the system can maintain intact        When you have a lot of threads, creating a process might ease the system load  Extra: Higher Order Models of SynchronizationWhen using atomics, you need to specify the right model of synchronization in order to make sure you have the correct behavior for your program.Sequentially ConsistentSequentially consistent is the simplest, least error prone and most expensive model. This model says that any change that happens, all changes before it will be synchronized between all threads.    -Thread 1-              -Thread 2-    y = 1                   if (atomic_load(x) == 2)    atomic_store(x, 2);         y != 1 &amp;&amp; *NULL = 1;Will never segfault. This is because either the store happens before the if statement in thread 2 and y == 1 or the store happens after and x does not equal 2.RelaxedRelaxedAcquire/RelaxedAcquire/RelaxedConsumeConsumeExamples were adapted fromExternal Resources      pthread_mutex_lock man page        pthread_mutex_unlock man page        pthread_mutex_init man page        pthread_mutex_destroy man page        sem_init        sem_wait        sem_post        sem_destroy  Topics      Atomic operations        Critical Section        Producer Consumer Problem        Using Condition Variables        Using Counting Semaphore        Implementing a barrier        Implementing a ring buffer        Using pthread_mutex        Implementing producer consumer        Analyzing multi-threaded coded  Questions      What is atomic operation?        Why will the following not work in parallel code    //In the global sectionsize_t a;//In pthread functionfor(int i = 0; i &lt; 100000000; i++) a++;        And this will?    //In the global sectionatomic_size_t a;//In pthread functionfor(int i = 0; i &lt; 100000000; i++) atomic_fetch_add(a, 1);            What are some downsides to atomic operations? What would be faster: keeping a local variable or many atomic operations?        What is the critical section?        Once you have identified a critical section, what is one way of assuring that only one thread will be in the section at a time?        Identify the critical section here    struct linked_list;struct node;void add_linked_list(linked_list *ll, void* elem){    node* packaged = new_node(elem);    if(ll-&gt;head){         ll-&gt;head =     }else{         packaged-&gt;next = ll-&gt;head;         ll-&gt;head = packaged;         ll-&gt;size++;    }        }void* pop_elem(linked_list *ll, size_t index){    if(index &gt;= ll-&gt;size) return NULL;            node *i, *prev;    for(i = ll-&gt;head; i &amp;&amp; index; i = i-&gt;next, index--){        prev = i;    }    //i points to the element we need to pop, prev before    if(prev-&gt;next) prev-&gt;next = prev-&gt;next-&gt;next;    ll-&gt;size--;    void* elem = i-&gt;elem;    destroy_node(i);    return elem;}            How tight can you make the critical section?        What is a producer consumer problem? How might the above be a producer consumer problem be used in the above section? How is a producer consumer problem related to a reader writer problem?        What is a condition variable? Why is there an advantage to using one over a while loop?        Why is this code dangerous?    if(not_ready){     pthread_cond_wait(&amp;cv, &amp;mtx);}            What is a counting semaphore? Give me an analogy to a cookie jar/pizza box/limited food item.        What is a thread barrier?        Use a counting semaphore to implement a barrier.        Write up a Producer/Consumer queue, How about a producer consumer stack?        Give me an implementation of a reader-writer lock with condition variables, make a struct with whatever you need, it just needs to be able to support the following functions    void reader_lock(rw_lock_t* lck);void writer_lock(rw_lock_t* lck);void reader_unlock(rw_lock_t* lck);void writer_unlock(rw_lock_t* lck);        The only specification is that in between reader_lock and reader_unlock, no writers can write. In between the writer locks, only one writer may be writing at a time.        Write code to implement a producer consumer using ONLY three counting semaphores. Assume there can be more than one thread calling enqueue and dequeue. Determine the initial value of each semaphore.        Write code to implement a producer consumer using condition variables and a mutex. Assume there can be more than one thread calling enqueue and dequeue.        Use CVs to implement add(unsigned int) and subtract(unsigned int) blocking functions that never allow the global value to be greater than 100.        Use CVs to implement a barrier for 15 threads.        How many of the following statements are true?        There can be multiple active readers        There can be multiple active writers        When there is an active writer the number of active readers must be zero        If there is an active reader the number of active writers must be zero        A writer must wait until the current active readers have finished        Todo: Analyzing mulithreaded code snippets  "
  },{
    "title": "Threads",
    "url": " /coursebook/Threads",
   "content": "  Threads          Processes vs threads      Thread Internals                  How many threads can my process have?                    Simple Usage      Pthread Functions      Race Conditions                  How can I find out more?          Embarrassingly Parallel Problems          Another problem, Parallel Map          Scheduling          Other Problems                    Topics      Questions      [1][]  ThreadsA thread is short for ‘thread-of-execution’. It represents the sequence of instructions that the CPU has (and will) execute. To remember how to return from function calls, and to store the values of automatic variables and parameters a thread uses a stack. A thread is a process (meaning that creating a thread is similar to fork) except there is no copying meaning no copy on write. What this allows is for a process to share the same address space, variables, heap, file descriptors and etc. The actual system call to create a thread is similar to fork; it’s clone. We won’t go into the specifics but you can read the man pages keeping in mind that it is outside the direct scope of this course. LWP or threads are preferred to forking for a lot of scenarios because there is a lot less overhead creating them. But in some cases (notably python uses this) multiprocessing is the way to make your code faster.Processes vs threadsCreating separate processes is useful when      When more security is desired. For example, Chrome browser uses different processes for different tabs.        When running an existing and complete program then a new process is required (e.g. starting ‘gcc’)        When you are running into synchronization primitives and each process is operating on something in the system.        When you have too many threads – the kernel tries to schedule all the threads near each other which could cause more harm than good.        When you don’t want to worry about race conditions        If one thread blocks in a task (say IO) then all threads block. Processes don’t have that same restriction.        When the amount of communication is minimal enough that simple IPC needs to be used.  On the other hand, creating threads is more useful when      You want to leverage the power of a multi-core system to do one task        When you can’t deal with the overhead of processes        When you want communication between the processes simplified        When you want to threads to be part of the same process  Thread InternalsYour main function (and other functions you might call) has automatic variables. We will store them in memory using a stack and keep track of how large the stack is by using a simple pointer (the “stack pointer”). If the thread calls another function, we move our stack pointer down, so that we have more space for parameters and automatic variables. Once it returns from a function, we can move the stack pointer back up to its previous value. We keep a copy of the old stack pointer value - on the stack! This is why returning from a function is very quick - it’s easy to ‘free’ the memory used by automatic variables - we just need to change the stack pointer.In a multi threaded program, there are multiple stack but only one address space. The pthread library allocates some stack space (either in the heap or using a part of the main program’s stack) and uses the clone function call to start the thread at that stack address.How many threads can my process have?You can have more than one thread running inside a process. You get the first thread for free! It runs the code you write inside ‘main’. If you need more threads you can call pthread_create to create a new thread using the pthread library. You’ll need to pass a pointer to a function so that the thread knows where to start.The threads you create all live inside the same virtual memory because they are part of the same process. Thus they can all see the heap, the global variables and the program code etc. Thus you can have two (or more) CPUs working on your program at the same time and inside the same process. It’s up to the operating system to assign the threads to CPUs. If you have more active threads than CPUs then the kernel will assign the thread to a CPU for a short duration (or until it runs out of things to do) and then will automatically switch the CPU to work on another thread. For example, one CPU might be processing the game AI while another thread is computing the graphics output.Simple UsageTo use pthreads you will need to include pthread.h and compile with -pthread (or -lpthread) compiler option. This option tells the compiler that your program requires threading support. To create a thread use the function pthread_create. This function takes four arguments:int pthread_create(pthread_t *thread, const pthread_attr_t *attr,                   void *(*start_routine) (void *), void *arg);      The first is a pointer to a variable that will hold the id of the newly created thread.        The second is a pointer to attributes that we can use to tweak and tune some of the advanced features of pthreads.        The third is a pointer to a function that we want to run        Fourth is a pointer that will be given to our function  The argument void *(*start_routine) (void *) is difficult to read! It means a pointer that takes a void * pointer and returns a void * pointer. It looks like a function declaration except that the name of the function is wrapped with (* .... )#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;// remember to set compilation option -pthreadvoid *busy(void *ptr) {// ptr will point to \"Hi\"    puts(\"Hello World\");    return NULL;}int main() {    pthread_t id;    pthread_create(&amp;id, NULL, busy, \"Hi\");#if loop_forever // Loop forever    while (1) {}#else // Join Threads    void *result;    pthread_join(id, &amp;result);#endif}In the above example, result will be null because the busy function returned null. We need to pass the address-of result because pthread_join will be writing into the contents of our pointer.Pthread Functions      pthread_create. Creates a new thread. Every thread that gets created gets a new stack. For example if you call pthread_create twice, Your process will contain three stacks - one for each thread. The first thread is created when the process starts, and you created two more. Actually there can be more stacks than this, but let’s keep it simple. The important idea is that each thread requires a stack because the stack contains automatic variables and the old CPU PC register, so that it can back to executing the calling function after the function is finished.        pthread_cancel stops a thread. Note the thread may not actually be stopped immediately. For example it can be terminated when the thread makes an operating system call (e.g. write). In practice, pthread_cancel is rarely used because it does not give a thread an opportunity to clean up after itself (for example, it may have opened some files). An alternative implementation is to use a boolean (int) variable whose value is used to inform other threads that they should finish and clean up.        pthread_exit(void *) stops the calling thread i.e. the thread never returns after calling pthread_exit. The pthread library will automatically finish the process if there are no other threads running. pthread_exit(...) is equivalent to returning from the thread’s function; both finish the thread and also set the return value (void *pointer) for the thread. Calling pthread_exit in the the main thread is a common way for simple programs to ensure that all threads finish. For example, in the following program, the myfunc threads will probably not have time to get started. On the other hand exit() exits the entire process and sets the processes exit value. This is equivalent to return (); in the main method. All threads inside the process are stopped. Note the pthread_exit version creates thread zombies, however this is not a long-running processes, so we don’t care.    int main() {  pthread_t tid1, tid2;  pthread_create(&amp;tid1, NULL, myfunc, \"Jabberwocky\");  pthread_create(&amp;tid2, NULL, myfunc, \"Vorpel\");  if (keep_threads_going) {    pthread_exit(NULL);   } else {    exit(42); //or return 42;  }  // No code is run after exit}            pthread_join() waits for a thread to finish and records its return value. Finished threads will continue to consume resources. Eventually, if enough threads are created, pthread_create will fail. In practice, this is only an issue for long-running processes but is not an issue for simple, short-lived processes as all thread resources are automatically freed when the process exits. This is equivalent to turning your children into zombies, so keep this in mind for long running processes. In the exit example, we could also wait on all the threads.    // ...  void* result;  pthread_join(tid1, &amp;result);  pthread_join(tid2, &amp;result);   return 42;// ...            You’ve heard about this already, but how can a thread be terminated?                  Returning from the thread function                    Calling pthread_exit                    Cancelling the thread with pthread_cancel                    Terminating the process (e.g. SIGTERM); exit(); returning from main            Race ConditionsRace conditions are whenever the outcome of a program is determined by its sequence of events. Meaning that the same program can run multiple times and depending on how the kernel schedules the threads could produce inaccurate results. Take for example this race condition with one thread. We create a stack variable and pass it to our pthread function.pthread_t start_threads() {  int start = 42;  pthread_t tid;  pthread_create(&amp;tid, 0, myfunc, &amp;start); // ERROR!  return tid;}The above code is invalid because the function start_threads will likely return before myfunc even starts. The function passes the address-of start, however by the time myfunc is executes, start is no longer in scope and its address will re-used for another variable. This ia race condition because there is a situation where the thread that called pthread_create could be suspended indefinitely, and the code actually works. One way we can fix this is keep the function from returning before the thread finishes.void start_threads() {  int start = 42;  void *result;  pthread_t tid;  pthread_create(&amp;tid, 0, myfunc, &amp;start); // OK - start will be valid!  pthread_join(tid, &amp;result);}Here is another small race condition. The following code is supposed to start ten threads with values 0,1,2,3,…9 However, when run prints out 1 7 8 8 8 8 8 8 8 10! Can you see why?#include &lt;pthread.h&gt;void* myfunc(void* ptr) {    int i = *((int *) ptr);    printf(\"%d \", i);    return NULL;}int main() {    // Each thread gets a different value of i to process    int i;    pthread_t tid;    for(i =0; i &lt; 10; i++) {        pthread_create(&amp;tid, NULL, myfunc, &amp;i); // ERROR    }    pthread_exit(NULL);}The above code suffers from a race condition - the value of i is changing. The new threads start later (in the example output the last thread starts after the loop has finished). To overcome this race-condition, we will give each thread a pointer to it’s own data area. For example, for each thread we may want to store the id, a starting value and an output value. We will instead treat i as a pointer and cast it by value.void* myfunc(void* ptr) {    int data = ((int) ptr);    printf(\"%d \", data);    return NULL;}int main() {    // Each thread gets a different value of i to process    int i;    pthread_t tid;    for(i =0; i &lt; 10; i++) {        pthread_create(&amp;tid, NULL, myfunc, (void *)i);    }    pthread_exit(NULL);}Some functions e.g. asctime, getenv, strtok, strerror not thread-safe. Let’s look at a simple function that is also not ‘thread-safe’ The result buffer could be stored in global memory. This is good - we wouldn’t want to return a pointer to an invalid address on the stack, but there’s only one result buffer in the entire memory. If two threads were to use it at the same time then one would corrupt the other:char *to_message(int num) {    char static result [256];    if (num &lt; 10) sprintf(result, \"%d : blah blah\" , num);    else strcpy(result, \"Unknown\");    return result;}There are ways around this like using synchronization locks. These are synchronization locks that are used to prevent race conditions and ensure proper synchronization between threads running in the same program. In addition, these locks are conceptually identical to the primitives used inside the kernel.In case you were wondering, you can fork inside a process with multiple threads! However, the child process only has a single thread, which is a clone of the thread that called fork. We can see this as a simple example, where the background threads never print out a second message in the child process.#include &lt;pthread.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;static pid_t child = -2;void *sleepnprint(void *arg) {  printf(\"%d:%s starting up...\\n\", getpid(), (char *) arg);  while (child == -2) {sleep(1);} /* Later we will use condition variables */  printf(\"%d:%s finishing...\\n\",getpid(), (char*)arg);  return NULL;  }int main() {  pthread_t tid1, tid2;  pthread_create(&amp;tid1,NULL, sleepnprint, \"New Thread One\");  pthread_create(&amp;tid2,NULL, sleepnprint, \"New Thread Two\");    child = fork();  printf(\"%d:%s\\n\",getpid(), \"fork()ing complete\");  sleep(3);      printf(\"%d:%s\\n\",getpid(), \"Main thread finished\");    pthread_exit(NULL);  return 0; /* Never executes */}8970:New Thread One starting up...8970:fork()ing complete8973:fork()ing complete8970:New Thread Two starting up...8970:New Thread Two finishing...8970:New Thread One finishing...8970:Main thread finished8973:Main thread finishedIn practice, creating threads before forking can lead to unexpected errors because (as demonstrated above) the other threads are immediately terminated when forking. Another thread might have just lock a mutex (e.g. by calling malloc) and never unlock it again. Advanced users may find pthread_atfork useful however we suggest you usually try to avoid creating threads before forking unless you fully understand the limitations and difficulties of this approach.How can I find out more?      man page        pthread reference guide        Concise third party sample code explaining create, join and exit  Embarrassingly Parallel ProblemsThe study of parallel algorithms has exploded over the past few years. An embarrassingly parallel problem is any problem that needs little effort to turn parallel. A lot of them have some synchronization concepts with them but not always. You already know a parallelizable algorithm, Merge Sort!void merge_sort(int *arr, size_t len){   if(len &gt; 1){   //Mergesort the left half   //Mergesort the right half   //Merge the two halves   }With your new understanding of threads, all you need to do is create a thread for the left half, and one for the right half. Given that your CPU has multiple real cores, you will see a speedup in accordance with Amdahl’s Law. The time complexity analysis gets interesting here as well. The parallel algorithm runs in (O(\\log^3(n))) running time (because we fancy analysis assuming that we have a lot of cores.In practice though, we typically do two changes. One, once the array gets small enough, we ditch the parallel mergesort algorithm and do a quicksort or other algorithm that works fast on small arrays (something something cache coherency). The other thing that we know is that CPUs don’t have infinite cores. To get around that, we typically keep a worker pool. You won’t see the speedup right away because of things like cache coherency and scheduling extra threads.Another problem, Parallel MapSay we want to apply a function to an entire array, one element at a time.int *map(int (*func)(int), int *arr, size_t len){  int *ret = malloc(len*sizeof(*arr));  for(size_t i = 0; i &lt; len; ++i)       ret[i] = func(arr[i]);  return ret;}Since none of the elements depend on any other element, how would you go about parallelizing this? What do you think would be the best way to split up the work between threads.SchedulingThere are a few ways to split up the work.      static scheduling break up the problems into fixed size chunks (predetermined) and have each thread work on each of the chunks. This works well when each of the subproblems take roughly the same time because there is no additional overhead. All you need to do is write a loop and give the map function to each subarray.        dynamic scheduling as a new problem becomes available have a thread serve it. This is useful when you don’t know how long the scheduling will take        guided scheduling This is a mix of the above with a mix of the benefits and the tradeoffs. You start with a static scheduling and move slowly to dynamic if needed        runtime scheduling You have absolutely no idea how long the problems are going to take. Instead of deciding it yourself, let the program decide what to do!  source, but no need to memorize.Other ProblemsFrom Wikipedia      Serving static files on a webserver to multiple users at once.        The Mandelbrot set, Perlin noise and similar images, where each point is calculated independently.        Rendering of computer graphics. In computer animation, each frame may be rendered independently (see parallel rendering).        Brute-force searches in cryptography.        Notable real-world examples include distributed.net and proof-of-work systems used in cryptocurrency.        BLAST searches in bioinformatics for multiple queries (but not for individual large queries)        Large scale facial recognition systems that compare thousands of arbitrary acquired faces (e.g., a security or surveillance video via closed-circuit television) with similarly large number of previously stored faces (e.g., a rogues gallery or similar watch list).        Computer simulations comparing many independent scenarios, such as climate models.        Evolutionary computation metaheuristics such as genetic algorithms.        Ensemble calculations of numerical weather prediction.        Event simulation and reconstruction in particle physics.        The marching squares algorithm        Sieving step of the quadratic sieve and the number field sieve.        Tree growth step of the random forest machine learning technique.        Discrete Fourier Transform where each harmonic is independently calculated.  Topics      pthread lifecycle        Each thread has a stack        Capturing return values from a thread        Using pthread_join        Using pthread_create        Using pthread_exit        Under what conditions will a process exit  Questions      What happens when a pthread gets created? (you don’t need to go into super specifics)        Where is each thread’s stack?        How do you get a return value given a pthread_t? What are the ways a thread can set that return value? What happens if you discard the return value?        Why is pthread_join important (think stack space, registers, return values)?        What does pthread_exit do under normal circumstances (ie you are not the last thread)? What other functions are called when you call pthread_exit?        Give me three conditions under which a multithreaded process will exit. Can you think of any more?        What is an embarrassingly parallel problem?  "
  },{
    "title": "Home",
    "url": " /coursebook/index",
   "content": "HomeWelcome to System Programming coursebook!This coursebook is being built by students and faculty from the University of Illinois. It is based on a crowd-source authoring wikibook experiment by Lawrence Angrave from CS @ Illinois, but is now its own .tex based projectThis book is an introduction to programming in C, and system programming (processes, threads, synchronization, networking and more!). We assume you’ve already had some programming experience, in an earlier computer science course.1. Introduction2. Background  Systems Architecture  Debugging and Environments  Valgrind  GDB  Homework 0  UIUC Specific Guidlines3. C Programming Language  History of C  Features  Crash course intro to C  Preprocessor  Language Facilities  Common C Functions  System Calls  C Memory Model  Pointers  Shell  Common Bugs  Logic and Program flow mistakes  Topics  Questions/Exercises4. Processes  Processes  Process Contents  Intro to Fork  Waiting and Execing  exec  The fork-exec-wait Pattern  Further Reading  Questions/Exercises5. Memory Allocators  Introduction  C Memory Allocation Functions  Intro to Allocating  Memory Allocator Tutorial  Case study: Buddy Allocator (an example of a segregated list)  Further Reading  Topics  Questions/Exercises6. Threads  Processes vs threads  Thread Internals  Simple Usage  Pthread Functions  Race Conditions  Topics  Questions7. Synchronization  Mutex  Condition Variables  Thread Safe Data Structures  Candidate Solutions  Working Solutions  Implementing Counting Semaphore  Ring Buffer  Extra: Process Synchronization  Extra: Higher Order Models of Synchronization  External Resources  Topics  Questions8. Deadlock  Resource Allocation Graphs  Coffman conditions  Approaches to solving deadlock  Dining Philosophers  Viable Solutions  Topics  Questions9. Scheduling  Measurements  Measures of Efficiency  Scheduling Algorithms  Topics  Questions10. Interprocess Communication  MMU and Translating Addresses  Advanced Frames and Page Protections  Pipes  Named Pipes11. Networking  The OSI Model  Layer 3: The Internet Protocol  Layer 4: TCP and Client  Layer 4: TCP Server  Layer 4: UDP  Layer 7: HTTP  Nonblocking IO  Remote Procedure Calls  Topics  Questions12. Filesystems  What is a filesystem?  Storing data on disk  Permissions and bits  Virtual filesystems and other filesystems  Memory Mapped IO  Reliable Single Disk Filesystems  Simple Filesystem Model  Topics  Questions13. Signals  The Deep Dive of Signals  Sending Signals  Handling Signals  Signal Disposition  Disposition in Child Processes (No Threads)  Signals in a multithreaded program  Topics  Questions14. Review  C  Threading  Deadlock  IPC  Processes  Mapped Memory  Networking  Signals15. Appendix  Shell  Stack Smashing  Assorted Man Pages  System Programming Jokes16. Post Mortems  Shell Shock  Heartbleed  Dirty Cow  Meltdown  Spectre  Mars Pathfinder  Mars Again  Year 2038  Northeast Blackout of 2003  Apple IOS Unicode Handling  Apple SSL Verification  Sony Rootkit Installation  Civilization and Ghandi  The Woes of Shell Scripting  Appnexus Double Free  ATT Cascading Failures - 1990"
  },{
    "title": "Networking",
    "url": " /coursebook/networking",
   "content": "bibliography:  ‘networking/networking.bib’…NetworkingNetworking has become arguably the most important use of computers inthe past 10-20 years. Most of us nowadays can’t stand a place withoutwifi or any connectivity, so it is crucial as programmers that you havean understanding of networking and how to program to communicate acrossnetworks. Although it may sound complicated, POSIX has defined nicestandards that make connecting to the outside world easy. POSIX alsolets you peer underneath the hood and optimize all the little parts ofeach connection to write high performantThe OSI ModelThe Open Source Interconnection 7 layer model (OSI Model) is a sequenceof segments that define standards for both infrastructure and protocolsfor forms of radio communication, in our case the internet. The 7 layermodel is as follows      Layer 1: The physical layer. These are the actual waves that carrythe bauds across the wire. As an aside, bits don’t cross the wirebecause in most mediums you can alter two characterstics of a wave –the amplitude and the frequency – and get more bits per clock cycle.        Layer 2: The link layer. This is how each of the agents react tocertain events (error detection, noisy channels, etc). This is whereand live.        Layer 3: The network layer. This is the heart of the internet. Thebottom two protocols deal with communication between two differentcomputers that are directly connected. This layer deals with routingpackets from one endpoint to another.        Layer 4: The transport layer. This layer specifies how the slices ofdata are received. The bottom three layers make no guarantee aboutthe order that packets are received and what happens when a packetis dropped. Using different protocols, this layer can.        Layer 5: The session layer. This layer makes sure that if aconnection in the previous layers is dropped, a new connection inthe lower layers can be established, and it looks like a nothinghappened to the end user.        Layer 6: The presentation layer. This layer deals with encryption,compression, and data translation. For example, portability betweendifferent operating systems like translating newlines towindows newlines.        Layer 7: The application layer. The application layer is where manydifferent protocols live. and are both defined at this level. Thisis typically where we define protocols across the internet. Asprogrammers, we only go lower when we think we can create algorithmsthat are more suited to our needs than all of the below.  Just to be clear this is not a networking class. We won’t go over mostof these layers in depth. We will focus on some aspects of layers 3, 4,and 7 because they are essential to know if you are going to be doingsomething with the internet, which at some point in your career you willbe. As for another definition, a protocol is a set of specifications putforward by the that govern how implementers of protocol have theirprogram or circuit behave under specific circumastnces.Layer 3: The Internet ProtocolThe following is the 30 second introduction to internet protocol (IP),the primary way to send datagrams of information from one machine toanother. “IP4”, or more precisely, is version 4 of the Internet Protocolthat describes how to send of information across a network from onemachine to another. Roughly 95% of all packets on the Internet today areIPv4 packets. A significant limitation of IPv4 is that source anddestination addresses are limited to 32 bits. IPv4 was designed at atime when the idea of 4 billion devices connected to the same networkwas unthinkable or at least not worth making the packet size larger. arewritten typically in a sequence of four octets delimited by periods“255.255.255.0” for example.Each IPv4 includes a very small header - typically 20 , that includes asource and destination address. Conceptually the source and destinationaddresses can be split into two: a network number the upper bits and thelower bits represent a particular host number on that network.A newer packet protocol solves many of the limitations of IPv4 likemaking routing tables simpler and 128 bit addresses. However, less than5% of web traffic is IPv6 based. We write IPv6 addresses in a sequenceof eight, four hexadecimal delimiters like“1F45:0000:0000:0000:0000:0000:0000:0000”. Since that can get unruly, wecan omit the zeros “1F45::”. A machine can have an IPv6 address and anIPv4 address.There are special IP Addresses. One such in IPv4 is , IPv6 as or alsoknown as localhost. Packets sent to 127.0.0.1 will never leave themachine; the address is specified to be the same machine. There are alot of others that are denoted by certain octets being zeros or 255, themaximum value. You won’t need to know all the terminology, just keep inmind that the actual number of IP addresses that a machine can haveglobally over the internet is smaller than the number of “raw”addresses. For the purposes of the class, you need to know at this layerthat IP deals with routing, fragmenting, and reassembling upper levelprotocols. A more in-depth aside follows.In-depth IPv4 SpecificationThe internet protocol deals with routing, fragmentation, and reassemblyof fragments. Datagrams are formatted as such{width=”.8\\textwidth”}      The first octet is the version number, either 4 or 6        The next octet is how long the header is. Although it may seem thatthe header is constant size, you can include optional parameters toaugment the path taken or other instructions        The next two octets specify the total length of the datagram. Thismeans this is the header, the data, footer, and padding. This isgiven in multiple of octets, meaning that a value of 20 means20 octets.        The next two are Identification number. IP handles taking packetsthat are too big to be sent over the phsyical wire and chunks themup. As such, this number identifies what datagram this originallybelonged to.        The next octet is various bit flags that can be set.        The next octet and half is fragment number. If this packet wasfragmented, this is the number this fragment represents        The next octet is time to live. So this is the number of “hops”(travels over a wire) a packet is allowed to go. This is set becausedifferent routing protocols could cuase packets to go in circles,the packets must be dropped at some point.        The next octet is the protocol number. Although protocols betweendifferent layers of the OCI model are supposed to be black boxes,this is included, so that hardware can peer into the underlyingprotocol efficiently. Take for example IP over IP (yes you can dothat!). Your ISP wraps IPv4 packets sent from your computer to theISP in another IP layer and sends the packet off to be delivered tothe website. On the reverse trip the packet is “unwrapped” and theoriginal IP datagram is sent to your computer. This was done becausewe ran out of IP addresses, and this adds additional overhead but itis a necessary fix. Other common protocols are TCP, UDP, etc.        The next two octets is an internet checksum. This is a CRC that iscalculated to make sure that a wide variety of bit errorsare detected.        Source address is what people generally refer to as the IP address.There is no verification of this, so one host can pretend to be anyIP address possible        Destination address is where you want the packet to be sent to. Thisis crucial in the routing process as you need that to route.        After: Your data! All layer of higher order protocols are put inthere        Additional options: Hosts of additional options        Footer: A bit of padding to make sure your data is a multiple of 8  RoutingThe internet protocol routing is an amazing intersection of theory andapplication. We can imagine the entire internet as a set of graphs. Mostpeers are connected to what we call “peering points” these are the WIFIrouters and the ethernet ports that one finds in their house, work, orpublic. These peering points are then connected to a wired network ofrouters, switches, and servers that all route themselves. At a top levelthere are two types of routing      Internal Routing Protocols. Internal protocols is routing designedfor within an ISP’s network. These protocols are meant to be fastand more trusting because all computers, switches, and routers arepart of an ISP. communication between two routers.        External Routing Protocols. These typically happen to be ISP to ISPprotocol. Certain routers are designated as border routers. Theserouters talk to routers from ISPs have have different policies fromaccepting or receiving packets. If an evil ISP is trying to dump allnetwork traffic onto your ISP, these routers would deal with that.These protocols also deal with gathering information about theoutside world to each router. In most routing protocols using linkstate or OSPF, a router must necessarily calculate the shortest pathto the destination. This means it needs information about the“foreign” routers which is disseminated according tothese protocols.  These two protocols have to interplay with each other nicely in order tomake sure that packets are mostly delivered. In addition, ISPs need tobe nice to each other because theoretically an ISP can handle lower loadby forwarding all packets to another ISP. If everyone does that then, nopackets get delivered at all which won’t make customers happy at all. Sothese two protocols need to be fair so the end result worksIf you want to read more about this, look at the wikipedia page forrouting here Routing.Fragmentation/ReassemblyLower layers like WiFi and Ethernet have maximum transmission sizes. Thereason being is      One host shouldn’t crowd the medium for too long        If an error occurs, we want some sort of “progress bar” on how farthe communication has gone instead of retransmitting the stream        There are physical limitations as well, keeping a laser beam inoptics working continuously may cause bit errors.  As such if the internet protocol receives a packet that is too big forthe maximum size, it must chunk it up. TCP calculates how many datagramsit needs to construct a packet and ensures that they are all transmittedand reconstructed at the end receiver. The reason that we barely usethis feature is that if any fragment is lost, the entire packet is lost.Meaning that, assuming the probability of receiving a packet assumingeach fragment is lost with an independent percentage, the probability ofsuccessfully sending a packet drops off exponentially as packet sizeincreases.As such, TCP slices its packets so that it fits inside on IP datagram.The only time that this applies is when sending UDP packets that are toobig, but most people who are using UDP optimize and set the same packetsize as well.IP MulticastA little known feature is that using the IP protocol one can send adatagram to all devices connected to a router in what is called amulticast. Multicasts can also be configured with groups, so one canefficiently slice up all connected routers and send a piece ofinformation to all of them efficiently. To access this in a higherprotocol, you need to use UDP and specify a few more options. Note thatthis will cause undo stress on the network, so a series of multicastscould flood the network fast.What’s the deal with IPv6?One of the big features of IPv6 is the address space. The world ran outof IP addresses a while ago and has been using hacks to get around that.With IPv6 there are enough internal and external addresses, so thatunless we discover alien civilizations, we probably won’t run out. Theother benefit is that these addresses are leased not bought, meaningthat if something drastic happens in let’s say the internet of thingsand there needs to be a change in the block addressing scheme, it can bedone.Another big feature is security through IPsec. IPv4 was designed withlittle to no security in mind. As such, now there is a key exchangesimilar to TLS in higher layers that allows you to encryptcommunication.Another feature is simplified processing. In order to make the internetfast, IPv4 and IPv6 headers alike are actually implemented in hardware.That means that all header options are processed in circuits as theycome in. The problem is that as the IPv4 spec grew to include a copiousamount of headers, the hardware had to become more and more advanced tosupport those headers. IPv6 reorders the headers so that packets can bedropped and routed with less hardware cycles. In the case of theinternet, every cycle matters when trying to route the world’s traffic.What’s My Address?To obtain a linked list of IP addresses of the current machine use whichwill return a linked list of IPv4 and IPv6 IP addresses among otherinterfaces as well. We can examine each entry and use to print thehost’s IP address. The struct includes the family but does not includethe sizeof the struct. Therefore we need to manually determine thestruct sized based on the family.``` {.c language=”C”} (family == AF_INET) ? sizeof(struct sockaddr_in) : sizeof(struct sockaddr_in6)The complete code is shown below.``` {.c language=\"C\"}    int required_family = AF_INET; // Change to AF_INET6 for IPv6    struct ifaddrs *myaddrs, *ifa;    getifaddrs(&amp;myaddrs);    char host[256], port[256];    for (ifa = myaddrs; ifa != NULL; ifa = ifa-&gt;ifa_next) {        int family = ifa-&gt;ifa_addr-&gt;sa_family;        if (family == required_family &amp;&amp; ifa-&gt;ifa_addr) {            if (0 == getnameinfo(ifa-&gt;ifa_addr,                                (family == AF_INET) ? sizeof(struct sockaddr_in) :                                sizeof(struct sockaddr_in6),                                host, sizeof(host), port, sizeof(port)                                 , NI_NUMERICHOST | NI_NUMERICSERV  ))                puts(host);            }        }To get your IP Address from the command line use (or Windows’s ) Howeverthis command generates a lot of output for each interface, so we canfilter the output using grepifconfig | grep inetExample output:    inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1     inet 127.0.0.1 netmask 0xff000000     inet6 ::1 prefixlen 128     inet6 fe80::7256:81ff:fe9a:9141%en1 prefixlen 64 scopeid 0x5     inet 192.168.1.100 netmask 0xffffff00 broadcast 192.168.1.255To actually grab the IP Address of a remote website. The function canconvert a human readable domain name (e.g. ) into an IPv4 and IPv6address. In fact it will return a linked-list of addrinfo structs:``` {.c language=”C”}struct addrinfo {    int              ai_flags;    int              ai_family;    int              ai_socktype;    int              ai_protocol;    socklen_t        ai_addrlen;    struct sockaddr *ai_addr;    char            *ai_canonname;    struct addrinfo *ai_next;};For example, suppose you wanted to find out the numeric IPv4 address ofa webserver at We do this in two stages. First use getaddrinfo to builda linked-list of possible connections. Secondly use to convert thebinary address into a readable form.``` {.c language=\"C\"}#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;netdb.h&gt;struct addrinfo hints, *infoptr; // So no need to use memset global variablesint main() {  hints.ai_family = AF_INET; // AF_INET means IPv4 only addresses  int result = getaddrinfo(\"www.bbc.com\", NULL, &amp;hints, &amp;infoptr);  if (result) {    fprintf(stderr, \"getaddrinfo: %s\\n\", gai_strerror(result));    exit(1);  }  struct addrinfo *p;  char host[256];  for(p = infoptr; p != NULL; p = p-&gt;ai_next) {    getnameinfo(p-&gt;ai_addr, p-&gt;ai_addrlen, host, sizeof(host), NULL, 0, NI_NUMERICHOST);    puts(host);  }  freeaddrinfo(infoptr);  return 0;}212.58.244.70212.58.244.71If you are wondering how the the computer maps to addresses, we willtalk about that in Layer 7. Spoiler: It is a service calledLayer 4: TCP and ClientMost services on the Internet today use because it efficiently hides thecomplexity of lower, packet-level nature of the Internet. TCP orTransport Control Protocol is a connection-based protocol that is builton top of IPv4 and IPv6 and therefore can be described as “TCP/IP” or“TCP over IP”. TCP creates a pipe between two machines and abstractsaway the low level packet-nature of the Internet. Thus, under mostconditions, bytes sent over a TCP connection will not be lost orcorrupted.TCP has a number of features that set it apart from the other transportprotocol UDP.      With IP, you are only allowed to send packets to a machine. If youwant one machine to handle multiple flows of data, you have to do itmanually with IP. TCP abstracts that an gives the programmer a setof virtual sockets. Clients specify the socket that you want thepacket sent to and the TCP protocol makes sure that applicationsthat are waiting for packets on that port receive that. A processcan listen for incoming packets on a particular port. However onlyprocesses with (root) access can listen on ports less than 1024. Anyprocess can listen on ports 1024 or higher. An often used port isport 80: Port 80 is used for unencrypted http requests or web pages.For example, if a web browser connects to then it will be connectingto port 80.        Packets can get dropped due to network errors or congestion. Assuch, they need to be retransmitted but at the same time theretransmission shouldn’t cause packets more packets to be dropped.This needs to balance the tradeoff between flooding the networkand speed.        Out of order packets. Packets may get routed more favorably due tovarious reasons in IP. If a later packet arrives before anotherpacket, the protocol should detect and reorder them.        Duplicate packets. Packets can arrive twice. Packets can arrivetwice. As such, a protocol need to be able to differentiate betweentwo packets given a sequence number subject to overflow.        Error correction. There is a TCP checksum that handles bit errors.This is rarely used though.        Flow Control. Flow control is performed on the receiver side. Thismay be done so that a slow receiver doesn’t get overwhelmed withpackets. Servers especially that may handle 10000 or 10 millionconcurrent connection may need to tell receivers to slow down, butnot disconnect due to load. There are also other prorblem of makingsure the local network is not overwhelmed        Congestion control. Congestion control is performed on the senderside. Congestion control is to avoid a sender from flooding thenetwork with too many packets. This is really important to make surethat each TCP connection is treated fairly. Meaning that twoconnections leaving a computer to google and youtube receive thesame bandwidth and ping as each other. One can easily define aprotocol that takes all the bandwidth and leaves other protocols inthe dust, but this tends to be malicious because more often than notlimiting a computer to a single TCP connection will yield thesame result.        Connection oriented/lifecycle oriented. You can really imagine a TCPconnection as a series of bytes sent through a pipe. There is a“lifecycle” to a TCP connection though. What this means is that aTCP connection has a series of states and certain packets receivedcan or not received can move it to another state. TCP handlessetting up the connection through SYN SYN-ACK ACK. This means theclient will send a SYNchronization packet that tells TCP whatstarting sequence to start on. Then the receiver will send a SYN-ACKmessage acknowledging the synchronization number. Then the clientwill ACKnowledge that with one last packet. The connection is nowopen for both reading and writing on both ends TCP will send dataand the receiver of the data will acknowledge that it received apacket. Then every so often if a packet is not sent, TCP will tradezero length packets to make sure the connection is still alive. Atany point, the client and server can send a FIN packet meaning thatthe server will not transmit. This packet can be altered with bitsthat only close the read or write end of a particular connection.When all ends are closed then the connection is over.  There are a list of things that TCP doesn’t provide though      Security. This means that if you connect to an IP address that saysthat it is a certain website, TCP does not verify that this websiteis in fact that IP address. You could be sending packets to amalicious computer.        Encryption. Anybody can listen in on plain TCP. The packets intransport are in plain text meaning that important things like yourpasswords could easily be skimmed by servers and regularly are.        Session Reconnection. This is handled by a higher protocols, but ifa TCP connection dies then a whole new one hast to be created andthe transmission has to be started over again.        Delimiting Requests. TCP is naturally connection oriented.Applications that are communicating over TCP need to find a uniqueway of telling each other that this request or response is over.HTTP delimits the header through two carriage returns and useseither a length field or one keeps listening until the connectioncloses  Note on network ordersIntegers can be represented in least significant byte first ormost-significant byte first. Either approach is reasonable as long asthe machine itself is internally consistent. For network communicationswe need to standardize on agreed format.returns the 16 bit unsigned integer ‘short’ value xyz in network byteorder. returns the 32 bit unsigned integer ‘long’ value xyz in networkbyte order.These functions are read as ‘host to network’; the inverse functions (,) convert network ordered byte values to host-ordered ordering. So, ishost-ordering little-endian or big-endian? The answer is - it depends onyour machine! It depends on the actual architecture of the host runningthe code. If the architecture happens to be the same as network orderingthen the result of these functions is just the argument. For x86machines, the host and network ordering is different.Unless agreed otherwise whenever you read or write the low level Cnetwork structures (e.g. port and address information), remember to usethe above functions to ensure correct conversion to/from a machineformat. Otherwise the displayed or specified value may be incorrect.This doesn’t apply to protocols that negotiate the endiannessbefore-hand. If two computers are CPU bound by converting the messagesbetween network orders – this happens with JSON parsing all the time inhigh performance systems – it may be worth it to negotiate if they areon similar endians to send in little endian order.TCP ClientThere are three basic system calls you need to connect to a remotemachine:      The call if successful, creates a linked-list of structs and setsthe given pointer to point to the first one.    In addition, you can use the hints struct to only grab certainentries like certain IP protocols etc. The addrinfo structure thatis passed into to define the kind of connection you’d like. Forexample, to specify stream-based protocols over IPv6:    ``` {.c language=”C”}struct addrinfo hints;memset(&amp;hints, 0, sizeof(hints));    hints.ai_family = AF_INET6; // Only want IPv6 (use AF_INET for IPv4)hints.ai_socktype = SOCK_STREAM; // Only want stream-based connection    Error handling with is a little different: The return value *is* theerror code. To convert to a human-readable error use to get theequivalent short English error text.``` {.c language=\"C\"}int result = getaddrinfo(...);if(result) {    const char *mesg = gai_strerror(result);    ...}            The socket call creates an outgoing socket and returns a descriptorthat can be used with and . In this sense it is the network analogof that opens a file stream - except that we haven’t connected thesocket to anything yet!    Socket creates a socket with domain  AF_INET for IPv4 or AF_INET6for IPv6, is whether to use UDP or TCP or other socket type, is anoptional choice of protocol configuration (for our examples this wecan just leave this as 0 for default). This call creates a socketobject in the kernel with which one can communicate with the outsideworld/network. You can use the result of to fill in the parameters,or provide them manually.    The socket call returns an integer - a file descriptor - and, forTCP clients, you can use it like a regular file descriptor i.e. youcan use and to receive or send packets.    TCP sockets are similar to except that they allow full duplexcommunication i.e. you can send and receive data in bothdirections independently.        Finally the connect call attempts the connection to the remotemachine. We pass the original socket descriptor and also the socketaddress information which is stored inside the addrinfo structure.There are different kinds of socket address structures which canrequire more memory. So in addition to passing the pointer, the sizeof the structure is also passed. To help identify errors andmistakes it is good practice to check the return value of allnetworking calls, including    {.c language=\"C\"}// Pull out the socket address info from the addrinfo struct:connect(sockfd, p-&gt;ai_addr, p-&gt;ai_addrlen)        (Optional) To clean up code call on the first level struct.  There is an old function is deprecated; it’s the old way convert a hostname into an IP address. The port address still needs to be manually setusing function. It’s much easier to write code to support IPv4 AND IPv6using the newerThis is all that is needed to create a simple TCP client - howevernetwork communications offers many different levels of abstraction andseveral attributes and options that can be set at each level ofabstraction. For example we haven’t talked about which can manipulateoptions for the socket. For more information see thisguide.Sending some dataOnce we have a successful connection we can read or write like any oldfile descriptor. Keep in mind if you are connected to a website, youwant to conform to the HTTP protocol specification in order to get anysort of meaningful results back. There are libraries to do this, usuallyyou don’t connect at the socket level because there are other librariesor packages around it. The number of bytes read or written may besmaller than expected. Thus it is important to check the return value ofread and write. A simple HTTP client that sends a request to compliantURL is below.``` {.c language=”C”}#include #include #include #include #include &lt;sys/types.h&gt; #include &lt;sys/socket.h&gt; #include #include #include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include #ifndef _GNU_SOURCE#define _GNU_SOURCE#endiftypedef struct _host_info {    char *hostname;    char *port;    char *resource;} host_info;host_info *get_info(char *uri);void free_info(host_info *info);host_info *send_request(host_info *info);ssize_t min(ssize_t a, ssize_t b) {    return a &lt; b ? a : b;}host_info *get_info(char *uri) {const char *http = \"http://\";int http_len = strlen(http);int uri_len = strlen(uri);if (uri_len &lt; http_len &amp;&amp; !strncmp(uri, http, min(strlen(http), uri_len))) {    fprintf(stderr, \"The uri must start with \\\"%s\\\"\", http);    exit(1);} else {    uri += http_len;    uri_len -= http_len;}char *hostname = malloc(uri_len+1);char *port = malloc(6);char *ptr = hostname;while(*uri &amp;&amp; *uri != '/' &amp;&amp; *uri != ':') {    *ptr++ = *uri++;}*ptr = '\\0';if(*uri == ':') {    ptr = port;    uri++;    while(*uri != '/') {        *ptr++ = *uri++;    }    *ptr = '\\0';} else {    free(port);    port = strdup(\"80\");}char *resource = NULL;int len = strlen(uri);if (len == 0) {    // Empty means get the index    resource = strdup(\"/\");} else {    resource = strdup(uri);}host_info *info = malloc(sizeof(*info));info-&gt;hostname = hostname;info-&gt;port = port;info-&gt;resource = resource;  return info; }void free_info(host_info *info) {    free(info-&gt;hostname);    free(info-&gt;port);    free(info-&gt;resource);    free(info);}static void send_get_request(FILE sock_file, host_info *info) {    char *buffer;    asprintf(&amp;buffer,         “GET %s HTTP/1.0\\r\\n”        “Connection: close\\r\\n”        “Accept: */\\r\\n\\r\\n”,         info-&gt;resource);    int sock_fd = fileno(sock_file);    write(sock_fd, buffer, strlen(buffer));    free(buffer);}static void connect_to_address(int sock_fd, host_info *info) {    struct addrinfo current, *result;    memset(&amp;current, 0, sizeof(struct addrinfo));    current.ai_family = AF_INET;    current.ai_socktype = SOCK_STREAM;    int s = getaddrinfo(info-&gt;hostname, info-&gt;port, &amp;current, &amp;result);    if (s != 0) {        fprintf(stderr, “getaddrinfo: %s\\n”, gai_strerror(s));        exit(1);    }    if(connect(sock_fd, result-&gt;ai_addr, result-&gt;ai_addrlen) == -1){        perror(“connection error”);        exit(1);    }    freeaddrinfo(result);}host_info *send_request(host_info *info) {    int sock_fd = socket(AF_INET, SOCK_STREAM, 0);    if (sock_fd == -1) {        perror(“socket”);        exit(1);    }    int optval = 1;    int retval = setsockopt(sock_fd, SOL_SOCKET, SO_REUSEADDR, &amp;optval,                sizeof(optval));    if(retval == -1) {        perror(“setsockopt”);        exit(1);    }    connect_to_address(sock_fd, info);// Open so you can use getlineFILE *sock_file = fdopen(sock_fd, \"r+\");setvbuf(sock_file, NULL, _IONBF, 0);send_get_request(sock_file, info);host_info *ret = NULL;if (is_redirect(sock_file)) {    ret = handle_redirect(sock_file);} else {    handle_okay(sock_file);}fclose(sock_file);return ret; }int main(int argc, char *argv[]) {    if(argc != 2) {        fprintf(stderr, “Usage: %s http://hostname[:port]/path\\n”, *argv);        return 1;     }    char *uri = argv[1];    host_info *info = get_info(uri);    do {        host_info *temp = send_request(info);        free_info(info);        info = NULL;        if (temp) {            info = temp;        }    } while(info);return 0; } ```The example above demonstrates a request to the server using HypertextTransfer Protocol. A web page (or other resources) are requested usingthe following request:GET / HTTP/1.0There are four parts the method e.g. GET,POST,…); the resource (e.g. //index.html /image.png); the proctocol “HTTP/1.0” and two new lines( r n r n)The server’s first response line describes the HTTP version used andwhether the request is successful using a 3 digit response code:HTTP/1.1 200 OKIf the client had requested a non existing file, e.g. Then the firstline includes the response code is the well-known response code:HTTP/1.1 404 Not FoundLayer 4: TCP ServerThe four system calls required to create a TCP server are: , and . Eachhas a specific purpose and should be called in roughly the above order      To create a endpoint for networking communication. A new socket byitself is not particularly useful. Though we’ve specified either apacket or stream-based connections, it is not bound to a particularnetwork interface or port. Instead socket returns a networkdescriptor that can be used with later calls to bind, listenand accept.    As one gotcha, these sockets must be declared passive. Passiveserver sockets do not actively try to connect to another host;instead they wait for incoming connections. Additionally, serversockets are not closed when the peer disconnects. Instead the clientcommunicates with a separate active socket on the server that isspecific to that connection.    Since a TCP connection is defined by the sender address and portalong with a receiver address and port, a particular server portthere can be one passive server socket but multiple active sockets:one for each currently open connection. The server’s operatingsystem maintains a lookup table that associates a unique tuple withactive sockets, so that incoming packets can be correctly routed tothe correct socket.        The call associates an abstract socket with an actual networkinterface and port. It is possible to call bind on a TCP client. Theport information used by bind can be set manually many olderIPv4-only C code examples do this, or be created using    By default a port is not immediately released when the server socketis closed. Instead, the port enters a “TIMED-WAIT” state. This canlead to significant confusion during development because the timeoutcan make valid networking code appear to fail.    To be able to immediately re-use a port, specify before binding tothe port.    ``` {.c language=”C”}  int optval = 1;  setsockopt(sfd, SOL_SOCKET, SO_REUSEPORT, &amp;optval, sizeof(optval));    bind(…);    ```    Here’s an extended stackoverflow introductory discussion of.        The call specifies the queue size for the number of incoming,unhandled connections i.e. that have not yet been assigned a networkdescriptor by Typical values for a high performance server are 128or more.        Once the server socket has been initialized the server calls to waitfor new connections. Unlike and , this call will block. i.e. ifthere are no new connections, this call will block and only returnwhen a new client connects. The returned TCP socket is associatedwith a particular tuple and will be used for all future incoming andoutgoing TCP packets that match this tuple.    Note the call returns a new file descriptor. This file descriptor isspecific to a particular client. It is common programming mistake touse the original server socket descriptor for server I/O and thenwonder why networking code has failed.    The system call can optionally provide information about the remoteclient, by passing in a sockaddr struct. Different protocols havedifferently variants of the , which are different sizes. Thesimplest struct to use is the which is sufficiently large torepresent all possible types of sockaddr. Notice that C does nothave any model of inheritance. Therefore we need to explicitly castour struct to the ‘base type’ struct sockaddr.    ``` {.c language=”C”}  struct sockaddr_storage clientaddr;  socklen_t clientaddrsize = sizeof(clientaddr);  int client_id = accept(passive_socket,          (struct sockaddr *) &amp;clientaddr,          &amp;clientaddrsize);    We’ve already seen that can build a linked list of addrinfo entries(and each one of these can include socket configuration data). Whatif we wanted to turn socket data into IP and port addresses? Enterthat can be used to convert a local or remote socket informationinto a domain name or numeric IP. Similarly the port number can berepresented as a service name (e.g. “http” for port 80). In theexample below we request numeric versions for the client IP addressand client port number.``` {.c language=\"C\"}  socklen_t clientaddrsize = sizeof(clientaddr);  int client_id = accept(sock_id, (struct sockaddr *) &amp;clientaddr, &amp;clientaddrsize);  char host[256], port[256];  getnameinfo((struct sockaddr *) &amp;clientaddr,        clientaddrsize, host, sizeof(host), port, sizeof(port),        NI_NUMERICHOST | NI_NUMERICSERV);                  (optional but highly recommended) and    Use the call when you no longer need to read any more data from thesocket, write more data, or have finished doing both. When youshutdown a socket for further writing (or reading) that informationis also sent to the other end of the connection. For example if youshutdown the socket for further writing at the server end, then amoment later, a blocked call could return 0 to indicate that no morebytes are expected.    Use when your process no longer needs the socket file descriptor.    If you -ed after creating a socket file descriptor, all processesneed to close the socket before the socket resources can be re-used.If you shutdown a socket for further read then all process are beaffected because you’ve changed the socket, not just thefile descriptor.    Well written code will a socket before calling it.  There are a few gotchas to creating a server.      Using the socket descriptor of the passive server socket(described above)        Not specifying SOCK_STREAM requirement for getaddrinfo        Not being able to re-use an existing port.        Not initializing the unused struct entries        The call will fail if the port is currently in use. Ports are permachine – not per process or user. In other words, you cannot useport 1234 while another process is using that port. Worse, ports areby default ‘tied up’ after a process has finished.  Server code exampleA working simple server example is shown below. Note this example isincomplete - for example it does not close either socket descriptor, orfree up memory created by``` {.c language=”C”}#include #include #include #include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include #include #include &lt;arpa/inet.h&gt;int main(int argc, char **argv){    int s;    int sock_fd = socket(AF_INET, SOCK_STREAM, 0);struct addrinfo hints, *result;memset(&amp;hints, 0, sizeof(struct addrinfo));hints.ai_family = AF_INET;hints.ai_socktype = SOCK_STREAM;hints.ai_flags = AI_PASSIVE;s = getaddrinfo(NULL, \"1234\", &amp;hints, &amp;result);if (s != 0) {        fprintf(stderr, \"getaddrinfo: %s\\n\", gai_strerror(s));        exit(1);}if (bind(sock_fd, result-&gt;ai_addr, result-&gt;ai_addrlen) != 0) {    perror(\"bind()\");    exit(1);}if (listen(sock_fd, 10) != 0) {    perror(\"listen()\");    exit(1);}struct sockaddr_in *result_addr = (struct sockaddr_in *) result-&gt;ai_addr;printf(\"Listening on file descriptor %d, port %d\\n\", sock_fd, ntohs(result_addr-&gt;sin_port));printf(\"Waiting for connection...\\n\");int client_fd = accept(sock_fd, NULL, NULL);printf(\"Connection made: client_fd=%d\\n\", client_fd);char buffer[1000];int len = read(client_fd, buffer, sizeof(buffer) - 1);buffer[len] = '\\0';printf(\"Read %d chars\\n\", len);printf(\"===\\n\");printf(\"%s\\n\", buffer);return 0; } ```Layer 4: UDPUDP is a connectionless protocol that is built on top of IPv4 and IPv6.It’s very simple to use: Decide the destination address and port andsend your data packet! However the network makes no guarantee aboutwhether the packets will arrive. Packets (aka Datagrams) may be droppedif the network is congested. Packets may be duplicated or arrive out oforder.Between two distant data-centers it’s typical to see 3% packet loss. Atypical use case for UDP is when receiving up to date data is moreimportant than receiving all of the data. For example, a game may sendcontinuous updates of player positions. A streaming video signal maysend picture updates using UDPUDP Attributes      Unreliable Datagram Protocol Packets sent through UDP are notguaranteed to reach their destination. The probability that thepacket gets delivered goes down over time.        Simple The UDP protocol is supposed to have much less fluff thanTCP. Meaning that for TCP there are a lot of configurable parametersand a lot of edge cases in the implementation. UDP is just fireand forget.        Stateless/Transaction The UDP protocol does not keep a “state” ofthe connection. This makes the protocol more simple and let’s theprotocol represent simple transactions like requesting or respondingto queries. There is also less overhead to sending a UDP messagebecause there is no three way handshake.        Manual Flow/Congestion Control You have to manually manage the flowand congestion control which is a double edged sword. On one handyou have full control over everything, but on the other hand TCP hasdecades of optimization, meaning your protocol for its use casesneeds to be more efficient that that to be more beneficial touse it.        Multicast This is one thing that you can only do with UDP. Thismeans that you can send a message to every peer connected to aparticular router that is part of a particular group.  UDP ClientUDP Clients are pretty versatile below is a simple client that sends apacket to a server specified through the command line. Note that thisclient sends a packet and doesn’t wait for acknowledgement. It fires andforgets. The example below also uses because some legacy functionalitystill works pretty well for setting up a client.``` {.c language=”C”}    struct sockaddr_in addr;    memset(&amp;addr, 0, sizeof(addr));    addr.sin_family = AF_INET;    addr.sin_port = htons((uint16_t)port);  struct hostent *serv = gethostbyname(hostname);  if (!serv) {    perror(“gethostbyname”);    exit(1);  }The previous code grabs an entry that matches by hostname. Even thoughthis isn’t portable, it definitely gets the job done. The full examplefollows.``` {.c language=\"C\"}#include &lt;stdint.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/time.h&gt;#include &lt;assert.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;sys/time.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;netdb.h&gt;#include &lt;pthread.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;#include &lt;sys/mman.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;int connectToUDP(int port, char *hostname, struct sockaddr_in *ipaddr) {    int sockfd = socket(AF_INET, SOCK_DGRAM, 0);    if (sockfd &lt; 0) {        perror(\"socket\");    }    int optval = 1;    // Let them reuse    setsockopt(sockfd, SOL_SOCKET, SO_REUSEPORT, &amp;optval, sizeof(optval));    struct sockaddr_in addr;    memset(&amp;addr, 0, sizeof(addr));    addr.sin_family = AF_INET;    addr.sin_port = htons((uint16_t)port);  struct hostent *serv = gethostbyname(hostname);  if (!serv) {    perror(\"gethostbyname\");    exit(1);  }   memcpy(&amp;addr.sin_addr.s_addr, serv-&gt;h_addr, serv-&gt;h_length);  if (ipaddr) {    memcpy(ipaddr, &amp;addr, sizeof(*ipaddr));  }    // Timeouts for resending acks and whatnot    struct timeval tv;    tv.tv_sec = 0;    tv.tv_usec = SOCKET_TIMEOUT;    setsockopt(sockfd, SOL_SOCKET, SO_RCVTIMEO, &amp;tv, sizeof(tv));    return sockfd;}int main(int argc, char **argv) {  char *hostname = argv[1];  int port = strtoll(argv[2], NULL, 10);  struct sock_addr_in ipaddr;  port = connectToUDP(port, hostname, &amp;ipaddr, 0)   char *to_send = \"Hello!\"  int send_ret = sendto(port, to_send, packet_size, 0,             (struct sockaddr *)&amp;ipaddr,             sizeof(ipaddr));  return 0;}UDP ServerThere are a variety of function calls available to send UDP sockets. Wewill use the newer getaddrinfo to help set up a socket structure.Remember that UDP is a simple packet-based (‘data-gram’) protocol ;there is no connection to set up between the two hosts. First,initialize the hints addrinfo struct to request an IPv6, passivedatagram socket.``` {.c language=”C”}memset(&amp;hints, 0, sizeof(hints));hints.ai_family = AF_INET6; // use AF_INET instead for IPv4hints.ai_socktype =  SOCK_DGRAM;hints.ai_flags =  AI_PASSIVE;Next, use getaddrinfo to specify the port number (we don’t need tospecify a host as we are creating a server socket, not sending a packetto a remote host).``` {.c language=\"C\"}getaddrinfo(NULL, \"300\", &amp;hints, &amp;res);sockfd = socket(res-&gt;ai_family, res-&gt;ai_socktype, res-&gt;ai_protocol);bind(sockfd, res-&gt;ai_addr, res-&gt;ai_addrlen);The port number is less than 1024, so the program will need privileges.We could have also specified a service name instead of a numeric portvalue.So far the calls have been similar to a TCP server. For a stream-basedservice we would call and accept. For our UDP-serve we can just startwaiting for the arrival of a packet on the socket-``` {.c language=”C”}struct sockaddr_storage addr;int addrlen = sizeof(addr);// ssize_t recvfrom(int socket, void* buffer, size_t buflen, int flags, struct sockaddr *addr, socklen_t * address_len);byte_count = recvfrom(sockfd, buf, sizeof(buf), 0, &amp;addr, &amp;addrlen);The addr struct will hold sender (source) information about the arrivingpacket. Note the type is a sufficiently large enough to hold allpossible types of socket addresses (e.g. IPv4, IPv6 and other sockettypes). The full UDP server code is below.``` {.c language=\"C\"}#include &lt;string.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;netdb.h&gt;#include &lt;unistd.h&gt;#include &lt;arpa/inet.h&gt;int main(int argc, char **argv){    int s;    struct addrinfo hints, *res;    memset(&amp;hints, 0, sizeof(hints));    hints.ai_family = AF_INET6; // INET for IPv4    hints.ai_socktype =  SOCK_DGRAM;    hints.ai_flags =  AI_PASSIVE;    getaddrinfo(NULL, \"300\", &amp;hints, &amp;res);    int sockfd = socket(res-&gt;ai_family, res-&gt;ai_socktype, res-&gt;ai_protocol);    if (bind(sockfd, res-&gt;ai_addr, res-&gt;ai_addrlen) != 0) {        perror(\"bind()\");        exit(1);    }    struct sockaddr_storage addr;    int addrlen = sizeof(addr);    while(1){        char buf[1024];        ssize_t byte_count = recvfrom(sockfd, buf, sizeof(buf), 0, &amp;addr, &amp;addrlen);        buf[byte_count] = '\\0';        printf(\"Read %d chars\\n\", byte_count);        printf(\"===\\n\");        printf(\"%s\\n\", buf);    }    return 0;}Layer 7: HTTPLayer 7 of the OSI layer deals with application level interfaces.Meaning that you can ignore everything below this layer and treat aninternet as a way of communicating with another computer than can besecure and the session may reconnect. Common layer 7 protocols are thefollowing      HTTP(S) - Hyper Text Transfer Protocol. Sends arbitrary data andexecutes remote actions on a web server.        FTP - File Transfer Protocol. Transfers a file from one computer toanother        TFTP - Trivial File Transfer Protocol. Same as above but using UDP.        DNS - Domain Name Service. Translates hostnames to IP addresses        SMTP - Simple Mail Transfer Protocol. Allows one to send plain textemails to an email server        SSH - Secure SHell. Allows one computer to connect to anothercomputer and execute commands remotely.        Bitcoin - Decentralized crypto currency        BitTorrent - Peer to peer file sharing protocol        NTP - Network Time Protocol. This protocol helps keep yourcomputer’s clock synced with the outside world  How is a website converted into an IP address?A system called “DNS” (Domain Name Service) is used. If a machine doesnot hold the answer locally then it sends a UDP packet to a local DNSserver. This server in turn may query other upstream DNS servers.DNS by itself is fast but not secure. DNS requests are not encrypted andsusceptible to ‘man-in-the-middle’ attacks. For example, a coffee shopinternet connection could easily subvert your DNS requests and send backdifferent IP addresses for a particular domain. The way this is usuallysubverted is that after the IP address is obtained then a connection isusually made over HTTPS. HTTPS uses what is called the TLS (formerlyknown as SSL) to secure transmissions and verify the IP address is whothey say they are.Nonblocking IONormally, when you call , if the data is not available yet it will waituntil the data is ready before the function returns. When you’re readingdata from a disk, that delay may not be long, but when you’re readingfrom a slow network connection it may take a long time for that data toarrive, if it ever arrives.POSIX lets you set a flag on a file descriptor such that any call to onthat file descriptor will return immediately, whether it has finished ornot. With your file descriptor in this mode, your call to will start theread operation, and while it’s working you can do other useful work.This is called “nonblocking” mode, since the call to doesn’t block.To set a file descriptor to be nonblocking:``` {.c language=”C”}// fd is my file descriptorint flags = fcntl(fd, F_GETFL, 0);fcntl(fd, F_SETFL, flags | O_NONBLOCK);For a socket, you can create it in nonblocking mode by adding to thesecond argument to :``` {.c language=\"C\"}fd = socket(AF_INET, SOCK_STREAM | SOCK_NONBLOCK, 0);When a file is in nonblocking mode and you call , it will returnimmediately with whatever bytes are available. Say 100 bytes havearrived from the server at the other end of your socket and you call .Read will return immediately with a value of 100, meaning it read 100 ofthe 150 bytes you asked for. Say you tried to read the remaining datawith a call to , but the last 50 bytes still hadn’t arrived yet. wouldreturn -1 and set the global error variable errno to either EAGAINor EWOULDBLOCK. That’s the system’s way of telling you the data isn’tready yet.also works in nonblocking mode. Say you want to send 40,000 bytes to aremote server using a socket. The system can only send so many bytes ata time. Common systems can send about 23,000 bytes at a time. Innonblocking mode, would return the number of bytes it was able to sendimmediately, or about 23,000. If you called right away again, it wouldreturn -1 and set errno to EAGAIN or EWOULDBLOCK. That’s the system’sway of telling you it’s still busy sending the last chunk of data, andisn’t ready to send more yet.How do I check when the I/O has finished?There are a few ways. Let’s see how to do it using select and epoll.``` {.c language=”C”}int select(int nfds,            fd_set *readfds,            fd_set *writefds,           fd_set *exceptfds,            struct timeval *timeout);Given three sets of file descriptors, will wait for any of those filedescriptors to become ‘ready’.1.  - a file descriptor in is ready when there is data that can be read    or EOF has been reached.2.  - a file descriptor in is ready when a call to write() will succeed.3.  - system-specific, not well-defined. Just pass NULL for this.returns the total number of file descriptors that are ready. If none ofthem become ready during the time defined by *timeout*, it will return0. After returns, the caller will need to loop through the filedescriptors in readfds and/or writefds to see which ones are ready. Asreadfds and writefds act as both input and output parameters, whenindicates that there are file descriptors which are ready, it would haveoverwritten them to reflect only the file descriptors which are ready.Unless it is the caller’s intention to call only once, it would be agood idea to save a copy of readfds and writefds before calling it.``` {.c language=\"C\"}fd_set readfds, writefds;FD_ZERO(&amp;readfds);FD_ZERO(&amp;writefds);for (int i=0; i &lt; read_fd_count; i++)  FD_SET(my_read_fds[i], &amp;readfds);for (int i=0; i &lt; write_fd_count; i++)  FD_SET(my_write_fds[i], &amp;writefds);struct timeval timeout;timeout.tv_sec = 3;timeout.tv_usec = 0;int num_ready = select(FD_SETSIZE, &amp;readfds, &amp;writefds, NULL, &amp;timeout);if (num_ready &lt; 0) {  perror(\"error in select()\");} else if (num_ready == 0) {  printf(\"timeout\\n\");} else {  for (int i=0; i &lt; read_fd_count; i++)    if (FD_ISSET(my_read_fds[i], &amp;readfds))      printf(\"fd %d is ready for reading\\n\", my_read_fds[i]);  for (int i=0; i &lt; write_fd_count; i++)    if (FD_ISSET(my_write_fds[i], &amp;writefds))      printf(\"fd %d is ready for writing\\n\", my_write_fds[i]);}For more information onselect()epollepoll is not part of POSIX, but it is supported by Linux. It is a moreefficient way to wait for many file descriptors. It will tell youexactly which descriptors are ready. It even gives you a way to store asmall amount of data with each descriptor, like an array index or apointer, making it easier to access your data associated with thatdescriptor.To use epoll, first you must create a special file descriptor withepoll_create(). You won’tread or write to this file descriptor; you’ll just pass it to the otherepoll_xxx functions and call close() on it at the end.``` {.c language=”C”}    epfd = epoll_create(1);For each file descriptor you want to monitor with epoll, you’ll need toadd it to the epoll data structures using[epoll\\_ctl()](http://linux.die.net/man/2/epoll_ctl) with the option.You can add any number of file descriptors to it.``` {.c language=\"C\"}struct epoll_event event;event.events = EPOLLOUT;  // EPOLLIN==read, EPOLLOUT==writeevent.data.ptr = mypointer;epoll_ctl(epfd, EPOLL_CTL_ADD, mypointer-&gt;fd, &amp;event)To wait for some of the file descriptors to become ready, useepoll_wait(). The epoll_eventstruct that it fills out will contain the data you provided inevent.data when you added this file descriptor. This makes it easy foryou to look up your own data associated with this file descriptor.``` {.c language=”C”}int num_ready = epoll_wait(epfd, &amp;event, 1, timeout_milliseconds);if (num_ready &gt; 0) {  MyData mypointer = (MyData) event.data.ptr;  printf(“ready to write on %d\\n”, mypointer-&gt;fd);}Say you were waiting to write data to a file descriptor, but now youwant to wait to read data from it. Just use with the option to changethe type of operation you’re monitoring.``` {.c language=\"C\"}event.events = EPOLLOUT;event.data.ptr = mypointer;epoll_ctl(epfd, EPOLL_CTL_MOD, mypointer-&gt;fd, &amp;event);To unsubscribe one file descriptor from epoll while leaving othersactive, use with the option.``` {.c language=”C”}epoll_ctl(epfd, EPOLL_CTL_DEL, mypointer-&gt;fd, NULL);To shut down an epoll instance, close its file descriptor.``` {.c language=\"C\"}close(epfd);In addition to nonblocking and , any calls to on a nonblocking socketwill also be nonblocking. To wait for the connection to complete, use orepoll to wait for the socket to be writable. There are definitelyreasons to use epoll over select but due to to interface, there arefundamental problems with doing so.Blogpost about select beingbrokenRemote Procedure CallsRemote Procedure Call. RPC is the idea that we can execute a procedure(function) on a different machine. In practice the procedure may executeon the same machine, however it may be in a different context - forexample under a different user with different permissions and differentlifecycle.What is Privilege Separation?The remote code will execute under a different user and with differentprivileges from the caller. In practice the remote call may execute withmore or fewer privileges than the caller. This in principle can be usedto improve the security of a system (by ensuring components operate withleast privilege). Unfortunately, security concerns need to be carefullyassessed to ensure that RPC mechanisms cannot be subverted to performunwanted actions. For example, an RPC implementation may implicitlytrust any connected client to perform any action, rather than a subsetof actions on a subset of the data.What is stub code? What is marshalling?The stub code is the necessary code to hide the complexity of performinga remote procedure call. One of the roles of the stub code is tomarshall the necessary data into a format that can be sent as a bytestream to a remote server.``` {.c language=”C”}// On the outside ‘getHiscore’ looks like a normal function call// On the inside the stub code performs all of the work to send and receive the data to and from the remote machine.int getHighScore(char* game) {  // Marshall the request into a sequence of bytes:  char* buffer;  asprintf(&amp;buffer,”getHiscore(%s)!”, name);// Send down the wire (we do not send the zero byte; the ‘!’ signifies the end of the message)  write(fd, buffer, strlen(buffer) );// Wait for the server to send a response  ssize_t bytesread = read(fd, buffer, sizeof(buffer));// Example: unmarshal the bytes received back from text into an int  buffer[bytesread] = 0; // Turn the result into a C stringint score= atoi(buffer);  free(buffer);  return score;}### What is server stub code? What is unmarshalling?The server stub code will receive the request, unmarshall the requestinto a valid in-memory data call the underlying implementation and sendthe result back to the caller.### How do you send an int? float? a struct? A linked list? A graph?To implement RPC you need to decide (and document) which conventions youwill use to serialize the data into a byte sequence. Even a simpleinteger has several common choices:1.  Signed or unsigned?2.  ASCII, Unicode Text Format 8, some other encoding?3.  Fixed number of bytes or variable depending on magnitude4.  Little or Big endian binary format?To marshall a struct, decide which fields need to be serialized. It maynot be necessary to send all data items (for example, some items may beirrelevant to the specific RPC or can be re-computed by the server fromthe other data items present).To marshall a linked list it is unnecessary to send the link pointers-just stream the values. As part of unmarshalling the server can recreatea linked list structure from the byte sequence.By starting at the head node/vertex, a simple tree can be recursivelyvisited to create a serialized version of the data. A cyclic graph willusually require additional memory to ensure that each edge and vertex isprocessed exactly once.### What is an Interface Description Language (IDL)?Writing stub code by hand is painful, tedious, error prone, difficult tomaintain and difficult to reverse engineer the wire protocol from theimplemented code. A better approach is specify the data objects,messages and services and automatically generate the client and servercode.A modern example of an Interface Description Language is Google’sProtocol Buffer .proto files.### Complexity and challenges of RPC vs local calls?Remote Procedure Calls are significantly slower (10x to 100x) and morecomplex than local calls. An RPC must marshall data into awire-compatible format. This may require multiple passes through thedata structure, temporary memory allocation and transformation of thedata representation.Robust RPC stub code must intelligently handle network failures andversioning. For example, a server may have to process requests fromclients that are still running an early version of the stub code.A secure RPC will need to implement additional security checks(including authentication and authorization), validate data and encryptcommunication between the client and host.### Transferring large amounts of structured dataLet’s examine three methods of transferring data using 3 differentformats - JSON, XML and Google Protocol Buffers. JSON and XML aretext-based protocols. Examples of JSON and XML messages are below.``` {.xml language=\"XML\"}&lt;ticket&gt;&lt;price currency='dollar'&gt;10&lt;/price&gt;&lt;vendor&gt;travelocity&lt;/vendor&gt;&lt;/ticket&gt;{ 'currency':'dollar' , 'vendor':'travelocity', 'price':'10' }Google Protocol Buffers is an open-source efficient binary protocol thatplaces a strong emphasis on high throughput with low CPU overhead andminimal memory copying. Implementations exist for multiple languagesincluding Go, Python, C++ and C. This means client and server stub codein multiple languages can be generated from the .proto specificationfile to marshall data to and from a binary stream.Google ProtocolBuffersreduces the versioning problem by ignoring unknown fields that arepresent in a message. See the introduction to Protocol Buffers for moreinformation.Topics      IPv4 vs IPv6        TCP vs UDP        Packet Loss/Connection Based        Get address info        DNS        TCP client calls        TCP server calls        shutdown        recvfrom        epoll vs select        RPC  Questions      What is IPv4? IPv6? What are the differences between them?        What is TCP? UDP? Give me advantages and disadvantages of both ofthem. When would I use one and not the other?        Which protocol is connection less and which one is connection based?        What is DNS? What is the route that DNS takes?        What does socket do?        What are the calls to set up a TCP client?        What are the calls to set up a TCP server?        What is the difference between a socket shutdown and closing?        When can you use and ? How about and ?        What are some advantages to over ? How about over ?        What is a remote procedure call? When should I use it?        What is marshalling/unmarshalling? Why is HTTP not an RPC?  "
  }
]
